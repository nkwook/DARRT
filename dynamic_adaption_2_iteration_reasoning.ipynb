{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Rule Adaption (2) Rule Generation & Dynamic Adaptation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "## Input\n",
    "- icews14 (/dataset/icews14/*)\n",
    "- sampled rules (sampled_path/icews14/original/rules_var.json)\n",
    "\n",
    "## Output\n",
    "- llm adapted rules (gen_rules_iteration/final_summary/rules.txt)\n",
    "- confidence files for each iterations (evaluation/eva/*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup requirements\n",
    "- python 3.12\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!pip install joblib>=1.5.0 \\\n",
    "    matplotlib \\\n",
    "    networkx==3.2.1 \\\n",
    "    numpy==1.26.3 \\\n",
    "    openai==1.79.0 \\\n",
    "    pandas==2.2.0 \\\n",
    "    python-dotenv==1.0.0 \\\n",
    "    scipy==1.12.0 \\\n",
    "    seaborn==0.13.1 \\\n",
    "    sentence-transformers>=3.0.1 \\\n",
    "    tiktoken==0.9.0 \\\n",
    "    torch==2.4.0 \\\n",
    "    tqdm==4.66.1 \\\n",
    "    transformers==4.35.2 \\\n",
    "    datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get LLM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "\n",
    "class BaseLanguageModel(object):\n",
    "    \"\"\"\n",
    "    Base lanuage model. Define how to generate sentence by using a LM\n",
    "    Args:\n",
    "        args: arguments for LM configuration\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def add_args(parser):\n",
    "        return\n",
    "\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "\n",
    "    def load_model(self, **kwargs):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def token_len(self, text):\n",
    "        '''\n",
    "        Return tokenized length of text\n",
    "\n",
    "        Args:\n",
    "            text (str): input text\n",
    "        '''\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def prepare_for_inference(self, **model_kwargs):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def prepare_model_prompt(self, query):\n",
    "        '''\n",
    "        Add model-specific prompt to the input\n",
    "\n",
    "        Args:\n",
    "            instruction (str)\n",
    "            input (str): str\n",
    "        '''\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def generate_sentence(self, llm_input):\n",
    "        \"\"\"\n",
    "        Generate sentence by using a LM\n",
    "\n",
    "        Args:\n",
    "            lm_input (LMInput): input for LM\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def gen_rule_statistic(self, input_dir, output_file_path):\n",
    "\n",
    "        raise NotImplementedError\n",
    "    \n",
    "\n",
    "import time\n",
    "import os\n",
    "from openai import OpenAI\n",
    "import openai\n",
    "import dotenv\n",
    "import tiktoken\n",
    "import glob\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "os.environ[\"TIKTOKEN_CACHE_DIR\"] = \"./tmp\"\n",
    "\n",
    "OPENAI_MODEL = [\"gpt-4\", \"gpt-3.5-turbo\"]\n",
    "\n",
    "\n",
    "def get_token_limit(model=\"gpt-4\"):\n",
    "    \"\"\"Returns the token limitation of provided model\"\"\"\n",
    "    if model in [\"gpt-4\", \"gpt-4-0613\"]:\n",
    "        num_tokens_limit = 8192\n",
    "    elif model in [\"gpt-3.5-turbo-16k\", \"gpt-3.5-turbo-16k-0613\", \"gpt-3.5-turbo-0125\"]:\n",
    "        num_tokens_limit = 16384\n",
    "    elif model in [\"gpt-3.5-turbo\", \"gpt-3.5-turbo-0613\", \"text-davinci-003\", \"text-davinci-002\"]:\n",
    "        num_tokens_limit = 4096\n",
    "    elif model in [\"gpt-4.1-mini\", \"gpt-4.1-nano\"]: \n",
    "        num_tokens_limit = 16384\n",
    "    else:\n",
    "        raise NotImplementedError(f\"\"\"get_token_limit() is not implemented for model {model}.\"\"\")\n",
    "    return num_tokens_limit\n",
    "\n",
    "\n",
    "PROMPT = \"\"\"{instruction}\n",
    "\n",
    "{input}\"\"\"\n",
    "\n",
    "\n",
    "class ChatGPT(BaseLanguageModel):\n",
    "    @staticmethod\n",
    "    def add_args(parser):\n",
    "        parser.add_argument(\"--retry\", type=int, help=\"retry time\", default=5)\n",
    "        parser.add_argument(\"--model_path\", type=str, default=\"None\")\n",
    "        \n",
    "    def __init__(self, args):\n",
    "        super().__init__(args)\n",
    "        self.retry = args.retry\n",
    "        self.model_name = args.model_name\n",
    "        self.maximum_token = get_token_limit(self.model_name)\n",
    "\n",
    "    def token_len(self, text):\n",
    "        \"\"\"Returns the number of tokens used by a list of messages.\"\"\"\n",
    "        try:\n",
    "            encoding = tiktoken.encoding_for_model(\"gpt-4o\")  # TODO: 수정했음. 4.1은 4o와 같은 tokenizer씀\n",
    "            num_tokens = len(encoding.encode(text))\n",
    "        except KeyError:\n",
    "            raise KeyError(f\"Warning: model {self.model_name} not found.\")\n",
    "        return num_tokens\n",
    "\n",
    "    def prepare_for_inference(self, model_kwargs={}):\n",
    "        client = OpenAI(\n",
    "            api_key=os.environ[\"OPENAI_API_KEY\"],  # this is also the default, it can be omitted\n",
    "        )\n",
    "        self.client = client\n",
    "\n",
    "    def prepare_model_prompt(self, query):\n",
    "        \"\"\"\n",
    "        Add model-specific prompt to the input\n",
    "        \"\"\"\n",
    "        return query\n",
    "\n",
    "    def generate_sentence(self, llm_input):\n",
    "        query = [{\"role\": \"user\", \"content\": llm_input}]\n",
    "        cur_retry = 0\n",
    "        num_retry = self.retry\n",
    "        # Check if the input is too long\n",
    "        input_length = self.token_len(llm_input)\n",
    "        if input_length > self.maximum_token:\n",
    "            print(\n",
    "                f\"Input length {input_length} is too long. The maximum token is {self.maximum_token}.\\n Right truncate the input to {self.maximum_token} tokens.\"\n",
    "            )\n",
    "            llm_input = llm_input[: self.maximum_token]\n",
    "            query = [{\"role\": \"user\", \"content\": llm_input}]\n",
    "        while cur_retry <= num_retry:\n",
    "            try:\n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=self.model_name, messages=query, timeout=60, temperature=0.0\n",
    "                )\n",
    "                result = response.choices[0].message.content.strip()  # type: ignore\n",
    "                return result\n",
    "            except openai.APITimeoutError as e:\n",
    "                wait_time = 30 + 10 * cur_retry  # Exponential backoff\n",
    "                print(f\"Request Time out. Retrying in {wait_time} seconds...\")\n",
    "                time.sleep(wait_time)\n",
    "                cur_retry += 1\n",
    "            except openai.RateLimitError as e:\n",
    "                wait_time = 30 + 10 * cur_retry  # Exponential backoff\n",
    "                print(f\"Rate limit exceeded. Retrying in {wait_time} seconds...\")\n",
    "                time.sleep(wait_time)\n",
    "                cur_retry += 1\n",
    "            except openai.APIConnectionError as e:\n",
    "                # 打印异常的详细信息\n",
    "                print(\"Failed to connect to OpenAI API.\")\n",
    "                print(\"Error message:\", e.args[0] if e.args else \"No details available.\")\n",
    "                if hasattr(e, \"response\") and e.response:\n",
    "                    print(\"HTTP response status:\", e.response.status_code)\n",
    "                    print(\"HTTP response body:\", e.response.text)\n",
    "                else:\n",
    "                    print(\"No HTTP response received.\")\n",
    "                wait_time = 30 + 10 * cur_retry  # Exponential backoff\n",
    "                print(f\"Retrying in {wait_time} seconds...\")\n",
    "                time.sleep(wait_time)\n",
    "                cur_retry += 1\n",
    "            except Exception as e:\n",
    "                print(\"Message: \", llm_input)\n",
    "                print(\"Number of token: \", self.token_len(llm_input))\n",
    "                print(e)\n",
    "                time.sleep(30)\n",
    "                cur_retry += 1\n",
    "        print(f\"Maximum retries reached. Unable to generate sentence\")\n",
    "        return None\n",
    "\n",
    "    def gen_rule_statistic(self, input_dir, output_file_path):\n",
    "        sum = 0\n",
    "        with open(output_file_path, \"w\") as fout:\n",
    "            for input_filepath in glob.glob(os.path.join(input_dir, \"*.txt\")):\n",
    "                file_name = input_filepath.split(\"/\")[-1]\n",
    "                if file_name.startswith(\"fail\"):\n",
    "                    continue\n",
    "                else:\n",
    "                    with open(input_filepath, \"r\") as fin:\n",
    "                        rules = fin.readlines()\n",
    "                        for rule in rules:\n",
    "                            if \"Rule_head\" in rule:\n",
    "                                continue\n",
    "                            elif \"Sample\" in rule:\n",
    "                                continue\n",
    "                            fout.write(rule)\n",
    "                            sum += 1\n",
    "\n",
    "            fout.write(f\"LL {sum}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def copy_folder_contents(source_folder, destination_folder):\n",
    "    if not os.path.exists(destination_folder):\n",
    "        os.makedirs(destination_folder)\n",
    "\n",
    "def filter_candidates(test_query, candidates, test_data):\n",
    "    \"\"\"\n",
    "    Filter out those candidates that are also answers to the test query\n",
    "    but not the correct answer.\n",
    "\n",
    "    Parameters:\n",
    "        test_query (np.ndarray): test_query\n",
    "        candidates (dict): answer candidates with corresponding confidence scores\n",
    "        test_data (np.ndarray): test dataset\n",
    "\n",
    "    Returns:\n",
    "        candidates (dict): filtered candidates\n",
    "    \"\"\"\n",
    "\n",
    "    other_answers = test_data[\n",
    "        (test_data[:, 0] == test_query[0])\n",
    "        * (test_data[:, 1] == test_query[1])\n",
    "        * (test_data[:, 2] != test_query[2])\n",
    "        * (test_data[:, 3] == test_query[3])\n",
    "    ]\n",
    "\n",
    "    if len(other_answers):\n",
    "        objects = other_answers[:, 2]\n",
    "        for obj in objects:\n",
    "            candidates.pop(obj, None)\n",
    "\n",
    "    return candidates\n",
    "\n",
    "\n",
    "\n",
    "import argparse\n",
    "import shutil\n",
    "\n",
    "\n",
    "def save_json_data(data, file_path):\n",
    "    try:\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            json.dump(data, file, indent=4, ensure_ascii=False)\n",
    "        print(f\"Data has been converted to JSON and saved to {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving JSON data to {file_path}: {e}\")\n",
    "\n",
    "def load_json_data(file_path, default=None):\n",
    "    try:\n",
    "        if os.path.exists(file_path):\n",
    "            print(f\"Use cache from: {file_path}\")\n",
    "            with open(file_path, \"r\") as file:\n",
    "                return json.load(file)\n",
    "        else:\n",
    "            print(f\"File not found: {file_path}\")\n",
    "            return default\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading JSON data from {file_path}: {e}\")\n",
    "    return default\n",
    "\n",
    "def str_to_bool(value):\n",
    "    if isinstance(value, bool):\n",
    "        return value\n",
    "    if value.lower() in (\"yes\", \"true\", \"t\", \"y\", \"1\"):\n",
    "        return True\n",
    "    elif value.lower() in (\"no\", \"false\", \"f\", \"n\", \"0\"):\n",
    "        return False\n",
    "    else:\n",
    "        raise argparse.ArgumentTypeError(\"Boolean value expected.\")\n",
    "\n",
    "def clear_folder(folder_path):\n",
    "    # 确保文件夹存在\n",
    "    if not os.path.exists(folder_path):\n",
    "        return\n",
    "\n",
    "    # 遍历文件夹中的所有文件和文件夹\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        # 如果是文件，则直接删除\n",
    "        if os.path.isfile(file_path):\n",
    "            os.remove(file_path)\n",
    "        # 如果是文件夹，则递归清空文件夹\n",
    "        elif os.path.isdir(file_path):\n",
    "            shutil.rmtree(file_path)\n",
    "\n",
    "def check_prompt_length(prompt, list_of_paths, model):\n",
    "    \"\"\"Check whether the input prompt is too long. If it is too long, remove the first path and check again.\"\"\"\n",
    "    all_paths = \"\\n\".join(list_of_paths)\n",
    "    all_tokens = prompt + all_paths\n",
    "    maximun_token = model.maximum_token\n",
    "    if model.token_len(all_tokens) < maximun_token:\n",
    "        return all_paths\n",
    "    else:\n",
    "        # Shuffle the paths\n",
    "        random.shuffle(list_of_paths)\n",
    "        new_list_of_paths = []\n",
    "        # check the length of the prompt\n",
    "        for p in list_of_paths:\n",
    "            tmp_all_paths = \"\\n\".join(new_list_of_paths + [p])\n",
    "            tmp_all_tokens = prompt + tmp_all_paths\n",
    "            if model.token_len(tmp_all_tokens) > maximun_token:\n",
    "                return \"\\n\".join(new_list_of_paths)\n",
    "            new_list_of_paths.append(p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import random\n",
    "from random import sample\n",
    "import networkx as nx\n",
    "from scipy import sparse\n",
    "\n",
    "\n",
    "def construct_nx(idx2rel, idx2ent, ent2idx, fact_rdf):\n",
    "    G = nx.Graph()\n",
    "    for rdf in fact_rdf:\n",
    "        fact = parse_rdf(rdf)\n",
    "        h, r, t = fact\n",
    "        h_idx, t_idx = ent2idx[h], ent2idx[t]\n",
    "        G.add_edge(h_idx, t_idx, relation=r)\n",
    "    return G\n",
    "\n",
    "\n",
    "def construct_fact_dict(fact_rdf):\n",
    "    fact_dict = {}\n",
    "    for rdf in fact_rdf:\n",
    "        fact = parse_rdf(rdf)\n",
    "        h, r, t = fact\n",
    "        if r not in fact_dict:\n",
    "            fact_dict[r] = []\n",
    "        fact_dict[r].append(rdf)\n",
    "\n",
    "    return fact_dict\n",
    "\n",
    "\n",
    "def construct_rmat(idx2rel, idx2ent, ent2idx, fact_rdf):\n",
    "    e_num = len(idx2ent)\n",
    "    r2mat = {}\n",
    "    # initialize rmat\n",
    "    for idx, rel in idx2rel.items():\n",
    "        mat = sparse.dok_matrix((e_num, e_num))\n",
    "        r2mat[rel] = mat\n",
    "    # fill rmat\n",
    "    for rdf in fact_rdf:\n",
    "        fact = parse_rdf(rdf)\n",
    "        h, r, t = fact\n",
    "        h_idx, t_idx = ent2idx[h], ent2idx[t]\n",
    "        r2mat[r][h_idx, t_idx] = 1\n",
    "    return r2mat\n",
    "\n",
    "\n",
    "class RuleDataset(object):\n",
    "    def __init__(self, r2mat, rules, e_num, idx2rel, args):\n",
    "        self.e_num = e_num\n",
    "        self.r2mat = r2mat\n",
    "        self.rules = rules\n",
    "        self.idx2rel = idx2rel\n",
    "        self.len = len(self.rules)\n",
    "        self.args = args\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        rel = self.idx2rel[idx]\n",
    "        _rules = self.rules[rel]\n",
    "        path_count = sparse.dok_matrix((self.e_num, self.e_num))\n",
    "        for rule in _rules:\n",
    "            head, body, conf_1, conf_2 = rule\n",
    "\n",
    "            body_adj = sparse.eye(self.e_num)\n",
    "            for b_rel in body:\n",
    "                body_adj = body_adj * self.r2mat[b_rel]\n",
    "\n",
    "            body_adj = body_adj * conf_1\n",
    "            path_count += body_adj\n",
    "\n",
    "        return rel, path_count\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(data):\n",
    "        head = [_[0] for _ in data]\n",
    "        path_count = [_[1] for _ in data]\n",
    "        return head, path_count\n",
    "\n",
    "\n",
    "def parse_rdf(rdf):\n",
    "    \"\"\"\n",
    "    return: head, relation, tail\n",
    "    \"\"\"\n",
    "    return rdf\n",
    "    # rdf_tail, rdf_rel, rdf_head = rdf\n",
    "    # return rdf_head, rdf_rel, rdf_tail\n",
    "\n",
    "\n",
    "class Dictionary(object):\n",
    "    def __init__(self):\n",
    "        self.rel2idx_ = {}\n",
    "        self.idx2rel_ = {}\n",
    "        self.idx = 0\n",
    "\n",
    "    def add_relation(self, rel):\n",
    "        if rel not in self.rel2idx_.keys():\n",
    "            self.rel2idx_[rel] = self.idx\n",
    "            self.idx2rel_[self.idx] = rel\n",
    "            self.idx += 1\n",
    "\n",
    "    @property\n",
    "    def rel2idx(self):\n",
    "        return self.rel2idx_\n",
    "\n",
    "    @property\n",
    "    def idx2rel(self):\n",
    "        return self.idx2rel_\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx2rel_)\n",
    "\n",
    "\n",
    "def load_entities(path):\n",
    "    idx2ent, ent2idx = {}, {}\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "        for idx, line in enumerate(lines):\n",
    "            e = line.strip()\n",
    "            ent2idx[e] = idx\n",
    "            idx2ent[idx] = e\n",
    "    return idx2ent, ent2idx\n",
    "\n",
    "\n",
    "class Dataset(object):\n",
    "    def __init__(self, data_root, sparsity=1, inv=False):\n",
    "        # Construct entity_list\n",
    "        entity_path = data_root + \"entities.txt\"\n",
    "        self.idx2ent_, self.ent2idx_ = load_entities(entity_path)\n",
    "        # Construct rdict which contains relation2idx & idx2relation2\n",
    "        relation_path = data_root + \"relations.txt\"\n",
    "        self.rdict = Dictionary()\n",
    "        self.load_relation_dict(relation_path)\n",
    "        # head relation\n",
    "        self.head_rdict = Dictionary()\n",
    "        self.head_rdict = copy.deepcopy(self.rdict)\n",
    "        # load (h, r, t) tuples\n",
    "        fact_path = data_root + \"facts.txt\"\n",
    "        train_path = data_root + \"train.txt\"\n",
    "        valid_path = data_root + \"valid.txt\"\n",
    "        test_path = data_root + \"test.txt\"\n",
    "        if inv:\n",
    "            fact_path += \".inv\"\n",
    "        self.rdf_data_ = self.load_data_(fact_path, train_path, valid_path, test_path, sparsity)\n",
    "        self.fact_rdf_, self.train_rdf_, self.valid_rdf_, self.test_rdf_ = self.rdf_data_\n",
    "        # inverse\n",
    "        if inv:\n",
    "            # add inverse relation to rdict\n",
    "            rel_list = list(self.rdict.rel2idx_.keys())\n",
    "            for rel in rel_list:\n",
    "                inv_rel = \"inv_\" + rel\n",
    "                self.rdict.add_relation(inv_rel)\n",
    "                self.head_rdict.add_relation(inv_rel)\n",
    "                # add None\n",
    "        self.head_rdict.add_relation(\"None\")\n",
    "\n",
    "    def load_rdfs(self, path):\n",
    "        rdf_list = []\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                tuples = line.strip().split(\"\\t\")\n",
    "                rdf_list.append(tuples)\n",
    "        return rdf_list\n",
    "\n",
    "    def load_data_(self, fact_path, train_path, valid_path, test_path, sparsity):\n",
    "        fact = self.load_rdfs(fact_path)\n",
    "        fact = sample(fact, int(len(fact) * sparsity))\n",
    "        train = self.load_rdfs(train_path)\n",
    "        valid = self.load_rdfs(valid_path)\n",
    "        test = self.load_rdfs(test_path)\n",
    "        return fact, train, valid, test\n",
    "\n",
    "    def load_relation_dict(self, relation_path):\n",
    "        \"\"\"\n",
    "        Read relation.txt to relation dictionary\n",
    "        \"\"\"\n",
    "        with open(relation_path, encoding=\"utf-8\") as f:\n",
    "            rel_list = f.readlines()\n",
    "            for r in rel_list:\n",
    "                relation = r.strip()\n",
    "                self.rdict.add_relation(relation)\n",
    "                # self.head_dict.add_relation(relation)\n",
    "\n",
    "    def get_relation_dict(self):\n",
    "        return self.rdict\n",
    "\n",
    "    def get_head_relation_dict(self):\n",
    "        return self.head_rdict\n",
    "\n",
    "    @property\n",
    "    def idx2ent(self):\n",
    "        return self.idx2ent_\n",
    "\n",
    "    @property\n",
    "    def ent2idx(self):\n",
    "        return self.ent2idx_\n",
    "\n",
    "    @property\n",
    "    def fact_rdf(self):\n",
    "        return self.fact_rdf_\n",
    "\n",
    "    @property\n",
    "    def train_rdf(self):\n",
    "        return self.train_rdf_\n",
    "\n",
    "    @property\n",
    "    def valid_rdf(self):\n",
    "        return self.valid_rdf_\n",
    "\n",
    "    @property\n",
    "    def test_rdf(self):\n",
    "        return self.test_rdf_\n",
    "\n",
    "\n",
    "def sample_anchor_rdf(rdf_data, num=1):\n",
    "    if num < len(rdf_data):\n",
    "        return sample(rdf_data, num)\n",
    "    else:\n",
    "        return rdf_data\n",
    "\n",
    "\n",
    "def construct_descendant(rdf_data):\n",
    "    # take entity as h, map it to its r, t\n",
    "    entity2desced = {}\n",
    "    for rdf_ in rdf_data:\n",
    "        h_, r_, t_ = parse_rdf(rdf_)\n",
    "        if h_ not in entity2desced.keys():\n",
    "            entity2desced[h_] = [(r_, t_)]\n",
    "        else:\n",
    "            entity2desced[h_].append((r_, t_))\n",
    "    return entity2desced\n",
    "\n",
    "\n",
    "def connected(entity2desced, head, tail):\n",
    "    if head in entity2desced:\n",
    "        decedents = entity2desced[head]\n",
    "        for d in decedents:\n",
    "            d_relation_, d_tail_ = d\n",
    "            if d_tail_ == tail:\n",
    "                return d_relation_\n",
    "        return False\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def search_closed_rel_paths(anchor_rdf, entity2desced, max_path_len=2):\n",
    "    anchor_h, anchor_r, anchor_t = parse_rdf(anchor_rdf)\n",
    "    visited = set()\n",
    "    rules = []\n",
    "\n",
    "    def dfs(current, rel_path):\n",
    "        if len(rel_path) > max_path_len:  # max path length\n",
    "            return\n",
    "        if current == anchor_t and len(rel_path) == 1 and rel_path[-1] == anchor_r:  # remove directly connected\n",
    "            return\n",
    "        if current == anchor_t:\n",
    "            rule = \"|\".join(rel_path)\n",
    "            if rule not in rules:\n",
    "                rules.append(rule)\n",
    "        else:\n",
    "            visited.add(current)\n",
    "            if current in entity2desced:\n",
    "                deced_list = entity2desced[current]\n",
    "                for r, t in deced_list:\n",
    "                    if t not in visited:\n",
    "                        dfs(t, rel_path + [r])\n",
    "            visited.remove(current)\n",
    "\n",
    "    dfs(anchor_h, [])\n",
    "    return rules\n",
    "\n",
    "def body2idx(body_list, head_rdict):\n",
    "    \"\"\"\n",
    "    Input a rule (string) and idx it\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    for body in body_list:\n",
    "        body_path = body.split(\"|\")\n",
    "        # indexs include body idx seq + notation + head idx\n",
    "        indexs = []\n",
    "        for rel in body_path:\n",
    "            indexs.append(head_rdict.rel2idx[rel])\n",
    "        res.append(indexs)\n",
    "    return res\n",
    "\n",
    "\n",
    "def inv_rel_idx(head_rdict):\n",
    "    inv_rel_idx = []\n",
    "    for i_ in range(len(head_rdict.idx2rel)):\n",
    "        r_ = head_rdict.idx2rel[i_]\n",
    "        if \"inv_\" in r_:\n",
    "            inv_rel_idx.append(i_)\n",
    "    return inv_rel_idx\n",
    "\n",
    "\n",
    "def idx2body(index, head_rdict):\n",
    "    body = \"|\".join([head_rdict.idx2rel[idx] for idx in index])\n",
    "    return body\n",
    "\n",
    "\n",
    "def rule2idx(rule, head_rdict):\n",
    "    \"\"\"\n",
    "    Input a rule (string) and idx it\n",
    "    \"\"\"\n",
    "    body, head = rule.split(\"-\")\n",
    "    body_path = body.split(\"|\")\n",
    "    # indexs include body idx seq + notation + head idx\n",
    "    indexs = []\n",
    "    for rel in body_path + [-1, head]:\n",
    "        indexs.append(head_rdict.rel2idx[rel] if rel != -1 else -1)\n",
    "    return indexs\n",
    "\n",
    "\n",
    "def idx2rule(index, head_rdict):\n",
    "    body_idx = index[0:-2]\n",
    "    body = \"|\".join([head_rdict.idx2rel[b] for b in body_idx])\n",
    "    rule = body + \"-\" + head_rdict.idx2rel[index[-1]]\n",
    "    return rule\n",
    "\n",
    "\n",
    "def enumerate_body(relation_num, body_len, rdict):\n",
    "    import itertools\n",
    "\n",
    "    all_body_idx = list(list(x) for x in itertools.product(range(relation_num), repeat=body_len))\n",
    "    # transfer index to relation name\n",
    "    idx2rel = rdict.idx2rel\n",
    "    all_body = []\n",
    "    for b_idx_ in all_body_idx:\n",
    "        b_ = [idx2rel[x] for x in b_idx_]\n",
    "        all_body.append(b_)\n",
    "    return all_body_idx, all_body\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rule Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import itertools\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "import copy\n",
    "import re\n",
    "import traceback\n",
    "\n",
    "\n",
    "\n",
    "class Rule_Learner(object):\n",
    "    def __init__(self, edges, id2relation, inv_relation_id, dataset):\n",
    "        \"\"\"\n",
    "        Initialize rule learner object.\n",
    "\n",
    "        Parameters:\n",
    "            edges (dict): edges for each relation\n",
    "            id2relation (dict): mapping of index to relation\n",
    "            inv_relation_id (dict): mapping of relation to inverse relation\n",
    "            dataset (str): dataset name\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "\n",
    "        self.edges = edges\n",
    "        self.id2relation = id2relation\n",
    "        self.inv_relation_id = inv_relation_id\n",
    "        self.num_individual = 0\n",
    "        self.num_shared = 0\n",
    "        self.num_original = 0\n",
    "\n",
    "        self.found_rules = []\n",
    "        self.rule2confidence_dict = {}\n",
    "        self.original_found_rules = []\n",
    "        self.rules_dict = dict()\n",
    "        self.output_dir = \"./sampled_path/\" + dataset + \"/\"\n",
    "        if not os.path.exists(self.output_dir):\n",
    "            os.makedirs(self.output_dir)\n",
    "\n",
    "    def create_rule(self, walk, confidence=0, use_relax_time=False):\n",
    "        \"\"\"\n",
    "        Create a rule given a cyclic temporal random walk.\n",
    "        The rule contains information about head relation, body relations,\n",
    "        variable constraints, confidence, rule support, and body support.\n",
    "        A rule is a dictionary with the content\n",
    "        {\"head_rel\": int, \"body_rels\": list, \"var_constraints\": list,\n",
    "         \"conf\": float, \"rule_supp\": int, \"body_supp\": int}\n",
    "\n",
    "        Parameters:\n",
    "            walk (dict): cyclic temporal random walk\n",
    "                         {\"entities\": list, \"relations\": list, \"timestamps\": list}\n",
    "            confidence (float): confidence value\n",
    "            use_relax_time (bool): whether the rule is created with relaxed time\n",
    "\n",
    "        Returns:\n",
    "            rule (dict): created rule\n",
    "        \"\"\"\n",
    "\n",
    "        rule = dict()\n",
    "        rule[\"head_rel\"] = int(walk[\"relations\"][0])\n",
    "        rule[\"body_rels\"] = [self.inv_relation_id[x] for x in walk[\"relations\"][1:][::-1]]\n",
    "        rule[\"var_constraints\"] = self.define_var_constraints(walk[\"entities\"][1:][::-1])\n",
    "\n",
    "        if rule not in self.found_rules:\n",
    "            self.found_rules.append(rule.copy())\n",
    "            (\n",
    "                rule[\"conf\"],\n",
    "                rule[\"rule_supp\"],\n",
    "                rule[\"body_supp\"],\n",
    "            ) = self.estimate_confidence(rule, is_relax_time=use_relax_time)\n",
    "\n",
    "            rule[\"llm_confidence\"] = confidence\n",
    "\n",
    "            if rule[\"conf\"] or confidence:\n",
    "                self.update_rules_dict(rule)\n",
    "\n",
    "    def create_rule_for_merge(\n",
    "        self, walk, confidence=0, rule_without_confidence=\"\", rules_var_dict=None, is_merge=False, is_relax_time=False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Create a rule given a cyclic temporal random walk.\n",
    "        The rule contains information about head relation, body relations,\n",
    "        variable constraints, confidence, rule support, and body support.\n",
    "        A rule is a dictionary with the content\n",
    "        {\"head_rel\": int, \"body_rels\": list, \"var_constraints\": list,\n",
    "         \"conf\": float, \"rule_supp\": int, \"body_supp\": int}\n",
    "\n",
    "        Parameters:\n",
    "            walk (dict): cyclic temporal random walk\n",
    "                         {\"entities\": list, \"relations\": list, \"timestamps\": list}\n",
    "\n",
    "        Returns:\n",
    "            rule (dict): created rule\n",
    "        \"\"\"\n",
    "\n",
    "        rule = dict()\n",
    "        rule[\"head_rel\"] = int(walk[\"relations\"][0])\n",
    "        rule[\"body_rels\"] = [self.inv_relation_id[x] for x in walk[\"relations\"][1:][::-1]]\n",
    "        rule[\"var_constraints\"] = self.define_var_constraints(walk[\"entities\"][1:][::-1])\n",
    "\n",
    "        if is_merge is True:\n",
    "            if rules_var_dict.get(rule_without_confidence) is None:\n",
    "                if rule not in self.found_rules:\n",
    "                    self.found_rules.append(rule.copy())\n",
    "                    (\n",
    "                        rule[\"conf\"],\n",
    "                        rule[\"rule_supp\"],\n",
    "                        rule[\"body_supp\"],\n",
    "                    ) = self.estimate_confidence(rule)\n",
    "\n",
    "                    rule[\"llm_confidence\"] = confidence\n",
    "\n",
    "                    if rule[\"conf\"] or confidence:\n",
    "                        self.num_individual += 1\n",
    "                        self.update_rules_dict(rule)\n",
    "\n",
    "            else:\n",
    "                rule_var = rules_var_dict[rule_without_confidence]\n",
    "                rule_var[\"llm_confidence\"] = confidence\n",
    "                temp_var = {}\n",
    "                temp_var[\"head_rel\"] = rule_var[\"head_rel\"]\n",
    "                temp_var[\"body_rels\"] = rule_var[\"body_rels\"]\n",
    "                temp_var[\"var_constraints\"] = rule_var[\"var_constraints\"]\n",
    "                if temp_var not in self.original_found_rules:\n",
    "                    self.original_found_rules.append(temp_var.copy())\n",
    "                    self.update_rules_dict(rule_var)\n",
    "                    self.num_shared += 1\n",
    "        else:\n",
    "            if rule not in self.found_rules:\n",
    "                self.found_rules.append(rule.copy())\n",
    "                (\n",
    "                    rule[\"conf\"],\n",
    "                    rule[\"rule_supp\"],\n",
    "                    rule[\"body_supp\"],\n",
    "                ) = self.estimate_confidence(rule, is_relax_time=is_relax_time)\n",
    "\n",
    "                # if rule[\"body_supp\"] == 0:\n",
    "                #     rule[\"body_supp\"] = 2\n",
    "\n",
    "                rule[\"llm_confidence\"] = confidence\n",
    "\n",
    "                if rule[\"conf\"] or confidence:\n",
    "                    self.update_rules_dict(rule)\n",
    "\n",
    "    def create_rule_for_merge_for_iteration(\n",
    "        self, walk, llm_confidence=0, rule_without_confidence=\"\", rules_var_dict=None, is_merge=False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Create a rule given a cyclic temporal random walk.\n",
    "        The rule contains information about head relation, body relations,\n",
    "        variable constraints, confidence, rule support, and body support.\n",
    "        A rule is a dictionary with the content\n",
    "        {\"head_rel\": int, \"body_rels\": list, \"var_constraints\": list,\n",
    "         \"conf\": float, \"rule_supp\": int, \"body_supp\": int}\n",
    "\n",
    "        Parameters:\n",
    "            walk (dict): cyclic temporal random walk\n",
    "                         {\"entities\": list, \"relations\": list, \"timestamps\": list}\n",
    "\n",
    "        Returns:\n",
    "            rule (dict): created rule\n",
    "        \"\"\"\n",
    "\n",
    "        rule = dict()\n",
    "        rule[\"head_rel\"] = int(walk[\"relations\"][0])\n",
    "        rule[\"body_rels\"] = [self.inv_relation_id[x] for x in walk[\"relations\"][1:][::-1]]\n",
    "        rule[\"var_constraints\"] = self.define_var_constraints(walk[\"entities\"][1:][::-1])\n",
    "\n",
    "        rule_with_confidence = \"\"\n",
    "\n",
    "        if is_merge is True:\n",
    "            if rules_var_dict.get(rule_without_confidence) is None:\n",
    "                if rule not in self.found_rules:\n",
    "                    self.found_rules.append(rule.copy())\n",
    "                    (\n",
    "                        rule[\"conf\"],\n",
    "                        rule[\"rule_supp\"],\n",
    "                        rule[\"body_supp\"],\n",
    "                    ) = self.estimate_confidence(rule)\n",
    "\n",
    "                    tuple_key = str(rule)\n",
    "                    self.rule2confidence_dict[tuple_key] = rule[\"conf\"]\n",
    "                    rule_with_confidence = rule_without_confidence + \"&\" + str(rule[\"conf\"])\n",
    "\n",
    "                    rule[\"llm_confidence\"] = llm_confidence\n",
    "\n",
    "                    if rule[\"conf\"] or llm_confidence:\n",
    "                        self.num_individual += 1\n",
    "                        self.update_rules_dict(rule)\n",
    "                else:\n",
    "                    tuple_key = tuple(rule)\n",
    "                    confidence = self.rule2confidence_dict[tuple_key]\n",
    "                    rule_with_confidence = rule_without_confidence + \"&\" + confidence\n",
    "\n",
    "            else:\n",
    "                rule_var = rules_var_dict[rule_without_confidence]\n",
    "                rule_var[\"llm_confidence\"] = llm_confidence\n",
    "                temp_var = {}\n",
    "                temp_var[\"head_rel\"] = rule_var[\"head_rel\"]\n",
    "                temp_var[\"body_rels\"] = rule_var[\"body_rels\"]\n",
    "                temp_var[\"var_constraints\"] = rule_var[\"var_constraints\"]\n",
    "                if temp_var not in self.original_found_rules:\n",
    "                    self.original_found_rules.append(temp_var.copy())\n",
    "                    self.update_rules_dict(rule_var)\n",
    "                    self.num_shared += 1\n",
    "        else:\n",
    "            if rule not in self.found_rules:\n",
    "                tuple_key = str(rule)\n",
    "                self.found_rules.append(rule.copy())\n",
    "                (\n",
    "                    rule[\"conf\"],\n",
    "                    rule[\"rule_supp\"],\n",
    "                    rule[\"body_supp\"],\n",
    "                ) = self.estimate_confidence(rule)\n",
    "\n",
    "                self.rule2confidence_dict[tuple_key] = rule[\"conf\"]\n",
    "                rule_with_confidence = rule_without_confidence + \"&\" + str(rule[\"conf\"])\n",
    "\n",
    "                if rule[\"body_supp\"] == 0:\n",
    "                    rule[\"body_supp\"] = 2\n",
    "\n",
    "                rule[\"llm_confidence\"] = llm_confidence\n",
    "\n",
    "                if rule[\"conf\"] or llm_confidence:\n",
    "                    self.update_rules_dict(rule)\n",
    "            else:\n",
    "                tuple_key = str(rule)\n",
    "                confidence = self.rule2confidence_dict[tuple_key]\n",
    "                rule_with_confidence = rule_without_confidence + \"&\" + str(confidence)\n",
    "\n",
    "        return rule_with_confidence\n",
    "\n",
    "    def define_var_constraints(self, entities):\n",
    "        \"\"\"\n",
    "        Define variable constraints, i.e., state the indices of reoccurring entities in a walk.\n",
    "\n",
    "        Parameters:\n",
    "            entities (list): entities in the temporal walk\n",
    "\n",
    "        Returns:\n",
    "            var_constraints (list): list of indices for reoccurring entities\n",
    "        \"\"\"\n",
    "\n",
    "        var_constraints = []\n",
    "        for ent in set(entities):\n",
    "            all_idx = [idx for idx, x in enumerate(entities) if x == ent]\n",
    "            var_constraints.append(all_idx)\n",
    "        var_constraints = [x for x in var_constraints if len(x) > 1]\n",
    "\n",
    "        return sorted(var_constraints)\n",
    "\n",
    "    def estimate_confidence(self, rule, num_samples=2000, is_relax_time=False):\n",
    "        \"\"\"\n",
    "        Estimate the confidence of the rule by sampling bodies and checking the rule support.\n",
    "\n",
    "        Parameters:\n",
    "            rule (dict): rule\n",
    "                         {\"head_rel\": int, \"body_rels\": list, \"var_constraints\": list}\n",
    "            num_samples (int): number of samples\n",
    "\n",
    "        Returns:\n",
    "            confidence (float): confidence of the rule, rule_support/body_support\n",
    "            rule_support (int): rule support\n",
    "            body_support (int): body support\n",
    "        \"\"\"\n",
    "\n",
    "        if any(body_rel not in self.edges for body_rel in rule[\"body_rels\"]):\n",
    "            return 0, 0, 0\n",
    "\n",
    "        if rule[\"head_rel\"] not in self.edges:\n",
    "            return 0, 0, 0\n",
    "\n",
    "        all_bodies = []\n",
    "        for _ in range(num_samples):\n",
    "            sample_successful, body_ents_tss = self.sample_body(\n",
    "                rule[\"body_rels\"], rule[\"var_constraints\"], is_relax_time\n",
    "            )\n",
    "            if sample_successful:\n",
    "                all_bodies.append(body_ents_tss)\n",
    "\n",
    "        all_bodies.sort()\n",
    "        unique_bodies = list(x for x, _ in itertools.groupby(all_bodies))\n",
    "        body_support = len(unique_bodies)\n",
    "\n",
    "        confidence, rule_support = 0, 0\n",
    "        if body_support:\n",
    "            rule_support = self.calculate_rule_support(unique_bodies, rule[\"head_rel\"])\n",
    "            confidence = round(rule_support / body_support, 6)\n",
    "\n",
    "        return confidence, rule_support, body_support\n",
    "\n",
    "    def sample_body(self, body_rels, var_constraints, use_relax_time=False):\n",
    "        \"\"\"\n",
    "        Sample a walk according to the rule body.\n",
    "        The sequence of timesteps should be non-decreasing.\n",
    "\n",
    "        Parameters:\n",
    "            body_rels (list): relations in the rule body\n",
    "            var_constraints (list): variable constraints for the entities\n",
    "            use_relax_time (bool): whether to use relaxed time sampling\n",
    "\n",
    "        Returns:\n",
    "            sample_successful (bool): if a body has been successfully sampled\n",
    "            body_ents_tss (list): entities and timestamps (alternately entity and timestamp)\n",
    "                                  of the sampled body\n",
    "        \"\"\"\n",
    "\n",
    "        sample_successful = True\n",
    "        body_ents_tss = []\n",
    "        cur_rel = body_rels[0]\n",
    "        rel_edges = self.edges[cur_rel]\n",
    "        next_edge = rel_edges[np.random.choice(len(rel_edges))]\n",
    "        cur_ts = next_edge[3]\n",
    "        cur_node = next_edge[2]\n",
    "        body_ents_tss.append(next_edge[0])\n",
    "        body_ents_tss.append(cur_ts)\n",
    "        body_ents_tss.append(cur_node)\n",
    "\n",
    "        for cur_rel in body_rels[1:]:\n",
    "            next_edges = self.edges[cur_rel]\n",
    "            if use_relax_time:\n",
    "                mask = next_edges[:, 0] == cur_node\n",
    "            else:\n",
    "                mask = (next_edges[:, 0] == cur_node) * (next_edges[:, 3] >= cur_ts)\n",
    "\n",
    "            filtered_edges = next_edges[mask]\n",
    "\n",
    "            if len(filtered_edges):\n",
    "                next_edge = filtered_edges[np.random.choice(len(filtered_edges))]\n",
    "                cur_ts = next_edge[3]\n",
    "                cur_node = next_edge[2]\n",
    "                body_ents_tss.append(cur_ts)\n",
    "                body_ents_tss.append(cur_node)\n",
    "            else:\n",
    "                sample_successful = False\n",
    "                break\n",
    "\n",
    "        if sample_successful and var_constraints:\n",
    "            # Check variable constraints\n",
    "            body_var_constraints = self.define_var_constraints(body_ents_tss[::2])\n",
    "            if body_var_constraints != var_constraints:\n",
    "                sample_successful = False\n",
    "\n",
    "        return sample_successful, body_ents_tss\n",
    "\n",
    "    def calculate_rule_support(self, unique_bodies, head_rel):\n",
    "        \"\"\"\n",
    "        Calculate the rule support. Check for each body if there is a timestamp\n",
    "        (larger than the timestamps in the rule body) for which the rule head holds.\n",
    "\n",
    "        Parameters:\n",
    "            unique_bodies (list): bodies from self.sample_body\n",
    "            head_rel (int): head relation\n",
    "\n",
    "        Returns:\n",
    "            rule_support (int): rule support\n",
    "        \"\"\"\n",
    "\n",
    "        rule_support = 0\n",
    "        try:\n",
    "            head_rel_edges = self.edges[head_rel]\n",
    "        except Exception as e:\n",
    "            print(head_rel)\n",
    "        for body in unique_bodies:\n",
    "            mask = (\n",
    "                (head_rel_edges[:, 0] == body[0])\n",
    "                * (head_rel_edges[:, 2] == body[-1])\n",
    "                * (head_rel_edges[:, 3] > body[-2])\n",
    "            )\n",
    "\n",
    "            if True in mask:\n",
    "                rule_support += 1\n",
    "\n",
    "        return rule_support\n",
    "\n",
    "    def update_rules_dict(self, rule):\n",
    "        \"\"\"\n",
    "        Update the rules if a new rule has been found.\n",
    "\n",
    "        Parameters:\n",
    "            rule (dict): generated rule from self.create_rule\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            self.rules_dict[rule[\"head_rel\"]].append(rule)\n",
    "        except KeyError:\n",
    "            self.rules_dict[rule[\"head_rel\"]] = [rule]\n",
    "\n",
    "    def sort_rules_dict(self):\n",
    "        \"\"\"\n",
    "        Sort the found rules for each head relation by decreasing confidence.\n",
    "\n",
    "        Parameters:\n",
    "            None\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "\n",
    "        for rel in self.rules_dict:\n",
    "            self.rules_dict[rel] = sorted(self.rules_dict[rel], key=lambda x: x[\"conf\"], reverse=True)\n",
    "\n",
    "    def save_rules(self, dt, rule_lengths, num_walks, transition_distr, seed):\n",
    "        \"\"\"\n",
    "        Save all rules.\n",
    "\n",
    "        Parameters:\n",
    "            dt (str): time now\n",
    "            rule_lengths (list): rule lengths\n",
    "            num_walks (int): number of walks\n",
    "            transition_distr (str): transition distribution\n",
    "            seed (int): random seed\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "\n",
    "        rules_dict = {int(k): v for k, v in self.rules_dict.items()}\n",
    "        filename = \"{0}_r{1}_n{2}_{3}_s{4}_rules.json\".format(dt, rule_lengths, num_walks, transition_distr, seed)\n",
    "        filename = filename.replace(\" \", \"\")\n",
    "        with open(self.output_dir + filename, \"w\", encoding=\"utf-8\") as fout:\n",
    "            json.dump(rules_dict, fout)\n",
    "\n",
    "    def save_rules_verbalized(self, dt, rule_lengths, num_walks, transition_distr, seed, rel2idx, relation_regex):\n",
    "        \"\"\"\n",
    "        Save all rules in a human-readable format.\n",
    "\n",
    "        Parameters:\n",
    "            dt (str): time now\n",
    "            rule_lengths (list): rule lengths\n",
    "            num_walks (int): number of walks\n",
    "            transition_distr (str): transition distribution\n",
    "            seed (int): random seed\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "\n",
    "        output_original_dir = os.path.join(self.output_dir, \"original/\")\n",
    "        os.makedirs(output_original_dir, exist_ok=True)\n",
    "\n",
    "        rules_str, rules_var = self.verbalize_rules()\n",
    "        save_json_data(rules_var, output_original_dir + \"rules_var.json\")\n",
    "\n",
    "        filename = self.generate_filename(dt, rule_lengths, num_walks, transition_distr, seed, \"rules.txt\")\n",
    "        write_to_file(rules_str, self.output_dir + filename)\n",
    "\n",
    "        original_rule_txt = self.output_dir + filename\n",
    "        remove_filename = self.generate_filename(\n",
    "            dt, rule_lengths, num_walks, transition_distr, seed, \"remove_rules.txt\"\n",
    "        )\n",
    "\n",
    "        rule_id_content = self.remove_first_three_columns(self.output_dir + filename, self.output_dir + remove_filename)\n",
    "\n",
    "        self.parse_and_save_rules(remove_filename, list(rel2idx.keys()), relation_regex, \"closed_rel_paths.jsonl\")\n",
    "        self.parse_and_save_rules_with_names(\n",
    "            remove_filename, rel2idx, relation_regex, \"rules_name.json\", rule_id_content\n",
    "        )\n",
    "        self.parse_and_save_rules_with_ids(rule_id_content, rel2idx, relation_regex, \"rules_id.json\")\n",
    "\n",
    "        self.save_rule_name_with_confidence(\n",
    "            original_rule_txt,\n",
    "            relation_regex,\n",
    "            self.output_dir + \"relation_name_with_confidence.json\",\n",
    "            list(rel2idx.keys()),\n",
    "        )\n",
    "\n",
    "    def verbalize_rules(self):\n",
    "        rules_str = \"\"\n",
    "        rules_var = {}\n",
    "        for rel in self.rules_dict:\n",
    "            for rule in self.rules_dict[rel]:\n",
    "                single_rule = verbalize_rule(rule, self.id2relation) + \"\\n\"\n",
    "                part = re.split(r\"\\s+\", single_rule.strip())\n",
    "                rule_with_confidence = f\"{part[-1]}\"\n",
    "                rules_var[rule_with_confidence] = rule\n",
    "                rules_str += single_rule\n",
    "        return rules_str, rules_var\n",
    "\n",
    "    def generate_filename(self, dt, rule_lengths, num_walks, transition_distr, seed, suffix):\n",
    "        filename = f\"{dt}_r{rule_lengths}_n{num_walks}_{transition_distr}_s{seed}_{suffix}\"\n",
    "        return filename.replace(\" \", \"\")\n",
    "\n",
    "    def remove_first_three_columns(self, input_path, output_path):\n",
    "        rule_id_content = []\n",
    "        with open(input_path, \"r\") as input_file, open(output_path, \"w\", encoding=\"utf-8\") as output_file:\n",
    "            for line in input_file:\n",
    "                columns = line.split()\n",
    "                new_line = \" \".join(columns[3:])\n",
    "                new_line_for_rule_id = \" \".join(columns[3:]) + \"&\" + columns[0] + \"\\n\"\n",
    "                rule_id_content.append(new_line_for_rule_id)\n",
    "                output_file.write(new_line + \"\\n\")\n",
    "        return rule_id_content\n",
    "\n",
    "    def parse_and_save_rules(self, remove_filename, keys, relation_regex, output_filename):\n",
    "        output_file_path = os.path.join(self.output_dir, output_filename)\n",
    "        with open(self.output_dir + remove_filename, \"r\") as file:\n",
    "            lines = file.readlines()\n",
    "            converted_rules = parse_rules_for_path(lines, keys, relation_regex)\n",
    "        with open(output_file_path, \"w\") as file:\n",
    "            for head, paths in converted_rules.items():\n",
    "                json.dump({\"head\": head, \"paths\": paths}, file)\n",
    "                file.write(\"\\n\")\n",
    "        print(f\"Rules have been converted and saved to {output_file_path}\")\n",
    "        return converted_rules\n",
    "\n",
    "    def parse_and_save_rules_with_names(\n",
    "        self, remove_filename, rel2idx, relation_regex, output_filename, rule_id_content\n",
    "    ):\n",
    "        input_file_path = os.path.join(self.output_dir, remove_filename)\n",
    "        output_file_path = os.path.join(self.output_dir, output_filename)\n",
    "        with open(input_file_path, \"r\") as file:\n",
    "            rules_content = file.readlines()\n",
    "            rules_name_dict = parse_rules_for_name(rules_content, list(rel2idx.keys()), relation_regex)\n",
    "        with open(output_file_path, \"w\") as file:\n",
    "            json.dump(rules_name_dict, file, indent=4)\n",
    "        print(f\"Rules have been converted and saved to {output_file_path}\")\n",
    "\n",
    "    def parse_and_save_rules_with_ids(self, rule_id_content, rel2idx, relation_regex, output_filename):\n",
    "        output_file_path = os.path.join(self.output_dir, output_filename)\n",
    "        rules_id_dict = parse_rules_for_id(rule_id_content, rel2idx, relation_regex)\n",
    "        with open(output_file_path, \"w\") as file:\n",
    "            json.dump(rules_id_dict, file, indent=4)\n",
    "        print(f\"Rules have been converted and saved to {output_file_path}\")\n",
    "\n",
    "    def save_rule_name_with_confidence(self, file_path, relation_regex, out_file_path, relations):\n",
    "        rules_dict = {}\n",
    "        with open(file_path, \"r\") as fin:\n",
    "            rules = fin.readlines()\n",
    "            for rule in rules:\n",
    "                # Split the string by spaces to get the columns\n",
    "                columns = rule.split()\n",
    "\n",
    "                # Extract the first and fourth columns\n",
    "                first_column = columns[0]\n",
    "                fourth_column = \"\".join(columns[3:])\n",
    "                output = f\"{fourth_column}&{first_column}\"\n",
    "\n",
    "                regrex_list = fourth_column.split(\"<-\")\n",
    "                match = re.search(relation_regex, regrex_list[0])\n",
    "                if match:\n",
    "                    head = match[1].strip()\n",
    "                    if head not in relations:\n",
    "                        raise ValueError(f\"Not exist relation:{head}\")\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "                if head not in rules_dict:\n",
    "                    rules_dict[head] = []\n",
    "                rules_dict[head].append(output)\n",
    "        save_json_data(rules_dict, out_file_path)\n",
    "\n",
    "\n",
    "def parse_rules_for_path(lines, relations, relation_regex):\n",
    "    converted_rules = {}\n",
    "    for line in lines:\n",
    "        rule = line.strip()\n",
    "        if not rule:\n",
    "            continue\n",
    "        temp_rule = re.sub(r\"\\s*<-\\s*\", \"&\", rule)\n",
    "        regrex_list = temp_rule.split(\"&\")\n",
    "\n",
    "        head = \"\"\n",
    "        body_list = []\n",
    "        for idx, regrex_item in enumerate(regrex_list):\n",
    "            match = re.search(relation_regex, regrex_item)\n",
    "            if match:\n",
    "                rel_name = match.group(1).strip()\n",
    "                if rel_name not in relations:\n",
    "                    raise ValueError(f\"Not exist relation:{rel_name}\")\n",
    "                if idx == 0:\n",
    "                    head = rel_name\n",
    "                    paths = converted_rules.setdefault(head, [])\n",
    "                else:\n",
    "                    body_list.append(rel_name)\n",
    "\n",
    "        path = \"|\".join(body_list)\n",
    "        paths.append(path)\n",
    "\n",
    "    return converted_rules\n",
    "\n",
    "\n",
    "def parse_rules_for_name(lines, relations, relation_regex):\n",
    "    rules_dict = {}\n",
    "    for rule in lines:\n",
    "        temp_rule = re.sub(r\"\\s*<-\\s*\", \"&\", rule)\n",
    "        regrex_list = temp_rule.split(\"&\")\n",
    "        match = re.search(relation_regex, regrex_list[0])\n",
    "        if match:\n",
    "            head = match[1].strip()\n",
    "            if head not in relations:\n",
    "                raise ValueError(f\"Not exist relation:{head}\")\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        if head not in rules_dict:\n",
    "            rules_dict[head] = []\n",
    "        rules_dict[head].append(rule)\n",
    "\n",
    "    return rules_dict\n",
    "\n",
    "\n",
    "def parse_rules_for_id(rules, rel2idx, relation_regex):\n",
    "    rules_dict = {}\n",
    "    for rule in rules:\n",
    "        temp_rule = re.sub(r\"\\s*<-\\s*\", \"&\", rule)\n",
    "        regrex_list = temp_rule.split(\"&\")\n",
    "        match = re.search(relation_regex, regrex_list[0])\n",
    "        if match:\n",
    "            head = match[1].strip()\n",
    "            if head not in rel2idx:\n",
    "                raise ValueError(f\"Relation '{head}' not found in rel2idx\")\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        rule_id = rule2id(rule.rsplit(\"&\", 1)[0], rel2idx, relation_regex)\n",
    "        rule_id = rule_id + \"&\" + rule.rsplit(\"&\", 1)[1].strip()\n",
    "        rules_dict.setdefault(head, []).append(rule_id)\n",
    "    return rules_dict\n",
    "\n",
    "\n",
    "def rule2id(rule, relation2id, relation_regex):\n",
    "    temp_rule = copy.deepcopy(rule)\n",
    "    temp_rule = re.sub(r\"\\s*<-\\s*\", \"&\", temp_rule)\n",
    "    temp_rule = temp_rule.split(\"&\")\n",
    "    rule2id_str = \"\"\n",
    "\n",
    "    try:\n",
    "        for idx, _ in enumerate(temp_rule):\n",
    "            match = re.search(relation_regex, temp_rule[idx])\n",
    "            rel_name = match[1].strip()\n",
    "            subject = match[2].strip()\n",
    "            object = match[3].strip()\n",
    "            timestamp = match[4].strip()\n",
    "            rel_id = relation2id[rel_name]\n",
    "            full_id = f\"{rel_id}({subject},{object},{timestamp})\"\n",
    "            if idx == 0:\n",
    "                full_id = f\"{full_id}<-\"\n",
    "            else:\n",
    "                full_id = f\"{full_id}&\"\n",
    "\n",
    "            rule2id_str += f\"{full_id}\"\n",
    "    except KeyError as keyerror:\n",
    "        # 捕获异常并打印调用栈信息\n",
    "        traceback.print_exc()\n",
    "        raise ValueError(f\"KeyError: {keyerror}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"An error occurred: {rule}\")\n",
    "\n",
    "    return rule2id_str[:-1]\n",
    "\n",
    "\n",
    "def verbalize_rule(rule, id2relation):\n",
    "    \"\"\"\n",
    "    Verbalize the rule to be in a human-readable format.\n",
    "\n",
    "    Parameters:\n",
    "        rule (dict): rule from Rule_Learner.create_rule\n",
    "        id2relation (dict): mapping of index to relation\n",
    "\n",
    "    Returns:\n",
    "        rule_str (str): human-readable rule\n",
    "    \"\"\"\n",
    "\n",
    "    if rule[\"var_constraints\"]:\n",
    "        var_constraints = rule[\"var_constraints\"]\n",
    "        constraints = [x for sublist in var_constraints for x in sublist]\n",
    "        for i in range(len(rule[\"body_rels\"]) + 1):\n",
    "            if i not in constraints:\n",
    "                var_constraints.append([i])\n",
    "        var_constraints = sorted(var_constraints)\n",
    "    else:\n",
    "        var_constraints = [[x] for x in range(len(rule[\"body_rels\"]) + 1)]\n",
    "\n",
    "    rule_str = \"{0:8.6f}  {1:4}  {2:4}  {3}(X0,X{4},T{5})<-\"\n",
    "    obj_idx = [idx for idx in range(len(var_constraints)) if len(rule[\"body_rels\"]) in var_constraints[idx]][0]\n",
    "    rule_str = rule_str.format(\n",
    "        rule[\"conf\"],\n",
    "        rule[\"rule_supp\"],\n",
    "        rule[\"body_supp\"],\n",
    "        id2relation[rule[\"head_rel\"]],\n",
    "        obj_idx,\n",
    "        len(rule[\"body_rels\"]),\n",
    "    )\n",
    "\n",
    "    for i in range(len(rule[\"body_rels\"])):\n",
    "        sub_idx = [idx for idx in range(len(var_constraints)) if i in var_constraints[idx]][0]\n",
    "        obj_idx = [idx for idx in range(len(var_constraints)) if i + 1 in var_constraints[idx]][0]\n",
    "        rule_str += \"{0}(X{1},X{2},T{3})&\".format(id2relation[rule[\"body_rels\"][i]], sub_idx, obj_idx, i)\n",
    "\n",
    "    return rule_str[:-1]\n",
    "\n",
    "\n",
    "def rules_statistics(rules_dict):\n",
    "    \"\"\"\n",
    "    Show statistics of the rules.\n",
    "\n",
    "    Parameters:\n",
    "        rules_dict (dict): rules\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Number of relations with rules: \", len(rules_dict))  # Including inverse relations\n",
    "    print(\"Total number of rules: \", sum([len(v) for k, v in rules_dict.items()]))\n",
    "\n",
    "    lengths = []\n",
    "    for rel in rules_dict:\n",
    "        lengths += [len(x[\"body_rels\"]) for x in rules_dict[rel]]\n",
    "    rule_lengths = [(k, v) for k, v in Counter(lengths).items()]\n",
    "    print(\"Number of rules by length: \", sorted(rule_lengths))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal Walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def initialize_temporal_walk(version_id, data, transition_distr):\n",
    "    idx_map = {\n",
    "        \"all\": np.array(data.train_idx.tolist() + data.valid_idx.tolist() + data.test_idx.tolist()),\n",
    "        \"train_valid\": np.array(data.train_idx.tolist() + data.valid_idx.tolist()),\n",
    "        \"train\": np.array(data.train_idx.tolist()),\n",
    "        \"test\": np.array(data.test_idx.tolist()),\n",
    "        \"valid\": np.array(data.valid_idx.tolist()),\n",
    "    }\n",
    "    return Temporal_Walk(idx_map[version_id], data.inv_relation_id, transition_distr)\n",
    "\n",
    "\n",
    "class Temporal_Walk(object):\n",
    "    def __init__(self, learn_data, inv_relation_id, transition_distr):\n",
    "        \"\"\"\n",
    "        Initialize temporal random walk object.\n",
    "\n",
    "        Parameters:\n",
    "            learn_data (np.ndarray): data on which the rules should be learned\n",
    "            inv_relation_id (dict): mapping of relation to inverse relation\n",
    "            transition_distr (str): transition distribution\n",
    "                                    \"unif\" - uniform distribution\n",
    "                                    \"exp\"  - exponential distribution\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "\n",
    "        self.learn_data = learn_data\n",
    "        self.inv_relation_id = inv_relation_id\n",
    "        self.transition_distr = transition_distr\n",
    "        self.neighbors = store_neighbors(learn_data)\n",
    "        self.edges = store_edges(learn_data)\n",
    "\n",
    "    def sample_start_edge(self, rel_idx):\n",
    "        \"\"\"\n",
    "        Define start edge distribution.\n",
    "\n",
    "        Parameters:\n",
    "            rel_idx (int): relation index\n",
    "\n",
    "        Returns:\n",
    "            start_edge (np.ndarray): start edge\n",
    "        \"\"\"\n",
    "\n",
    "        rel_edges = self.edges[rel_idx]\n",
    "        start_edge = rel_edges[np.random.choice(len(rel_edges))]\n",
    "\n",
    "        return start_edge\n",
    "\n",
    "    def sample_next_edge(self, filtered_edges, cur_ts):\n",
    "        \"\"\"\n",
    "        Define next edge distribution.\n",
    "\n",
    "        Parameters:\n",
    "            filtered_edges (np.ndarray): filtered (according to time) edges\n",
    "            cur_ts (int): current timestamp\n",
    "\n",
    "        Returns:\n",
    "            next_edge (np.ndarray): next edge\n",
    "        \"\"\"\n",
    "\n",
    "        if self.transition_distr == \"unif\":\n",
    "            next_edge = filtered_edges[np.random.choice(len(filtered_edges))]\n",
    "        elif self.transition_distr == \"exp\":\n",
    "            tss = filtered_edges[:, 3]\n",
    "            prob = np.exp(tss - cur_ts)\n",
    "            try:\n",
    "                prob = prob / np.sum(prob)\n",
    "                next_edge = filtered_edges[np.random.choice(range(len(filtered_edges)), p=prob)]\n",
    "            except ValueError:  # All timestamps are far away\n",
    "                next_edge = filtered_edges[np.random.choice(len(filtered_edges))]\n",
    "\n",
    "        return next_edge\n",
    "\n",
    "    def transition_step(self, cur_node, cur_ts, prev_edge, start_node, step, L, target_cur_ts=None):\n",
    "        \"\"\"\n",
    "        Sample a neighboring edge given the current node and timestamp.\n",
    "        In the second step (step == 1), the next timestamp should be smaller than the current timestamp.\n",
    "        In the other steps, the next timestamp should be smaller than or equal to the current timestamp.\n",
    "        In the last step (step == L-1), the edge should connect to the source of the walk (cyclic walk).\n",
    "        It is not allowed to go back using the inverse edge.\n",
    "\n",
    "        Parameters:\n",
    "            cur_node (int): current node\n",
    "            cur_ts (int): current timestamp\n",
    "            prev_edge (np.ndarray): previous edge\n",
    "            start_node (int): start node\n",
    "            step (int): number of current step\n",
    "            L (int): length of random walk\n",
    "            target_cur_ts (int, optional): target current timestamp for relaxed time. Defaults to cur_ts.\n",
    "\n",
    "        Returns:\n",
    "            next_edge (np.ndarray): next edge\n",
    "        \"\"\"\n",
    "\n",
    "        next_edges = self.neighbors[cur_node]\n",
    "        if target_cur_ts is None:\n",
    "            target_cur_ts = cur_ts\n",
    "\n",
    "        if step == 1:  # The next timestamp should be smaller than the current timestamp\n",
    "            filtered_edges = next_edges[next_edges[:, 3] < target_cur_ts]\n",
    "        else:  # The next timestamp should be smaller than or equal to the current timestamp\n",
    "            filtered_edges = next_edges[next_edges[:, 3] <= target_cur_ts]\n",
    "            # Delete inverse edge\n",
    "            inv_edge = [\n",
    "                cur_node,\n",
    "                self.inv_relation_id[prev_edge[1]],\n",
    "                prev_edge[0],\n",
    "                cur_ts,\n",
    "            ]\n",
    "            row_idx = np.where(np.all(filtered_edges == inv_edge, axis=1))\n",
    "            filtered_edges = np.delete(filtered_edges, row_idx, axis=0)\n",
    "\n",
    "        if step == L - 1:  # Find an edge that connects to the source of the walk\n",
    "            filtered_edges = filtered_edges[filtered_edges[:, 2] == start_node]\n",
    "\n",
    "        if len(filtered_edges):\n",
    "            next_edge = self.sample_next_edge(filtered_edges, cur_ts)\n",
    "        else:\n",
    "            next_edge = []\n",
    "\n",
    "        return next_edge\n",
    "\n",
    "    def transition_step_with_relax_time(self, cur_node, cur_ts, prev_edge, start_node, step, L, target_cur_ts):\n",
    "        \"\"\"\n",
    "        Wrapper for transition_step with relaxed time handling.\n",
    "\n",
    "        Parameters:\n",
    "            cur_node (int): current node\n",
    "            cur_ts (int): current timestamp\n",
    "            prev_edge (np.ndarray): previous edge\n",
    "            start_node (int): start node\n",
    "            step (int): number of current step\n",
    "            L (int): length of random walk\n",
    "            target_cur_ts (int): target current timestamp for relaxed time\n",
    "\n",
    "        Returns:\n",
    "            next_edge (np.ndarray): next edge\n",
    "        \"\"\"\n",
    "        return self.transition_step(cur_node, cur_ts, prev_edge, start_node, step, L, target_cur_ts)\n",
    "\n",
    "    def sample_walk(self, L, rel_idx, use_relax_time=False):\n",
    "        \"\"\"\n",
    "        Try to sample a cyclic temporal random walk of length L (for a rule of length L-1).\n",
    "\n",
    "        Parameters:\n",
    "            L (int): length of random walk\n",
    "            rel_idx (int): relation index\n",
    "            use_relax_time (bool): whether to use relaxed time sampling\n",
    "\n",
    "        Returns:\n",
    "            walk_successful (bool): if a cyclic temporal random walk has been successfully sampled\n",
    "            walk (dict): information about the walk (entities, relations, timestamps)\n",
    "        \"\"\"\n",
    "\n",
    "        walk_successful = True\n",
    "        walk = dict()\n",
    "        prev_edge = self.sample_start_edge(rel_idx)\n",
    "        start_node = prev_edge[0]\n",
    "        cur_node = prev_edge[2]\n",
    "        cur_ts = prev_edge[3]\n",
    "        target_cur_ts = cur_ts\n",
    "        walk[\"entities\"] = [start_node, cur_node]\n",
    "        walk[\"relations\"] = [prev_edge[1]]\n",
    "        walk[\"timestamps\"] = [cur_ts]\n",
    "\n",
    "        for step in range(1, L):\n",
    "            if use_relax_time:\n",
    "                next_edge = self.transition_step_with_relax_time(\n",
    "                    cur_node, cur_ts, prev_edge, start_node, step, L, target_cur_ts\n",
    "                )\n",
    "            else:\n",
    "                next_edge = self.transition_step(cur_node, cur_ts, prev_edge, start_node, step, L)\n",
    "\n",
    "            if len(next_edge):\n",
    "                cur_node = next_edge[2]\n",
    "                cur_ts = next_edge[3]\n",
    "                walk[\"relations\"].append(next_edge[1])\n",
    "                walk[\"entities\"].append(cur_node)\n",
    "                walk[\"timestamps\"].append(cur_ts)\n",
    "                prev_edge = next_edge\n",
    "            else:  # No valid neighbors (due to temporal or cyclic constraints)\n",
    "                walk_successful = False\n",
    "                break\n",
    "\n",
    "        return walk_successful, walk\n",
    "\n",
    "\n",
    "def store_neighbors(quads):\n",
    "    \"\"\"\n",
    "    Store all neighbors (outgoing edges) for each node.\n",
    "\n",
    "    Parameters:\n",
    "        quads (np.ndarray): indices of quadruples\n",
    "\n",
    "    Returns:\n",
    "        neighbors (dict): neighbors for each node\n",
    "    \"\"\"\n",
    "\n",
    "    # 将 quads 转换为 DataFrame\n",
    "    df = pd.DataFrame(quads, columns=[\"head\", \"relation\", \"target\", \"timestamp\"])\n",
    "\n",
    "    # 按 'node' 列分组，并将每组转换为数组\n",
    "    neighbors = {node: group.values for node, group in df.groupby(\"head\")}\n",
    "\n",
    "    return neighbors\n",
    "\n",
    "\n",
    "def store_edges(quads):\n",
    "    \"\"\"\n",
    "    Store all edges for each relation.\n",
    "\n",
    "    Parameters:\n",
    "        quads (np.ndarray): indices of quadruples\n",
    "\n",
    "    Returns:\n",
    "        edges (dict): edges for each relation\n",
    "    \"\"\"\n",
    "\n",
    "    edges = dict()\n",
    "    relations = list(set(quads[:, 1]))\n",
    "    for rel in relations:\n",
    "        edges[rel] = quads[quads[:, 1] == rel]\n",
    "\n",
    "    return edges\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grapher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "class Grapher(object):\n",
    "    def __init__(self, dataset_dir, args=None, test_mask=None):\n",
    "        \"\"\"\n",
    "        Store information about the graph (train/valid/test set).\n",
    "        Add corresponding inverse quadruples to the data.\n",
    "\n",
    "        Parameters:\n",
    "            dataset_dir (str): path to the graph dataset directory\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "\n",
    "        self.args = args\n",
    "        self.dataset_dir = dataset_dir\n",
    "        self.entity2id = load_json_data(os.path.join(self.dataset_dir, \"entity2id.json\"))\n",
    "        self.relation2id_old = load_json_data(os.path.join(self.dataset_dir, \"relation2id.json\"))\n",
    "        self.relation2id = self.relation2id_old.copy()\n",
    "        counter = len(self.relation2id_old)\n",
    "        for relation in self.relation2id_old:\n",
    "            self.relation2id[\"inv_\" + relation] = counter  # Inverse relation\n",
    "            counter += 1\n",
    "        self.ts2id = load_json_data(os.path.join(self.dataset_dir, \"ts2id.json\"))\n",
    "        self.id2entity = dict([(v, k) for k, v in self.entity2id.items()])\n",
    "        self.id2relation = dict([(v, k) for k, v in self.relation2id.items()])\n",
    "        self.id2ts = dict([(v, k) for k, v in self.ts2id.items()])\n",
    "\n",
    "        self.inv_relation_id = dict()\n",
    "        num_relations = len(self.relation2id_old)\n",
    "        for i in range(num_relations):\n",
    "            self.inv_relation_id[i] = i + num_relations\n",
    "        for i in range(num_relations, num_relations * 2):\n",
    "            self.inv_relation_id[i] = i % num_relations\n",
    "\n",
    "        self.train_idx = self.create_store(\"train.txt\")\n",
    "        self.valid_idx = self.create_store(\"valid.txt\")\n",
    "        self.test_idx = self.create_store(\"test.txt\")\n",
    "\n",
    "        if test_mask is not None:\n",
    "            mask = (self.test_idx[:, 3] >= test_mask[0]) * (self.test_idx[:, 3] <= test_mask[1])\n",
    "            self.test_idx = self.test_idx[mask]\n",
    "\n",
    "        if self.args is not None:\n",
    "            if self.args[\"bgkg\"] == \"all\":\n",
    "                self.all_idx = np.vstack((self.train_idx, self.valid_idx, self.test_idx))\n",
    "            elif self.args[\"bgkg\"] == \"train\":\n",
    "                self.all_idx = self.train_idx\n",
    "            elif self.args[\"bgkg\"] == \"valid\":\n",
    "                self.all_idx = self.valid_idx\n",
    "            elif self.args[\"bgkg\"] == \"test\":\n",
    "                self.all_idx = self.test_idx\n",
    "            elif self.args[\"bgkg\"] == \"train_valid\":\n",
    "                self.all_idx = np.vstack((self.train_idx, self.valid_idx))\n",
    "            elif self.args[\"bgkg\"] == \"train_test\":\n",
    "                self.all_idx = np.vstack((self.train_idx, self.test_idx))\n",
    "            elif self.args[\"bgkg\"] == \"valid_test\":\n",
    "                self.all_idx = np.vstack((self.valid_idx, self.test_idx))\n",
    "\n",
    "        print(\"Grapher initialized.\")\n",
    "\n",
    "    def create_store(self, file):\n",
    "        \"\"\"\n",
    "        Store the quadruples from the file as indices.\n",
    "        The quadruples in the file should be in the format \"subject\\trelation\\tobject\\ttimestamp\\n\".\n",
    "\n",
    "        Parameters:\n",
    "            file (str): file name\n",
    "\n",
    "        Returns:\n",
    "            store_idx (np.ndarray): indices of quadruples\n",
    "        \"\"\"\n",
    "\n",
    "        with open(os.path.join(self.dataset_dir, file), \"r\", encoding=\"utf-8\") as f:\n",
    "            quads = f.readlines()\n",
    "        store = self.split_quads(quads)\n",
    "        store_idx = self.map_to_idx(store)\n",
    "        store_idx = self.add_inverses(store_idx)\n",
    "\n",
    "        return store_idx\n",
    "\n",
    "    def split_quads(self, quads):\n",
    "        \"\"\"\n",
    "        Split quadruples into a list of strings.\n",
    "\n",
    "        Parameters:\n",
    "            quads (list): list of quadruples\n",
    "                          Each quadruple has the form \"subject\\trelation\\tobject\\ttimestamp\\n\".\n",
    "\n",
    "        Returns:\n",
    "            split_q (list): list of quadruples\n",
    "                            Each quadruple has the form [subject, relation, object, timestamp].\n",
    "        \"\"\"\n",
    "\n",
    "        split_q = []\n",
    "        for quad in quads:\n",
    "            split_q.append(quad[:-1].split(\"\\t\"))\n",
    "\n",
    "        return split_q\n",
    "\n",
    "    def map_to_idx(self, quads):\n",
    "        \"\"\"\n",
    "        Map quadruples to their indices.\n",
    "\n",
    "        Parameters:\n",
    "            quads (list): list of quadruples\n",
    "                          Each quadruple has the form [subject, relation, object, timestamp].\n",
    "\n",
    "        Returns:\n",
    "            quads (np.ndarray): indices of quadruples\n",
    "        \"\"\"\n",
    "\n",
    "        subs = [self.entity2id[x[0]] for x in quads]\n",
    "        rels = [self.relation2id[x[1]] for x in quads]\n",
    "        objs = [self.entity2id[x[2]] for x in quads]\n",
    "        tss = [self.ts2id[x[3]] for x in quads]\n",
    "        quads = np.column_stack((subs, rels, objs, tss))\n",
    "\n",
    "        return quads\n",
    "\n",
    "    def add_inverses(self, quads_idx):\n",
    "        \"\"\"\n",
    "        Add the inverses of the quadruples as indices.\n",
    "\n",
    "        Parameters:\n",
    "            quads_idx (np.ndarray): indices of quadruples\n",
    "\n",
    "        Returns:\n",
    "            quads_idx (np.ndarray): indices of quadruples along with the indices of their inverses\n",
    "        \"\"\"\n",
    "\n",
    "        subs = quads_idx[:, 2]\n",
    "        rels = [self.inv_relation_id[x] for x in quads_idx[:, 1]]\n",
    "        objs = quads_idx[:, 0]\n",
    "        tss = quads_idx[:, 3]\n",
    "        inv_quads_idx = np.column_stack((subs, rels, objs, tss))\n",
    "        quads_idx = np.vstack((quads_idx, inv_quads_idx))\n",
    "\n",
    "        return quads_idx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import glob\n",
    "import os.path\n",
    "import time\n",
    "import traceback\n",
    "from difflib import get_close_matches\n",
    "\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "from multiprocessing.pool import ThreadPool\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "\n",
    "\n",
    "def read_paths(path):\n",
    "    results = []\n",
    "    with open(path, \"r\") as f:\n",
    "        for line in f:\n",
    "            results.append(json.loads(line.strip()))\n",
    "    return results\n",
    "\n",
    "\n",
    "def build_prompt(head, prompt_dict):\n",
    "    definition = prompt_dict[\"common\"][\"definition\"].format(head=head)\n",
    "    context = prompt_dict[\"common\"][\"context\"].format(head=head)\n",
    "    chain = prompt_dict[\"common\"][\"chain\"]\n",
    "    predict = prompt_dict[\"common\"][\"predict\"].format(head=head)\n",
    "    return_rules = prompt_dict[\"common\"][\"return\"]\n",
    "    return definition + context + chain, predict, return_rules\n",
    "\n",
    "\n",
    "def build_prompt_for_zero(head, prompt_dict):\n",
    "    context = prompt_dict[\"zero\"][\"context\"].format(head=head)\n",
    "    predict = prompt_dict[\"zero\"][\"predict\"].format(head=head)\n",
    "    return_rules = prompt_dict[\"zero\"][\"return\"]\n",
    "    return context, predict, return_rules\n",
    "\n",
    "\n",
    "def build_prompt_based_high(head, candidate_rels, is_zero, args, prompt_dict):\n",
    "    # head = clean_symbol_in_rel(head)\n",
    "    chain_defination = prompt_dict[\"chain_defination_for_high\"].format(head=head)\n",
    "\n",
    "    context = prompt_dict[\"iteration_context_for_high\"].format(head=head)\n",
    "\n",
    "    high_quality_context = prompt_dict[\"example_for_high\"]\n",
    "    # predict = prompt_dict['interaction_finale_predict_for_high'].format(head=head, k=20)\n",
    "    predict = prompt_dict[\"interaction_finale_predict_for_high\"].format(head=head)\n",
    "    return_rules = prompt_dict[\"return_for_high\"]\n",
    "\n",
    "    return chain_defination + context + high_quality_context, predict, return_rules\n",
    "\n",
    "\n",
    "def build_prompt_based_low(head, candidate_rels, is_zero, args, prompt_dict):\n",
    "    # head = clean_symbol_in_rel(head)\n",
    "    # chain_defination = prompt_dict['iteration']['defination']['chain'].format(head=head)\n",
    "\n",
    "    context = prompt_dict[\"iteration\"][\"context\"].format(head=head)\n",
    "    low_quality_context = prompt_dict[\"iteration\"][\"low_quality_example\"]\n",
    "    sampled_rules = prompt_dict[\"iteration\"][\"sampled_rules\"]\n",
    "    predict = prompt_dict[\"iteration\"][\"predict\"].format(head=head)\n",
    "    return_rules = prompt_dict[\"iteration\"][\"return\"]\n",
    "    return context + low_quality_context, sampled_rules, predict, return_rules\n",
    "\n",
    "\n",
    "# def build_prompt_for_unknown(head, candidate_rels, is_zero, args, prompt_dict): # UNUSED\n",
    "#     # head = clean_symbol_in_rel(head)\n",
    "#     chain_defination = prompt_dict[\"chain_defination\"].format(head=head)\n",
    "\n",
    "#     context = prompt_dict[\"unknown_relation_context\"].format(head=head)\n",
    "\n",
    "#     predict = prompt_dict[\"unknown_relation_final_predict\"].format(head=head)\n",
    "#     return_rules = prompt_dict[\"unknown_relation_return\"]\n",
    "#     return chain_defination + context, predict, return_rules\n",
    "\n",
    "\n",
    "def get_rule_format(head, path, kg_rules_path):\n",
    "    kg_rules_dict = load_json_data(kg_rules_path)\n",
    "    if kg_rules_dict is None:\n",
    "        path_list = []\n",
    "        # head = clean_symbol_in_rel(head)\n",
    "        for p in path:\n",
    "            context = f\"{head}(X,Y) <-- \"\n",
    "            for i, r in enumerate(p.split(\"|\")):\n",
    "                # r = clean_symbol_in_rel(r)\n",
    "                if i == 0:\n",
    "                    first = \"X\"\n",
    "                else:\n",
    "                    first = f\"Z_{i}\"\n",
    "                if i == len(p.split(\"|\")) - 1:\n",
    "                    last = \"Y\"\n",
    "                else:\n",
    "                    last = f\"Z_{i + 1}\"\n",
    "                context += f\"{r}({first}, {last}) & \"\n",
    "            context = context.strip(\" & \")\n",
    "            path_list.append(context)\n",
    "        return path_list\n",
    "    else:\n",
    "        return kg_rules_dict[head]\n",
    "\n",
    "\n",
    "def generate_rule(\n",
    "    row, rdict, rule_path, kg_rules_path, model, args, relation_regex, similiary_rel_dict, prompt_info_dict\n",
    "):\n",
    "    relation2id = rdict.rel2idx\n",
    "    head = row[\"head\"]\n",
    "    paths = row[\"paths\"]\n",
    "    head_id = relation2id[head]\n",
    "\n",
    "    head_formate = head\n",
    "    if args.is_rel_name is True:\n",
    "        all_rels = list(relation2id.keys())\n",
    "        candidate_rels = \", \".join(all_rels)\n",
    "        head_formate = head\n",
    "    else:\n",
    "        all_rels = list(relation2id.values())\n",
    "        str_list = [str(item) for item in all_rels]\n",
    "        candidate_rels = \", \".join(str_list)\n",
    "        head_formate = head_id\n",
    "\n",
    "    # Raise an error if k=0 for zero-shot setting\n",
    "    if args.k == 0 and args.is_zero:\n",
    "        raise NotImplementedError(f\"\"\"Cannot implement for zero-shot(f=0) and generate zero(k=0) rules.\"\"\")\n",
    "    # Build prompt excluding rules\n",
    "    fixed_context, predict, return_rules = build_prompt(head_formate, prompt_info_dict)\n",
    "    current_prompt = fixed_context + predict + return_rules\n",
    "\n",
    "    if args.is_zero:  # For zero-shot setting\n",
    "        with open(os.path.join(rule_path, f\"{head}_zero_shot.query\"), \"w\") as f:\n",
    "            f.write(current_prompt + \"\\n\")\n",
    "            f.close()\n",
    "        if not args.dry_run:\n",
    "            response = query(current_prompt, model)\n",
    "            with open(os.path.join(rule_path, f\"{head}_zero_shot.txt\"), \"w\") as f:\n",
    "                f.write(response + \"\\n\")\n",
    "                f.close()\n",
    "    else:  # For few-shot setting\n",
    "        path_content_list = get_rule_format(head, paths, kg_rules_path)\n",
    "        file_name = head.replace(\"/\", \"-\")\n",
    "        with (\n",
    "            open(os.path.join(rule_path, f\"{file_name}.txt\"), \"w\") as rule_file,\n",
    "            open(os.path.join(rule_path, f\"{file_name}.query\"), \"w\") as query_file,\n",
    "        ):\n",
    "            rule_file.write(f\"Rule_head: {head}\\n\")\n",
    "            for i in range(args.l):\n",
    "\n",
    "                if args.select_with_confidence is True:\n",
    "                    sorted_list = sorted(path_content_list, key=lambda x: float(x.split(\"&\")[-1]), reverse=True)\n",
    "                    # few_shot_samples = sorted_list[:args.f]\n",
    "                    new_shot_samples = [item for item in sorted_list if float(item.split(\"&\")[-1]) > 0.01]\n",
    "                    if len(new_shot_samples) >= args.f:\n",
    "                        few_shot_samples = new_shot_samples\n",
    "                    else:\n",
    "                        few_shot_samples = sorted_list[: args.f]\n",
    "                else:\n",
    "                    few_shot_samples = random.sample(path_content_list, min(args.f, len(path_content_list)))\n",
    "                    relation_set = set()\n",
    "                    for rule in few_shot_samples:\n",
    "                        rule_body = rule.split(\"<-\")[-1]\n",
    "                        matches = re.findall(relation_regex, rule_body)\n",
    "                        for match in matches:\n",
    "                            relation = match[0]\n",
    "                            relation_set.update([relation])\n",
    "\n",
    "                    similiary_rel_set = set()\n",
    "                    for rel_name in relation_set:\n",
    "                        similiary_rel_set.update(similiary_rel_dict[rel_name])\n",
    "\n",
    "                    condicate = similiary_rel_set.union(relation_set)\n",
    "\n",
    "                    formatted_string = \";\".join([f\"{name}\" for name in condicate])\n",
    "\n",
    "                return_rules = return_rules.format(candidate_rels=formatted_string)\n",
    "\n",
    "                temp_current_prompt = fixed_context + predict + return_rules\n",
    "\n",
    "                few_shot_paths = check_prompt_length(temp_current_prompt, few_shot_samples, model)\n",
    "\n",
    "                if not few_shot_paths:\n",
    "                    raise ValueError(\"few_shot_paths is empty, head:{}\".format(head))\n",
    "\n",
    "                few_shot_paths = few_shot_paths + \"\\n\\n\"\n",
    "\n",
    "                return_rules = \"\\n\\n\" + return_rules\n",
    "\n",
    "                prompt = fixed_context + few_shot_paths + predict + return_rules\n",
    "                query_file.write(f\"Sample {i + 1} time: \\n\")\n",
    "                query_file.write(prompt + \"\\n\")\n",
    "                if not args.dry_run:\n",
    "                    response = model.generate_sentence(prompt)\n",
    "                    if response is not None:\n",
    "                        # tqdm.write(\"Response: \\n{}\".format(response))\n",
    "                        rule_file.write(f\"Sample {i + 1} time: \\n\")\n",
    "                        rule_file.write(response + \"\\n\")\n",
    "                    else:\n",
    "                        with open(os.path.join(rule_path, f\"fail_{file_name}.txt\"), \"w\") as fail_rule_file:\n",
    "                            fail_rule_file.write(prompt + \"\\n\")\n",
    "                        break\n",
    "\n",
    "\n",
    "def generate_rule_for_zero(head, rdict, rule_path, model, args, prompt_info_zero_dict):\n",
    "    relation2id = rdict.rel2idx\n",
    "    all_rels = list(relation2id.keys())\n",
    "\n",
    "    fixed_context, predict, return_rules_template = build_prompt_for_zero(head, prompt_info_zero_dict)\n",
    "    return_rules = return_rules_template.format(candidate_rels=all_rels)\n",
    "    current_prompt = fixed_context + predict + return_rules\n",
    "\n",
    "    # 정의 파일 경로\n",
    "    query_file_path = os.path.join(rule_path, f\"{head}.query\")\n",
    "    txt_file_path = os.path.join(rule_path, f\"{head}.txt\")\n",
    "    fail_file_path = os.path.join(rule_path, f\"fail_{head}.txt\")\n",
    "\n",
    "    try:\n",
    "        with open(query_file_path, \"w\") as fout_zero_query, open(txt_file_path, \"w\") as fout_zero_txt:\n",
    "            for i in range(args.l):\n",
    "                entry = f\"Sample {i + 1} time:\\n\"\n",
    "                fout_zero_query.write(entry + current_prompt + \"\\n\")\n",
    "\n",
    "                response = query(current_prompt, model)\n",
    "                if response:\n",
    "                    fout_zero_txt.write(entry + response + \"\\n\")\n",
    "                else:\n",
    "                    raise ValueError(\"Failed to generate response.\")\n",
    "    except ValueError as e:\n",
    "        with open(fail_file_path, \"w\") as fail_rule_file:\n",
    "            fail_rule_file.write(current_prompt + \"\\n\")\n",
    "        print(e)  # Optional: Handle the exception as needed\n",
    "\n",
    "\n",
    "def extract_and_expand_relations(args, path_content_list, similiary_rel_dict, relation_regex):\n",
    "    \"\"\"\n",
    "    제공된 규칙 샘플에서 일정 수의 규칙을 무작위로 추출하고 이러한 규칙의 관계 집합을 확장합니다.\n",
    "\n",
    "    :param args: 명명 공간, f 매개변수를 포함하여 매개변수를 포함합니다.\n",
    "    :param path_content_list: 규칙 목록을 포함합니다.\n",
    "    :param similiary_rel_dict: 관계 이름을 키로 하고 해당 관계와 유사한 관계 집합을 값으로 하는 사전입니다.\n",
    "    :param relation_regex: 규칙에서 관계를 추출하는 데 사용되는 정규 표현식입니다.\n",
    "    :return: 원본 관계와 유사한 관계의 집합을 반환합니다.\n",
    "    \"\"\"\n",
    "    # 무작위로 샘플 추출\n",
    "    few_shot_samples = random.sample(path_content_list, min(args.f, len(path_content_list)))\n",
    "\n",
    "    # 추출된 샘플에서 관계 추출\n",
    "    relation_set = set()\n",
    "    for rule in few_shot_samples:\n",
    "        rule_body = rule.split(\"<-\")[-1]\n",
    "        matches = re.findall(relation_regex, rule_body)\n",
    "        for match in matches:\n",
    "            relation = match[0]\n",
    "            relation_set.update([relation])\n",
    "\n",
    "    # 찾은 관계 집합 확장, 유사한 관계 포함\n",
    "    similiary_rel_set = set()\n",
    "    for rel_name in relation_set:\n",
    "        similiary_rel_set.update(similiary_rel_dict[rel_name])\n",
    "\n",
    "    # 원본 관계 집합과 유사한 관계 집합 합치기\n",
    "    condicate = similiary_rel_set.union(relation_set)\n",
    "\n",
    "    return condicate\n",
    "\n",
    "\n",
    "def generate_rule_for_iteration_by_multi_thread(\n",
    "    row,\n",
    "    rdict,\n",
    "    rule_path,\n",
    "    kg_rules_path,\n",
    "    model,\n",
    "    args,\n",
    "    relation_regex,\n",
    "    similiary_rel_dict,\n",
    "    kg_rules_path_with_valid,\n",
    "    prompt_dict_low,\n",
    "    prompt_dict_high,\n",
    "):\n",
    "    relation2id = rdict.rel2idx\n",
    "    head = row[\"head\"]\n",
    "    rules = row[\"rules\"]\n",
    "\n",
    "    head_id = relation2id[head]\n",
    "\n",
    "    valid_rules_name = load_json_data(kg_rules_path_with_valid)\n",
    "\n",
    "    if args.is_rel_name is True:\n",
    "        all_rels = list(relation2id.keys())\n",
    "        candidate_rels = \", \".join(all_rels)\n",
    "        head_formate = head\n",
    "    else:\n",
    "        all_rels = list(relation2id.values())\n",
    "        str_list = [str(item) for item in all_rels]\n",
    "        candidate_rels = \", \".join(str_list)\n",
    "        head_formate = head_id\n",
    "\n",
    "    # Raise an error if k=0 for zero-shot setting\n",
    "    if args.k == 0 and args.is_zero:\n",
    "        raise NotImplementedError(f\"\"\"Cannot implement for zero-shot(f=0) and generate zero(k=0) rules.\"\"\")\n",
    "\n",
    "    if args.based_rule_type == \"low\":\n",
    "        # Build prompt excluding rules\n",
    "        fixed_context, sampled_rules, predict, return_rules = build_prompt_based_low(\n",
    "            head_formate, candidate_rels, args.is_zero, args, prompt_dict_low\n",
    "        )\n",
    "    else:\n",
    "        fixed_context, predict, return_rules = build_prompt_based_high(\n",
    "            head_formate, candidate_rels, args.is_zero, args, prompt_dict_high\n",
    "        )\n",
    "\n",
    "    kg_rules_dict = load_json_data(kg_rules_path)\n",
    "    path_content_list = kg_rules_dict.get(head, None)\n",
    "    file_name = head.replace(\"/\", \"-\")\n",
    "    with (\n",
    "        open(os.path.join(rule_path, f\"{file_name}.txt\"), \"w\") as rule_file,\n",
    "        open(os.path.join(rule_path, f\"{file_name}.query\"), \"w\") as query_file,\n",
    "    ):\n",
    "        rule_file.write(f\"Rule_head: {head}\\n\")\n",
    "        for i in range(args.second):\n",
    "\n",
    "            if path_content_list is not None:\n",
    "                condicate = extract_and_expand_relations(args, path_content_list, similiary_rel_dict, relation_regex)\n",
    "            else:\n",
    "                condicate = set(all_rels)\n",
    "\n",
    "            temp_valid_rules = valid_rules_name.get(head, \"\")\n",
    "            valid_rules_with_head = random.sample(temp_valid_rules, min(20, len(temp_valid_rules)))\n",
    "\n",
    "            temp_rules = random.sample(rules, min(20, len(rules)))\n",
    "            quitity_string = \"\".join(temp_rules)\n",
    "            quitity_string = quitity_string + \"\\n\"\n",
    "\n",
    "            valid_rules_string = \"\".join(valid_rules_with_head)\n",
    "            valid_rules_string = valid_rules_string + \"\\n\"\n",
    "\n",
    "            temp_current_prompt = fixed_context + quitity_string + valid_rules_string + predict\n",
    "\n",
    "            formatted_string = iteration_check_prompt_length(temp_current_prompt, list(condicate), return_rules, model)\n",
    "\n",
    "            return_rules = return_rules.format(candidate_rels=formatted_string)\n",
    "\n",
    "            prompt = temp_current_prompt + return_rules\n",
    "            query_file.write(f\"Sample {i + 1} time: \\n\")\n",
    "            query_file.write(prompt + \"\\n\")\n",
    "            if not args.dry_run:\n",
    "                response = model.generate_sentence(prompt)\n",
    "                if response is not None:\n",
    "                    # tqdm.write(\"Response: \\n{}\".format(response))\n",
    "                    rule_file.write(f\"Sample {i + 1} time: \\n\")\n",
    "                    rule_file.write(response + \"\\n\")\n",
    "                else:\n",
    "                    with open(os.path.join(rule_path, f\"fail_{file_name}.txt\"), \"w\") as fail_rule_file:\n",
    "                        fail_rule_file.write(prompt + \"\\n\")\n",
    "\n",
    "\n",
    "# def generate_rule_for_unknown_relation_by_multi_thread( # UNUSED\n",
    "#     row, rdict, rule_path, kg_rules_path, model, args, relation_subgraph, relation_regex, similiary_rel_dict\n",
    "# ):\n",
    "#     relation2id = rdict.rel2idx\n",
    "#     head = row\n",
    "\n",
    "#     head_id = relation2id[head]\n",
    "#     # print(\"Head: \", head)\n",
    "\n",
    "#     head_formate = head\n",
    "#     if args.is_rel_name is True:\n",
    "#         all_rels = list(relation2id.keys())\n",
    "#         candidate_rels = \", \".join(all_rels)\n",
    "#         head_formate = head\n",
    "#     else:\n",
    "#         all_rels = list(relation2id.values())\n",
    "#         str_list = [str(item) for item in all_rels]\n",
    "#         candidate_rels = \", \".join(str_list)\n",
    "#         head_formate = head_id\n",
    "\n",
    "#     # Build prompt excluding rules\n",
    "#     fixed_context, predict, return_rules = build_prompt_for_unknown(\n",
    "#         head_formate, candidate_rels, args.is_zero, args, prompt_dict\n",
    "#     )\n",
    "\n",
    "#     file_name = head.strip()\n",
    "#     with (\n",
    "#         open(os.path.join(rule_path, f\"{file_name}.txt\"), \"w\") as rule_file,\n",
    "#         open(os.path.join(rule_path, f\"{file_name}.query\"), \"w\") as query_file,\n",
    "#     ):\n",
    "#         rule_file.write(f\"Rule_head: {head}\\n\")\n",
    "#         for i in range(args.second):\n",
    "#             # # Convert list elements to the desired string format\n",
    "#             # formatted_string = ';'.join([f'{name}' for name in condicate])\n",
    "\n",
    "#             formatted_string = unknown_check_prompt_length(fixed_context + predict, all_rels, return_rules, model)\n",
    "\n",
    "#             return_rules = return_rules.format(candidate_rels=formatted_string)\n",
    "\n",
    "#             prompt = fixed_context + predict + return_rules\n",
    "#             # tqdm.write(\"Prompt: \\n{}\".format(prompt))\n",
    "#             query_file.write(f\"Sample {i + 1} time: \\n\")\n",
    "#             query_file.write(prompt + \"\\n\")\n",
    "#             if not args.dry_run:\n",
    "#                 response = model.generate_sentence(prompt)\n",
    "#                 if response is not None:\n",
    "#                     # tqdm.write(\"Response: \\n{}\".format(response))\n",
    "#                     rule_file.write(f\"Sample {i + 1} time: \\n\")\n",
    "#                     rule_file.write(response + \"\\n\")\n",
    "#                 else:\n",
    "#                     with open(os.path.join(rule_path, f\"fail_{file_name}.txt\"), \"w\") as fail_rule_file:\n",
    "#                         fail_rule_file.write(prompt + \"\\n\")\n",
    "\n",
    "\n",
    "def copy_files(source_dir, destination_dir, file_extension):\n",
    "    # 목적 폴더 만들기\n",
    "    if not os.path.exists(destination_dir):\n",
    "        os.makedirs(destination_dir)\n",
    "\n",
    "    # 소스 폴더의 파일 반복\n",
    "    for filename in os.listdir(source_dir):\n",
    "        # 파일 유형이 요구 사항에 부합하는지 확인\n",
    "        if filename.endswith(file_extension):\n",
    "            source_file = os.path.join(source_dir, filename)\n",
    "            destination_file = os.path.join(destination_dir, filename)\n",
    "            # 파일 복사\n",
    "            shutil.copyfile(source_file, destination_file)\n",
    "\n",
    "\n",
    "def process_rules_files(input_dir, output_dir, rdict, relation_regex, error_file_path):\n",
    "    sum = 0\n",
    "    with open(error_file_path, \"w\") as f_error_out:\n",
    "        for input_filepath in glob.glob(os.path.join(input_dir, \"*.txt\")):\n",
    "            file_name = input_filepath.split(\"/\")[-1]\n",
    "            if file_name.startswith(\"fail\"):\n",
    "                continue\n",
    "            else:\n",
    "                with open(input_filepath, \"r\") as fin, open(os.path.join(output_dir, file_name), \"w\") as fout:\n",
    "                    rules = fin.readlines()\n",
    "                    for idx, rule in enumerate(rules):\n",
    "                        is_save = True\n",
    "                        if rule.startswith(\"Rule_head:\"):\n",
    "                            continue\n",
    "                        elif rule.startswith(\"Sample\"):\n",
    "                            continue\n",
    "                        else:\n",
    "                            rule_by_name = \"\"\n",
    "                            temp_rule = re.sub(r\"\\s*<-\\s*\", \"&\", rule)\n",
    "                            regrex_list = re.split(r\"\\s*&\\s*|\\t\", temp_rule)\n",
    "                            confidence = regrex_list[-1].strip()\n",
    "                            for id, regrex in enumerate(regrex_list[:-1]):\n",
    "                                match = re.search(relation_regex, regrex)\n",
    "                                if match:\n",
    "                                    if match[1].strip().isdigit():\n",
    "                                        rel_id = int(match[1].strip())\n",
    "                                        if rel_id not in list(rdict.idx2rel):\n",
    "                                            print(f\"Error relation id:{rel_id}, rule:{rule}\")\n",
    "                                            f_error_out.write(f\"Error relation id:{rel_id}, rule:{rule}\")\n",
    "                                            sum = sum + 1\n",
    "                                            is_save = False\n",
    "                                            break\n",
    "\n",
    "                                        relation_name = rdict.idx2rel[rel_id]\n",
    "                                        subject = match[2].strip()\n",
    "                                        object = match[3].strip()\n",
    "                                        timestamp = match[4].strip()\n",
    "                                        regrex_name = f\"{relation_name}({subject},{object},{timestamp})\"\n",
    "                                        if id == 0:\n",
    "                                            regrex_name += \"<-\"\n",
    "                                        else:\n",
    "                                            regrex_name += \"&\"\n",
    "                                        rule_by_name += regrex_name\n",
    "                                    else:\n",
    "                                        print(f\"Error relation id:{match[1].strip()}, rule:{rule}\")\n",
    "                                        f_error_out.write(f\"Error relation id:{match[1].strip()}, rule:{rule}\")\n",
    "                                        sum = sum + 1\n",
    "                                        is_save = False\n",
    "                                        break\n",
    "\n",
    "                                else:\n",
    "                                    print(f\"Error rule:{rule}, rule:{rule}\")\n",
    "                                    f_error_out.write(f\"Error rule:{rule}, rule:{rule}\")\n",
    "                                    sum = sum + 1\n",
    "                                    is_save = False\n",
    "                                    break\n",
    "                            if is_save:\n",
    "                                rule_by_name += confidence\n",
    "                                fout.write(rule_by_name + \"\\n\")\n",
    "        f_error_out.write(f\"The number of error during id maps name is:{sum}\")\n",
    "\n",
    "\n",
    "def get_topk_similiary_rel(topk, similary_matrix, transformers_id2rel, transformers_rel2id):\n",
    "    # 각 행에서 숫자가 가장 큰 앞에서 topk 요소의 인덱스 계산\n",
    "    topk = -topk\n",
    "    top_k_indices = np.argsort(similary_matrix, axis=1)[:, topk:]\n",
    "    similiary_rel_dict = {}\n",
    "    for idx, similary_rels in enumerate(top_k_indices):\n",
    "        rel_name = transformers_id2rel[str(idx)]\n",
    "        similary_rel_name = [transformers_id2rel[str(i)] for i in similary_rels]\n",
    "        similiary_rel_dict[rel_name] = similary_rel_name\n",
    "\n",
    "    return similiary_rel_dict\n",
    "\n",
    "\n",
    "def get_low_conf(low_conf_file_path, relation_regex, rdict):\n",
    "    rule_dict = {}\n",
    "    with open(low_conf_file_path, \"r\") as fin_low:\n",
    "        rules = fin_low.readlines()\n",
    "        for rule in rules:\n",
    "            if \"index\" in rule:\n",
    "                continue\n",
    "            regrex_list = rule.split(\"<-\")\n",
    "            match = re.search(relation_regex, regrex_list[0])\n",
    "            if match:\n",
    "                head = match[1].strip()\n",
    "                if head not in list(rdict.rel2idx.keys()):\n",
    "                    raise ValueError(f\"Not exist relation:{head}\")\n",
    "\n",
    "                if head not in rule_dict:\n",
    "                    rule_dict[head] = []\n",
    "                rule_dict[head].append(rule)\n",
    "\n",
    "    rule_list = []\n",
    "    for key, value in rule_dict.items():\n",
    "        rule_list.append({\"head\": key, \"rules\": value})\n",
    "\n",
    "    return rule_list\n",
    "\n",
    "\n",
    "def get_high_conf(high_conf_file_path, relation_regex, rdict):\n",
    "    rule_dict = {}\n",
    "    with open(high_conf_file_path, \"r\") as fin_low:\n",
    "        rules = fin_low.readlines()\n",
    "        for rule in rules:\n",
    "            if \"index\" in rule:\n",
    "                continue\n",
    "            regrex_list = rule.split(\"<-\")\n",
    "            match = re.search(relation_regex, regrex_list[0])\n",
    "            if match:\n",
    "                head = match[1].strip()\n",
    "                if head not in list(rdict.rel2idx.keys()):\n",
    "                    raise ValueError(f\"Not exist relation:{head}\")\n",
    "\n",
    "                if head not in rule_dict:\n",
    "                    rule_dict[head] = []\n",
    "                rule_dict[head].append(rule)\n",
    "\n",
    "    rule_list = []\n",
    "    for key, value in rule_dict.items():\n",
    "        rule_list.append({\"head\": key, \"rules\": value})\n",
    "\n",
    "    return rule_list\n",
    "\n",
    "\n",
    "def analysis_data(confidence_folder, kg_rules_path):\n",
    "    with (\n",
    "        open(os.path.join(confidence_folder, \"hight_conf.txt\"), \"r\") as fin_hight,\n",
    "        open(os.path.join(confidence_folder, \"low_conf.txt\"), \"r\") as fin_low,\n",
    "    ):\n",
    "        hight_rule_set = set()\n",
    "        rules = fin_hight.readlines()\n",
    "        for rule in rules:\n",
    "            if \"index\" in rule:\n",
    "                continue\n",
    "            hight_rule_set.update([rule.strip()])\n",
    "\n",
    "        low_rule_set = set()\n",
    "        rules = fin_low.readlines()\n",
    "        for rule in rules:\n",
    "            if \"index\" in rule:\n",
    "                continue\n",
    "            low_rule_set.update([rule.strip()])\n",
    "\n",
    "    rules_dict = load_json_data(kg_rules_path)\n",
    "\n",
    "    all_rules = [item.strip() for sublist in rules_dict.values() for item in sublist]\n",
    "    all_rules_set = set(all_rules)\n",
    "\n",
    "    with open(os.path.join(confidence_folder, \"statistic.txt\"), \"w\") as fout_state:\n",
    "        fout_state.write(f\"valid_high:{len(hight_rule_set-all_rules_set)}\\n\")\n",
    "        fout_state.write(f\"valid_low:{len(low_rule_set-all_rules_set)}\\n\")\n",
    "\n",
    "\n",
    "def load_data_and_paths(args):\n",
    "    data_path = os.path.join(args.data_path, args.dataset) + \"/\"\n",
    "    dataset = Dataset(data_root=data_path, inv=True)\n",
    "\n",
    "    sampled_path_with_valid_dir = os.path.join(args.sampled_paths, args.dataset + \"_valid\")\n",
    "    sampled_path_dir = os.path.join(args.sampled_paths, args.dataset)\n",
    "    sampled_path = read_paths(os.path.join(sampled_path_dir, \"closed_rel_paths.jsonl\"))\n",
    "\n",
    "    prompt_path = os.path.join(args.prompt_paths, \"common.json\")\n",
    "    prompt_path_for_zero = os.path.join(args.prompt_paths, \"zero.json\")\n",
    "    prompt_path_low = os.path.join(args.prompt_paths, \"iteration_low.json\")\n",
    "    prompt_path_high = os.path.join(args.prompt_paths, \"iteration_high.json\")\n",
    "\n",
    "    return (\n",
    "        dataset,\n",
    "        sampled_path,\n",
    "        sampled_path_with_valid_dir,\n",
    "        sampled_path_dir,\n",
    "        prompt_path,\n",
    "        prompt_path_for_zero,\n",
    "        prompt_path_low,\n",
    "        prompt_path_high,\n",
    "    )\n",
    "\n",
    "\n",
    "def prepare_rule_heads(dataset, sampled_path):\n",
    "    rule_head_without_zero = {rule[\"head\"] for rule in sampled_path}\n",
    "    rule_head_with_zero = set(dataset.rdict.rel2idx.keys()) - rule_head_without_zero\n",
    "    return rule_head_without_zero, rule_head_with_zero\n",
    "\n",
    "\n",
    "def determine_kg_rules_path(args, sampled_path_dir):\n",
    "    if args.is_rel_name:\n",
    "        return os.path.join(sampled_path_dir, \"rules_name.json\")\n",
    "    else:\n",
    "        return os.path.join(sampled_path_dir, \"rules_id.json\")\n",
    "\n",
    "\n",
    "def load_configuration(dataset, sampled_path_dir, args):\n",
    "    constant_config = load_json_data(\"./Config/constant.json\")\n",
    "    relation_regex = constant_config[\"relation_regex\"][args.dataset]\n",
    "\n",
    "    rdict = dataset.get_relation_dict()\n",
    "    similarity_matrix = np.load(os.path.join(sampled_path_dir, \"matrix.npy\"))\n",
    "    transformers_id2rel = load_json_data(os.path.join(sampled_path_dir, \"transfomers_id2rel.json\"))\n",
    "    transformers_rel2id = load_json_data(os.path.join(sampled_path_dir, \"transfomers_rel2id.json\"))\n",
    "\n",
    "    similar_rel_dict = get_topk_similiary_rel(args.topk, similarity_matrix, transformers_id2rel, transformers_rel2id)\n",
    "\n",
    "    return rdict, relation_regex, similar_rel_dict\n",
    "\n",
    "\n",
    "def create_directories(args):\n",
    "    rule_path = os.path.join(\n",
    "        args.rule_path, args.dataset, f\"{args.prefix}{args.model_name}-top-{args.k}-f-{args.f}-l-{args.l}\"\n",
    "    )\n",
    "    os.makedirs(rule_path, exist_ok=True)\n",
    "\n",
    "    filter_rule_path = os.path.join(\n",
    "        args.rule_path, args.dataset, f\"copy_{args.prefix}{args.model_name}-top-{args.k}-f-{args.f}-l-{args.l}\"\n",
    "    )\n",
    "    if not os.path.exists(filter_rule_path):\n",
    "        os.makedirs(filter_rule_path)\n",
    "    else:\n",
    "        clear_folder(filter_rule_path)\n",
    "\n",
    "    return rule_path, filter_rule_path\n",
    "\n",
    "\n",
    "def main(args, LLM):\n",
    "    (\n",
    "        dataset,\n",
    "        sampled_path,\n",
    "        sampled_path_with_valid_dir,\n",
    "        sampled_path_dir,\n",
    "        prompt_path,\n",
    "        prompt_path_for_zero,\n",
    "        prompt_path_low,\n",
    "        prompt_path_high,\n",
    "    ) = load_data_and_paths(args)\n",
    "    rule_head_without_zero, rule_head_with_zero = prepare_rule_heads(dataset, sampled_path)\n",
    "    kg_rules_path = determine_kg_rules_path(args, sampled_path_dir)\n",
    "    rdict, relation_regex, similar_rel_dict = load_configuration(dataset, sampled_path_dir, args)\n",
    "    rule_path, filter_rule_path = create_directories(args)\n",
    "\n",
    "    prompt_info_dict = load_json_data(prompt_path)\n",
    "    prompt_info_zero_dict = load_json_data(prompt_path_for_zero)\n",
    "    prompt_dict_low = load_json_data(prompt_path_low)\n",
    "    prompt_dict_high = load_json_data(prompt_path_high)\n",
    "\n",
    "    model = LLM(args)\n",
    "    print(\"Preparing pipeline for inference...\")\n",
    "    model.prepare_for_inference()\n",
    "\n",
    "    llm_rule_generate(\n",
    "        args,\n",
    "        filter_rule_path,\n",
    "        kg_rules_path,\n",
    "        model,\n",
    "        rdict,\n",
    "        relation_regex,\n",
    "        rule_path,\n",
    "        sampled_path,\n",
    "        similar_rel_dict,\n",
    "        sampled_path_with_valid_dir,\n",
    "        rule_head_with_zero,\n",
    "        prompt_info_dict,\n",
    "        prompt_info_zero_dict,\n",
    "        prompt_dict_low,\n",
    "        prompt_dict_high,\n",
    "    )\n",
    "\n",
    "\n",
    "def llm_rule_generate(\n",
    "    args,\n",
    "    filter_rule_path,\n",
    "    kg_rules_path,\n",
    "    model,\n",
    "    rdict,\n",
    "    relation_regex,\n",
    "    rule_path,\n",
    "    sampled_path,\n",
    "    similiary_rel_dict,\n",
    "    sampled_path_with_valid_dir,\n",
    "    rule_head_with_zero,\n",
    "    prompt_info_dict,\n",
    "    prompt_info_zero_dict,\n",
    "    prompt_dict_low,\n",
    "    prompt_dict_high,\n",
    "):\n",
    "    # 규칙 생성\n",
    "    with ThreadPool(args.n) as p:\n",
    "        for _ in tqdm(\n",
    "            p.imap_unordered(\n",
    "                partial(\n",
    "                    generate_rule,\n",
    "                    rdict=rdict,\n",
    "                    rule_path=rule_path,\n",
    "                    kg_rules_path=kg_rules_path,\n",
    "                    model=model,\n",
    "                    args=args,\n",
    "                    relation_regex=relation_regex,\n",
    "                    similiary_rel_dict=similiary_rel_dict,\n",
    "                    prompt_info_dict=prompt_info_dict,\n",
    "                ),\n",
    "                sampled_path,\n",
    "            ),\n",
    "            total=len(sampled_path),\n",
    "        ):\n",
    "            pass\n",
    "    print(\"Generating rules for zero-shot...\")\n",
    "\n",
    "    with ThreadPool(args.n) as p:\n",
    "        for _ in tqdm(\n",
    "            p.imap_unordered(\n",
    "                partial(\n",
    "                    generate_rule_for_zero,\n",
    "                    rdict=rdict,\n",
    "                    rule_path=rule_path,\n",
    "                    model=model,\n",
    "                    args=args,\n",
    "                    prompt_info_zero_dict=prompt_info_zero_dict,\n",
    "                ),\n",
    "                rule_head_with_zero,\n",
    "            ),\n",
    "            total=len(rule_head_with_zero),\n",
    "        ):\n",
    "            pass\n",
    "\n",
    "    for input_filepath in tqdm(glob.glob(os.path.join(rule_path, \"fail_*.txt\")), desc=\"Generating rules for failed...\"):\n",
    "        filename = input_filepath.split(\"/\")[-1].split(\"fail_\")[-1]\n",
    "        with open(input_filepath, \"r\") as fin, open(os.path.join(rule_path, filename), \"w\") as fout:\n",
    "            content = fin.read()\n",
    "            response = model.generate_sentence(content)\n",
    "            if response is not None:\n",
    "                fout.write(response + \"\\n\")\n",
    "            else:\n",
    "                print(f\"Error:{filename}\")\n",
    "\n",
    "    # valid 데이터셋의 규칙\n",
    "    kg_rules_path_with_valid = os.path.join(sampled_path_with_valid_dir, \"rules_name.json\")\n",
    "\n",
    "    # LLM 첫 번째 규칙 생성 결과 분석\n",
    "    statistics_dir = os.path.join(\n",
    "        args.rule_path,\n",
    "        args.dataset,\n",
    "        \"statistics\",\n",
    "    )\n",
    "\n",
    "    if not os.path.exists(statistics_dir):\n",
    "        os.makedirs(statistics_dir)\n",
    "    else:\n",
    "        clear_folder(statistics_dir)\n",
    "\n",
    "    statistics_file_path = os.path.join(statistics_dir, \"statistics.txt\")\n",
    "    error_file_path = os.path.join(statistics_dir, \"error.txt\")\n",
    "\n",
    "    constant_config = load_json_data(\"./Config/constant.json\")\n",
    "    relation_regex = constant_config[\"relation_regex\"][args.dataset]\n",
    "    if args.is_rel_name is True:\n",
    "        copy_files(rule_path, filter_rule_path, \"txt\")\n",
    "    else:\n",
    "        process_rules_files(rule_path, filter_rule_path, rdict, relation_regex, error_file_path)\n",
    "\n",
    "    model.gen_rule_statistic(rule_path, statistics_file_path)\n",
    "\n",
    "    output_clean_folder = os.path.join(args.rule_path, args.dataset, \"clean\")\n",
    "    if not os.path.exists(output_clean_folder):\n",
    "        os.makedirs(output_clean_folder)\n",
    "    else:\n",
    "        clear_folder(output_clean_folder)\n",
    "\n",
    "    output_filter_train_folder = os.path.join(args.rule_path, args.dataset, \"filter\", \"train\")\n",
    "    if not os.path.exists(output_filter_train_folder):\n",
    "        os.makedirs(output_filter_train_folder)\n",
    "    else:\n",
    "        clear_folder(output_filter_train_folder)\n",
    "\n",
    "    # filter 폴더: 반복적으로 생성된 규칙 저장 (high conf 및 low conf 파일 포함)\n",
    "    output_filter_eva_folder = os.path.join(args.rule_path, args.dataset, \"filter\", \"eva\")\n",
    "    if not os.path.exists(output_filter_eva_folder):\n",
    "        os.makedirs(output_filter_eva_folder)\n",
    "    else:\n",
    "        clear_folder(output_filter_eva_folder)\n",
    "\n",
    "    output_filter_train_eva_folder = os.path.join(args.rule_path, args.dataset, \"filter\", \"train_eva\")\n",
    "    if not os.path.exists(output_filter_train_eva_folder):\n",
    "        os.makedirs(output_filter_train_eva_folder)\n",
    "    else:\n",
    "        clear_folder(output_filter_train_eva_folder)\n",
    "\n",
    "    # evaluation 폴더: 각 반복에서 중간 생성된 confidence.json 기록\n",
    "    output_eva_train_folder = os.path.join(args.rule_path, args.dataset, \"evaluation\", \"train\")\n",
    "    if not os.path.exists(output_eva_train_folder):\n",
    "        os.makedirs(output_eva_train_folder)\n",
    "    else:\n",
    "        clear_folder(output_eva_train_folder)\n",
    "\n",
    "    output_eva_eva_folder = os.path.join(args.rule_path, args.dataset, \"evaluation\", \"eva\")\n",
    "    if not os.path.exists(output_eva_eva_folder):\n",
    "        os.makedirs(output_eva_eva_folder)\n",
    "    else:\n",
    "        clear_folder(output_eva_eva_folder)\n",
    "\n",
    "    output_eva_train_eva_folder = os.path.join(args.rule_path, args.dataset, \"evaluation\", \"train_eva\")\n",
    "    if not os.path.exists(output_eva_train_eva_folder):\n",
    "        os.makedirs(output_eva_train_eva_folder)\n",
    "    else:\n",
    "        clear_folder(output_eva_train_eva_folder)\n",
    "\n",
    "    # 임시 폴더: LLM 요청 및 응답 저장\n",
    "    iteration_rule_file_path = os.path.join(args.rule_path, args.dataset, \"iteration\")\n",
    "    if not os.path.exists(iteration_rule_file_path):\n",
    "        os.makedirs(iteration_rule_file_path)\n",
    "    else:\n",
    "        clear_folder(iteration_rule_file_path)\n",
    "\n",
    "    # 각 반복에서 LLM 응답 저장\n",
    "    for i in range(args.num_iter + 1):\n",
    "        temp_only_txt_file_path = os.path.join(args.rule_path, args.dataset, f\"only_txt_{i}\")\n",
    "        clear_folder(temp_only_txt_file_path)\n",
    "\n",
    "    # 임시 폴더: LLM 응답 저장\n",
    "    iteration_only_txt_file_path = os.path.join(args.rule_path, args.dataset, \"only_txt\")\n",
    "    if not os.path.exists(iteration_only_txt_file_path):\n",
    "        os.makedirs(iteration_only_txt_file_path)\n",
    "    else:\n",
    "        clear_folder(iteration_only_txt_file_path)\n",
    "\n",
    "    copy_folder_contents(filter_rule_path, iteration_only_txt_file_path)\n",
    "\n",
    "    for i in range(args.num_iter):\n",
    "        temp_only_txt_file_path = os.path.join(args.rule_path, args.dataset, f\"only_txt_{i}\")\n",
    "        copy_folder_contents(iteration_only_txt_file_path, temp_only_txt_file_path)\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        output_rules_folder_dir = clean(args, model, iteration_only_txt_file_path, output_clean_folder)\n",
    "        conf_folder = None\n",
    "        if args.bgkg == \"train\":\n",
    "            train_rule_set = evaluation(args, output_rules_folder_dir, output_eva_train_folder, \"train\", index=i)\n",
    "            filter_rules_based_confidence(train_rule_set, args.min_conf, output_filter_train_folder, i)\n",
    "            conf_folder = output_filter_train_folder\n",
    "        elif args.bgkg == \"valid\":\n",
    "            env_rule_set = evaluation(args, output_rules_folder_dir, output_eva_eva_folder, \"eva\", index=i)\n",
    "            filter_rules_based_confidence(env_rule_set, args.min_conf, output_filter_eva_folder, i)\n",
    "            conf_folder = output_filter_eva_folder\n",
    "        elif args.bgkg == \"train_valid\":\n",
    "            train_env_rule_set = evaluation(\n",
    "                args, output_rules_folder_dir, output_eva_train_eva_folder, \"train_eva\", index=i\n",
    "            )\n",
    "            filter_rules_based_confidence(train_env_rule_set, args.min_conf, output_filter_train_eva_folder, i)\n",
    "            conf_folder = output_filter_train_eva_folder\n",
    "\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        elapsed_minutes = elapsed_time / 60\n",
    "\n",
    "        print(f\"프로그램 실행 시간: {elapsed_minutes}분\")\n",
    "\n",
    "        if args.is_high is False:\n",
    "            conf = get_low_conf(os.path.join(conf_folder, \"temp_low_conf.txt\"), relation_regex, rdict)\n",
    "        else:\n",
    "            conf = get_high_conf(os.path.join(conf_folder, \"temp_hight_conf.txt\"), relation_regex, rdict)\n",
    "\n",
    "        clear_folder(iteration_only_txt_file_path)\n",
    "        clear_folder(iteration_rule_file_path)\n",
    "        gen_rules_iteration(\n",
    "            args,\n",
    "            kg_rules_path,\n",
    "            model,\n",
    "            rdict,\n",
    "            relation_regex,\n",
    "            iteration_rule_file_path,\n",
    "            conf,\n",
    "            similiary_rel_dict,\n",
    "            kg_rules_path_with_valid,\n",
    "            prompt_dict_low,\n",
    "            prompt_dict_high,\n",
    "        )\n",
    "        copy_files(iteration_rule_file_path, iteration_only_txt_file_path, \"txt\")\n",
    "\n",
    "    temp_only_txt_file_path = os.path.join(args.rule_path, args.dataset, f\"only_txt_{args.num_iter}\")\n",
    "    copy_folder_contents(iteration_only_txt_file_path, temp_only_txt_file_path)\n",
    "\n",
    "    output_rules_folder_dir = clean(args, model, iteration_only_txt_file_path, output_clean_folder)\n",
    "\n",
    "    source_rule_path = None\n",
    "    if args.bgkg == \"train\":\n",
    "        train_rule_set = evaluation(\n",
    "            args, output_rules_folder_dir, output_eva_train_folder, \"train\", index=args.num_iter\n",
    "        )\n",
    "        filter_rules_based_confidence(train_rule_set, args.min_conf, output_filter_train_folder, args.num_iter)\n",
    "        analysis_data(output_filter_train_folder, kg_rules_path)\n",
    "        source_rule_path = output_filter_train_folder\n",
    "    elif args.bgkg == \"valid\":\n",
    "        env_rule_set = evaluation(args, output_rules_folder_dir, output_eva_eva_folder, \"eva\", index=args.num_iter)\n",
    "        filter_rules_based_confidence(env_rule_set, args.min_conf, output_filter_eva_folder, args.num_iter)\n",
    "        analysis_data(output_filter_eva_folder, kg_rules_path)\n",
    "        source_rule_path = output_filter_eva_folder\n",
    "    elif args.bgkg == \"train_valid\":\n",
    "        train_env_rule_set = evaluation(\n",
    "            args, output_rules_folder_dir, output_eva_train_eva_folder, \"train_eva\", index=args.num_iter\n",
    "        )\n",
    "        filter_rules_based_confidence(train_env_rule_set, args.min_conf, output_filter_train_eva_folder, args.num_iter)\n",
    "        analysis_data(output_filter_train_eva_folder, kg_rules_path)\n",
    "        source_rule_path = output_filter_train_eva_folder\n",
    "\n",
    "    final_sumary_file_path = os.path.join(\"gen_rules_iteration\", args.dataset, \"final_summary\")\n",
    "    os.makedirs(final_sumary_file_path, exist_ok=True)\n",
    "    if args.rule_domain == \"high\":\n",
    "        high_train_eva_file_path = os.path.join(source_rule_path, \"hight_conf.txt\")\n",
    "        with open(high_train_eva_file_path, \"r\") as fin_high:\n",
    "            high_unique_strings = set(fin_high.read().split())\n",
    "\n",
    "        unique_strings = high_unique_strings\n",
    "\n",
    "    elif args.rule_domain == \"iteration\":\n",
    "        high_train_eva_file_path = os.path.join(source_rule_path, \"hight_conf.txt\")\n",
    "        with open(high_train_eva_file_path, \"r\") as fin_high:\n",
    "            high_unique_strings = set(fin_high.read().split())\n",
    "\n",
    "        low_train_eva_file_path = os.path.join(source_rule_path, \"low_conf.txt\")\n",
    "        with open(low_train_eva_file_path, \"r\") as fin_low:\n",
    "            low_unique_strings = set(fin_low.read().split())\n",
    "\n",
    "        unique_strings = low_unique_strings.union(high_unique_strings)\n",
    "\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    with open(os.path.join(final_sumary_file_path, \"rules.txt\"), \"w\") as fout_final:\n",
    "        for rule in unique_strings:\n",
    "            fout_final.write(f\"{rule}\\n\")\n",
    "\n",
    "\n",
    "def gen_rules_iteration(\n",
    "    args,\n",
    "    kg_rules_path,\n",
    "    model,\n",
    "    rdict,\n",
    "    relation_regex,\n",
    "    rule_path,\n",
    "    conf,\n",
    "    similiary_rel_dict,\n",
    "    kg_rules_path_with_valid,\n",
    "    prompt_dict_low,\n",
    "    prompt_dict_high,\n",
    "):\n",
    "    with ThreadPool(args.n) as p:\n",
    "        for _ in tqdm(\n",
    "            p.imap_unordered(\n",
    "                partial(\n",
    "                    generate_rule_for_iteration_by_multi_thread,\n",
    "                    prompt_dict_low=prompt_dict_low,\n",
    "                    prompt_dict_high=prompt_dict_high,\n",
    "                    rdict=rdict,\n",
    "                    rule_path=rule_path,\n",
    "                    kg_rules_path=kg_rules_path,\n",
    "                    model=model,\n",
    "                    args=args,\n",
    "                    relation_regex=relation_regex,\n",
    "                    similiary_rel_dict=similiary_rel_dict,\n",
    "                    kg_rules_path_with_valid=kg_rules_path_with_valid,\n",
    "                ),\n",
    "                conf,\n",
    "            ),\n",
    "            total=len(conf),\n",
    "        ):\n",
    "            pass\n",
    "\n",
    "\n",
    "# def gen_rules_for_unknown_relation( # UNUSED\n",
    "#     args, kg_rules_path, model, rdict, relation_regex, relation_subgraph, rule_path, low_conf, similiary_rel_dict\n",
    "# ):\n",
    "#     with ThreadPool(args.n) as p:\n",
    "#         for _ in tqdm(\n",
    "#             p.imap_unordered(\n",
    "#                 partial(\n",
    "#                     generate_rule_for_unknown_relation_by_multi_thread,\n",
    "#                     rdict=rdict,\n",
    "#                     rule_path=rule_path,\n",
    "#                     kg_rules_path=kg_rules_path,\n",
    "#                     model=model,\n",
    "#                     args=args,\n",
    "#                     relation_subgraph=relation_subgraph,\n",
    "#                     relation_regex=relation_regex,\n",
    "#                     similiary_rel_dict=similiary_rel_dict,\n",
    "#                 ),\n",
    "#                 low_conf,\n",
    "#             ),\n",
    "#             total=len(low_conf),\n",
    "#         ):\n",
    "#             pass\n",
    "\n",
    "\n",
    "def filter_rules_based_confidence(rule_set, min_conf, output_folder, index):\n",
    "    with (\n",
    "        open(os.path.join(output_folder, \"hight_conf.txt\"), \"a\") as fout_hight,\n",
    "        open(os.path.join(output_folder, \"low_conf.txt\"), \"a\") as fout_low,\n",
    "        open(os.path.join(output_folder, \"temp_hight_conf.txt\"), \"w\") as fout_temp_hight,\n",
    "        open(os.path.join(output_folder, \"temp_low_conf.txt\"), \"w\") as fout_temp_low,\n",
    "    ):\n",
    "        fout_hight.write(f\"index:{index}\\n\")\n",
    "        fout_low.write(f\"index:{index}\\n\")\n",
    "        for rule in rule_set:\n",
    "            confidence = float(rule.split(\"&\")[-1].strip())\n",
    "            temp_rule = rule.split(\"&\")[:-1]\n",
    "            rule_without_confidence = \"&\".join(temp_rule)\n",
    "            if confidence > min_conf:\n",
    "                fout_hight.write(rule_without_confidence + \"\\n\")\n",
    "                fout_temp_hight.write(rule_without_confidence + \"\\n\")\n",
    "            else:\n",
    "                fout_low.write(rule_without_confidence + \"\\n\")\n",
    "                fout_temp_low.write(rule_without_confidence + \"\\n\")\n",
    "\n",
    "\n",
    "def evaluation(args, output_rules_folder_dir, output_evaluation_folder, dataset_type, index=0):\n",
    "    is_merge = args.is_merge\n",
    "    dataset_dir = \"./datasets/\" + args.dataset + \"/\"\n",
    "    data = Grapher(dataset_dir)\n",
    "\n",
    "    if dataset_type == \"train\":\n",
    "        temporal_walk = Temporal_Walk(np.array(data.train_idx.tolist()), data.inv_relation_id, args.transition_distr)\n",
    "    elif dataset_type == \"eva\":\n",
    "        temporal_walk = Temporal_Walk(np.array(data.valid_idx.tolist()), data.inv_relation_id, args.transition_distr)\n",
    "    else:\n",
    "        temporal_walk = Temporal_Walk(\n",
    "            np.array(data.valid_idx.tolist() + data.train_idx.tolist()), data.inv_relation_id, args.transition_distr\n",
    "        )\n",
    "\n",
    "    rl = Rule_Learner(temporal_walk.edges, data.id2relation, data.inv_relation_id, args.dataset)\n",
    "    rule_path = output_rules_folder_dir\n",
    "    constant_config = load_json_data(\"./Config/constant.json\")\n",
    "    relation_regex = constant_config[\"relation_regex\"][args.dataset]\n",
    "\n",
    "    rules_var_path = os.path.join(\"sampled_path\", args.dataset, \"original\", \"rules_var.json\")\n",
    "    rules_var_dict = load_json_data(rules_var_path)\n",
    "\n",
    "    if args.is_only_with_original_rules:\n",
    "        for key, value in rules_var_dict.items():\n",
    "            temp_var = {}\n",
    "            temp_var[\"head_rel\"] = value[\"head_rel\"]\n",
    "            temp_var[\"body_rels\"] = value[\"body_rels\"]\n",
    "            temp_var[\"var_constraints\"] = value[\"var_constraints\"]\n",
    "            if temp_var not in rl.original_found_rules:\n",
    "                rl.original_found_rules.append(temp_var.copy())\n",
    "                rl.update_rules_dict(value)\n",
    "                rl.num_original += 1\n",
    "    else:\n",
    "        llm_gen_rules_list, fail_calc_confidence = calculate_confidence(\n",
    "            rule_path,\n",
    "            data.relation2id,\n",
    "            data.inv_relation_id,\n",
    "            rl,\n",
    "            relation_regex,\n",
    "            rules_var_dict,\n",
    "            is_merge,\n",
    "            is_has_confidence=False,\n",
    "        )\n",
    "\n",
    "    rules_statistics(rl.rules_dict)\n",
    "\n",
    "    if args.is_only_with_original_rules:\n",
    "        dir_path = output_evaluation_folder\n",
    "        confidence_file_path = os.path.join(dir_path, \"original_confidence.json\")\n",
    "        save_json_data(rl.rules_dict, confidence_file_path)\n",
    "    else:\n",
    "        if is_merge is True:\n",
    "            original_rules_set = set(list(rules_var_dict.keys()))\n",
    "            llm_gen_rules_set = set(llm_gen_rules_list)\n",
    "            for idx, rule_chain in enumerate(original_rules_set - llm_gen_rules_set):\n",
    "                rule = rules_var_dict[rule_chain]\n",
    "                rl.update_rules_dict(rule)\n",
    "\n",
    "            rules_statistics(rl.rules_dict)\n",
    "\n",
    "            dir_path = output_evaluation_folder\n",
    "            confidence_file_path = os.path.join(dir_path, \"merge_confidence.json\")\n",
    "            save_json_data(rl.rules_dict, confidence_file_path)\n",
    "        else:\n",
    "            dir_path = output_evaluation_folder\n",
    "            confidence_file_path = os.path.join(dir_path, f\"{index}_confidence.json\")\n",
    "            save_json_data(rl.rules_dict, confidence_file_path)\n",
    "\n",
    "            fail_confidence_file_path = os.path.join(dir_path, \"fail_confidence.txt\")\n",
    "            with open(fail_confidence_file_path, \"a\") as fout:\n",
    "                for fail_rule in fail_calc_confidence:\n",
    "                    fout.write(f\"{fail_rule}\\n\")\n",
    "\n",
    "    return set(llm_gen_rules_list)\n",
    "\n",
    "\n",
    "def calculate_confidence(\n",
    "    rule_path, relation2id, inv_relation_id, rl, relation_regex, rules_var_dict, is_merge, is_has_confidence=False\n",
    "):\n",
    "    llm_gen_rules_list = []\n",
    "    fail_calc_confidence = []\n",
    "    for input_filepath in glob.glob(os.path.join(rule_path, \"*_cleaned_rules.txt\")):\n",
    "        with open(input_filepath, \"r\") as f:\n",
    "            rules = f.readlines()\n",
    "            for i_, rule in enumerate(rules):\n",
    "                try:\n",
    "                    if is_has_confidence:\n",
    "                        try:\n",
    "                            confidence = float(rule.split(\"&\")[-1].strip())\n",
    "                            temp_rule = rule.split(\"&\")[:-1]\n",
    "                            rule_without_confidence = \"&\".join(temp_rule)\n",
    "                            rule_without_confidence = rule_without_confidence.strip()\n",
    "                            walk = get_walk(rule_without_confidence, relation2id, inv_relation_id, relation_regex)\n",
    "\n",
    "                            rule_with_confidence = rl.create_rule_for_merge_for_iteration(\n",
    "                                walk, confidence, rule_without_confidence, rules_var_dict, is_merge\n",
    "                            )\n",
    "                            llm_gen_rules_list.append(rule_with_confidence + \"\\n\")\n",
    "                        except Exception as e:\n",
    "                            print(e)\n",
    "                            fail_calc_confidence.append(rule + \"\\n\")\n",
    "                    else:\n",
    "                        try:\n",
    "                            confidence = 0\n",
    "                            temp_rule = rule.split(\"&\")\n",
    "                            rule_without_confidence = \"&\".join(temp_rule)\n",
    "                            rule_without_confidence = rule_without_confidence.strip()\n",
    "                            walk = get_walk(rule_without_confidence, relation2id, inv_relation_id, relation_regex)\n",
    "                            rule_with_confidence = rl.create_rule_for_merge_for_iteration(\n",
    "                                walk, confidence, rule_without_confidence, rules_var_dict, is_merge\n",
    "                            )\n",
    "                            llm_gen_rules_list.append(rule_with_confidence + \"\\n\")\n",
    "                        except Exception as e:\n",
    "                            print(e)\n",
    "                            fail_calc_confidence.append(rule + \"\\n\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing rule: {rule}\")\n",
    "                    traceback.print_exc()  # 예외의 자세한 정보와 호출 스택 인쇄\n",
    "\n",
    "    return llm_gen_rules_list, fail_calc_confidence\n",
    "\n",
    "\n",
    "def process_rule(rule, relation2id, inv_relation_id, rl, relation_regex, rules_var_dict, is_merge, is_has_confidence):\n",
    "    try:\n",
    "        if is_has_confidence:\n",
    "            confidence = float(rule.split(\"&\")[-1].strip())\n",
    "            temp_rule = rule.split(\"&\")[:-1]\n",
    "            rule_without_confidence = \"&\".join(temp_rule).strip()\n",
    "            walk = get_walk(rule_without_confidence, relation2id, inv_relation_id, relation_regex)\n",
    "            rule_with_confidence = rl.create_rule_for_merge_for_iteration(\n",
    "                walk, confidence, rule_without_confidence, rules_var_dict, is_merge\n",
    "            )\n",
    "            return rule_with_confidence + \"\\n\", None\n",
    "        else:\n",
    "            confidence = 0\n",
    "            temp_rule = rule.split(\"&\")\n",
    "            rule_without_confidence = \"&\".join(temp_rule).strip()\n",
    "            walk = get_walk(rule_without_confidence, relation2id, inv_relation_id, relation_regex)\n",
    "            rule_with_confidence = rl.create_rule_for_merge_for_iteration(\n",
    "                walk, confidence, rule_without_confidence, rules_var_dict, is_merge\n",
    "            )\n",
    "            return rule_with_confidence + \"\\n\", None\n",
    "    except Exception as e:\n",
    "        return None, rule + \"\\n\"\n",
    "\n",
    "\n",
    "def calculate_confidence_O(\n",
    "    rule_path, relation2id, inv_relation_id, rl, relation_regex, rules_var_dict, is_merge, is_has_confidence=False\n",
    "):\n",
    "    llm_gen_rules_list = []\n",
    "    fail_calc_confidence = []\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=12) as executor:\n",
    "        futures = []\n",
    "        for input_filepath in glob.glob(os.path.join(rule_path, \"*_cleaned_rules.txt\")):\n",
    "            with open(input_filepath, \"r\") as f:\n",
    "                rules = f.readlines()\n",
    "                for rule in rules:\n",
    "                    future = executor.submit(\n",
    "                        process_rule,\n",
    "                        rule,\n",
    "                        relation2id,\n",
    "                        inv_relation_id,\n",
    "                        rl,\n",
    "                        relation_regex,\n",
    "                        rules_var_dict,\n",
    "                        is_merge,\n",
    "                        is_has_confidence,\n",
    "                    )\n",
    "                    futures.append(future)\n",
    "\n",
    "        for future in futures:\n",
    "            result, error = future.result()\n",
    "            if result:\n",
    "                llm_gen_rules_list.append(result)\n",
    "            if error:\n",
    "                fail_calc_confidence.append(error)\n",
    "\n",
    "    return llm_gen_rules_list, fail_calc_confidence\n",
    "\n",
    "\n",
    "def calculate_confidence_1(\n",
    "    rule_path,\n",
    "    relation2id,\n",
    "    inv_relation_id,\n",
    "    rl,\n",
    "    relation_regex,\n",
    "    rules_var_dict,\n",
    "    is_merge,\n",
    "    is_has_confidence=False,\n",
    "    num_threads=10,\n",
    "):\n",
    "    llm_gen_rules_list = []\n",
    "    fail_calc_confidence = []\n",
    "\n",
    "    all_rules = []\n",
    "    for input_filepath in glob.glob(os.path.join(rule_path, \"*_cleaned_rules.txt\")):\n",
    "        with open(input_filepath, \"r\") as f:\n",
    "            all_rules.extend(f.readlines())\n",
    "\n",
    "    with ThreadPool(num_threads) as p:\n",
    "        for result, error in tqdm(\n",
    "            p.imap_unordered(\n",
    "                partial(\n",
    "                    process_rule,\n",
    "                    relation2id=relation2id,\n",
    "                    inv_relation_id=inv_relation_id,\n",
    "                    rl=rl,\n",
    "                    relation_regex=relation_regex,\n",
    "                    rules_var_dict=rules_var_dict,\n",
    "                    is_merge=is_merge,\n",
    "                    is_has_confidence=is_has_confidence,\n",
    "                ),\n",
    "                all_rules,\n",
    "            ),\n",
    "            total=len(all_rules),\n",
    "        ):\n",
    "            if result:\n",
    "                llm_gen_rules_list.append(result)\n",
    "            if error:\n",
    "                fail_calc_confidence.append(error)\n",
    "\n",
    "    return llm_gen_rules_list, fail_calc_confidence\n",
    "\n",
    "\n",
    "def get_walk(rule, relation2id, inv_relation_id, regex):\n",
    "    head_body = rule.split(\"<-\")\n",
    "    rule_head_full_name = head_body[0].strip()\n",
    "    condition_string = head_body[1].strip()\n",
    "\n",
    "    # 정규 표현식 정의\n",
    "    relation_regex = regex\n",
    "\n",
    "    # 규칙 헤드의 관계, 주어와 목적어 추출\n",
    "    match = re.search(relation_regex, rule_head_full_name)\n",
    "    head_relation_name, head_subject, head_object, head_timestamp = match.groups()[:4]\n",
    "\n",
    "    # 규칙 본문의 관계와 엔티티 추출\n",
    "    matches = re.findall(relation_regex, condition_string)\n",
    "    entities = (\n",
    "        [head_object] + [match[1].strip() for match in matches[:-1]] + [matches[-1][1].strip(), matches[-1][2].strip()]\n",
    "    )\n",
    "\n",
    "    relation_ids = [relation2id[head_relation_name]] + [relation2id[match[0].strip()] for match in matches]\n",
    "\n",
    "    # 첫 번째 요소를 제외한 목록 반전\n",
    "    entities = entities[:1] + entities[1:][::-1]\n",
    "    relation_ids = relation_ids[:1] + [inv_relation_id[x] for x in relation_ids[:0:-1]]\n",
    "\n",
    "    # 결과 사전 구성\n",
    "    result = {\"entities\": entities, \"relations\": relation_ids}\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def clean(args, llm_model, filter_rule_path, output_folder):\n",
    "    data_path = os.path.join(args.data_path, args.dataset) + \"/\"\n",
    "    dataset = Dataset(data_root=data_path, inv=True)\n",
    "    rdict = dataset.get_relation_dict()\n",
    "    all_rels = list(rdict.rel2idx.keys())\n",
    "    input_folder = filter_rule_path\n",
    "\n",
    "    output_statistic_folder_dir = os.path.join(output_folder, \"clean_statistics\")\n",
    "    if not os.path.exists(output_statistic_folder_dir):\n",
    "        os.makedirs(output_statistic_folder_dir)\n",
    "\n",
    "    output_rules_folder_dir = os.path.join(output_folder, \"rules\")\n",
    "    if not os.path.exists(output_rules_folder_dir):\n",
    "        os.makedirs(output_rules_folder_dir)\n",
    "    else:\n",
    "        clear_folder(output_rules_folder_dir)\n",
    "\n",
    "    # 분석 clean 과정에서 success와 error 경우\n",
    "    output_error_file_path = os.path.join(output_statistic_folder_dir, \"error.txt\")\n",
    "    output_suc_file_path = os.path.join(output_statistic_folder_dir, \"suc.txt\")\n",
    "    with open(output_error_file_path, \"a\") as fout_error, open(output_suc_file_path, \"a\") as fout_suc:\n",
    "        num_error, num_suc = clean_processing(\n",
    "            all_rels, args, fout_error, input_folder, llm_model, output_rules_folder_dir, fout_suc\n",
    "        )\n",
    "        fout_error.write(f\"The number of cleaned rules is {num_error}\\n\")\n",
    "        fout_suc.write(f\"The number of retain rules is {num_suc}\\n\")\n",
    "\n",
    "    return output_rules_folder_dir\n",
    "\n",
    "\n",
    "def clean_processing(all_rels, args, fout_error, input_folder, llm_model, output_folder, fout_suc):\n",
    "    constant_config = load_json_data(\"./Config/constant.json\")\n",
    "    rule_start_with_regex = constant_config[\"rule_start_with_regex\"]\n",
    "    replace_regex = constant_config[\"replace_regex\"]\n",
    "    relation_regex = constant_config[\"relation_regex\"][args.dataset]\n",
    "    num_error = 0\n",
    "    num_suc = 0\n",
    "    for filename in os.listdir(input_folder):\n",
    "        if filename.endswith(\".txt\") and \"query\" not in filename and filename.startswith(\"fail\") is False:\n",
    "            input_filepath = os.path.join(input_folder, filename)\n",
    "            name, ext = os.path.splitext(filename)\n",
    "            summarized_filepath = os.path.join(output_folder, f\"{name}_summarized_rules.txt\")\n",
    "            clean_filename = name + \"_cleaned_rules.txt\"\n",
    "            clean_filepath = os.path.join(output_folder, clean_filename)\n",
    "\n",
    "            if not args.clean_only:\n",
    "                # Step 1: Summarize rules from the input file\n",
    "                print(\"Start summarize: \", filename)\n",
    "                # Summarize rules\n",
    "                summarized_rules = summarize_rule(input_filepath, llm_model, args, rule_start_with_regex, replace_regex)\n",
    "                print(\"write file\", summarized_filepath)\n",
    "                with open(summarized_filepath, \"w\") as f:\n",
    "                    f.write(\"\\n\".join(summarized_rules))\n",
    "\n",
    "            # Step 2: Clean summarized rules and keep format\n",
    "            print(f\"Clean file {summarized_filepath} with keeping the format\")\n",
    "            cleaned_rules, num, num_0 = clean_rules(summarized_filepath, all_rels, relation_regex, fout_error, fout_suc)\n",
    "            num_error = num_error + num\n",
    "            num_suc = num_suc + num_0\n",
    "\n",
    "            if len(cleaned_rules) != 0:\n",
    "                with open(clean_filepath, \"w\") as f:\n",
    "                    f.write(\"\\n\".join(cleaned_rules))\n",
    "    return num_error, num_suc\n",
    "\n",
    "\n",
    "def extract_rules(content_list, rule_start_with_regex, replace_regex):\n",
    "    \"\"\"Extract the rules in the content without any explanation and the leading number if it has.\"\"\"\n",
    "    rule_pattern = re.compile(rule_start_with_regex)\n",
    "    extracted_rules = [s.strip() for s in content_list if rule_pattern.match(s)]\n",
    "    number_pattern = re.compile(replace_regex)\n",
    "    cleaned_rules = [number_pattern.sub(\"\", s) for s in extracted_rules]\n",
    "    return list(set(cleaned_rules))  # Remove duplicates by converting to set and back to list\n",
    "\n",
    "\n",
    "def summarize_rules_prompt(relname, k):\n",
    "    \"\"\"\n",
    "    Generate prompt for the relation in the content_list\n",
    "    \"\"\"\n",
    "\n",
    "    if k != 0:\n",
    "        prompt = f'\\n\\nPlease identify the most important {k} rules from the following rules for the rule head: \"{relname}(X,Y,T)\". '\n",
    "    else:  # k ==0\n",
    "        prompt = f'\\n\\nPlease identify as many of the most important rules for the rule head: \"{relname}(X,Y,T)\" as possible. '\n",
    "\n",
    "    prompt += (\n",
    "        \"You can summarize the rules that have similar meanings as one rule, if you think they are important. \"\n",
    "        \"Return the rules only without any explanations. \"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def summarize_rule_for_unkown(file, llm_model, args, rule_start_with_regex, replace_regex):\n",
    "    \"\"\"\n",
    "    Summarize the rules\n",
    "    \"\"\"\n",
    "    with open(file, \"r\") as f:  # Load files\n",
    "        content = f.read()\n",
    "        rel_name = os.path.splitext(file)[0].split(\"/\")[-1]\n",
    "\n",
    "    content_list = content.split(\"\\n\")\n",
    "    rule_list = extract_rules(\n",
    "        content_list, rule_start_with_regex, replace_regex\n",
    "    )  # Extract rules and remove any explanations\n",
    "    if not args.force_summarize or llm_model is None:  # just return the whole rule_list\n",
    "        return rule_list\n",
    "    else:  # Do summarization and correct the spelling error\n",
    "        summarize_prompt = summarize_rules_prompt(rel_name, args.k)\n",
    "        summarize_prompt_len = num_tokens_from_message(summarize_prompt, args.model_name)\n",
    "        list_of_rule_lists = shuffle_split_path_list(rule_list, summarize_prompt_len, args.model_name)\n",
    "        response_list = []\n",
    "        for rule_list in list_of_rule_lists:\n",
    "            message = \"\\n\".join(rule_list) + summarize_prompt\n",
    "            print(\"prompt: \", message)\n",
    "            response = query(message, llm_model)\n",
    "            response_list.extend(response.split(\"\\n\"))\n",
    "        response_rules = extract_rules(\n",
    "            response_list, rule_start_with_regex, replace_regex\n",
    "        )  # Extract rules and remove any explanations from summarized response\n",
    "\n",
    "        return response_rules\n",
    "\n",
    "\n",
    "def summarize_rule(file, llm_model, args, rule_start_with_regex, replace_regex):\n",
    "    \"\"\"\n",
    "    Summarize the rules\n",
    "    \"\"\"\n",
    "    with open(file, \"r\") as f:  # Load files\n",
    "        content = f.read()\n",
    "        rel_name = os.path.splitext(file)[0].split(\"/\")[-1]\n",
    "\n",
    "    content_list = content.split(\"\\n\")\n",
    "    rule_list = extract_rules(\n",
    "        content_list, rule_start_with_regex, replace_regex\n",
    "    )  # Extract rules and remove any explanations\n",
    "    if not args.force_summarize or llm_model is None:  # just return the whole rule_list\n",
    "        return rule_list\n",
    "    else:  # Do summarization and correct the spelling error\n",
    "        summarize_prompt = summarize_rules_prompt(rel_name, args.k)\n",
    "        summarize_prompt_len = num_tokens_from_message(summarize_prompt, args.model_name)\n",
    "        list_of_rule_lists = shuffle_split_path_list(rule_list, summarize_prompt_len, args.model_name)\n",
    "        response_list = []\n",
    "        for rule_list in list_of_rule_lists:\n",
    "            message = \"\\n\".join(rule_list) + summarize_prompt\n",
    "            print(\"prompt: \", message)\n",
    "            response = query(message, llm_model)\n",
    "            response_list.extend(response.split(\"\\n\"))\n",
    "        response_rules = extract_rules(\n",
    "            response_list, rule_start_with_regex, replace_regex\n",
    "        )  # Extract rules and remove any explanations from summarized response\n",
    "\n",
    "        return response_rules\n",
    "\n",
    "\n",
    "def modify_process(temp_rule, relation_regex):\n",
    "    regrex_list = temp_rule.split(\"&\")\n",
    "    for idx, regrex in enumerate(regrex_list):\n",
    "        match = re.search(relation_regex, regrex)\n",
    "        if match:\n",
    "            relation_name = match[1].strip()\n",
    "            subject = match[2].strip()\n",
    "            object = match[3].strip()\n",
    "            timestamp = match[4].strip()\n",
    "\n",
    "\n",
    "def clean_rules_for_unknown(summarized_file_path, all_rels, relation_regex, fout_error, fout_suc):\n",
    "    \"\"\"\n",
    "    Clean error rules and remove rules with error relation.\n",
    "    \"\"\"\n",
    "    num_error = 0\n",
    "    num_suc = 0\n",
    "    with open(summarized_file_path, \"r\") as f:\n",
    "        input_rules = [line.strip() for line in f]\n",
    "    cleaned_rules = list()\n",
    "    # Correct spelling error/grammar error for the relation in the rules and Remove rules with error relation.\n",
    "    for input_rule in input_rules:\n",
    "        if input_rule == \"\":\n",
    "            continue\n",
    "        rule_list = []\n",
    "        temp_rule = re.sub(r\"\\s*<-\\s*\", \"&\", input_rule)\n",
    "        regrex_list = temp_rule.split(\"&\")\n",
    "        last_subject = None\n",
    "        final_object = None\n",
    "        time_squeque = []\n",
    "        final_time = None\n",
    "        is_save = True\n",
    "        is_check = True\n",
    "        try:\n",
    "            for idx, regrex in enumerate(regrex_list):\n",
    "                match = re.search(relation_regex, regrex)\n",
    "                if match:\n",
    "                    relation_name = match[1].strip()\n",
    "                    subject = match[2].strip()\n",
    "                    object = match[3].strip()\n",
    "                    timestamp = match[4].strip()\n",
    "\n",
    "                    if timestamp[1:].isdigit() is False:\n",
    "                        correct_rule = modify_process(temp_rule, relation_regex)\n",
    "                        is_check = False\n",
    "                        break\n",
    "\n",
    "                    if relation_name not in all_rels:\n",
    "                        best_match = get_close_matches(relation_name, all_rels, n=1)\n",
    "                        if not best_match:\n",
    "                            print(f\"Cannot correctify this rule, head not in relation:{input_rule}\\n\")\n",
    "                            fout_error.write(f\"Cannot correctify this rule, head not in relation:{input_rule}\\n\")\n",
    "                            is_save = False\n",
    "                            num_error = num_error + 1\n",
    "                            break\n",
    "                        relation_name = best_match[0].strip()\n",
    "\n",
    "                    rule_list.append(f\"{relation_name}({subject},{object},{timestamp})\")\n",
    "\n",
    "                    if idx == 0:\n",
    "                        head_subject = subject\n",
    "                        head_object = object\n",
    "                        head_subject = head_subject\n",
    "\n",
    "                        last_subject = head_subject\n",
    "                        final_object = head_object\n",
    "\n",
    "                        final_time = int(timestamp[1:])\n",
    "                    else:\n",
    "                        if last_subject == subject:\n",
    "                            last_subject = object\n",
    "                        else:\n",
    "                            print(f\"Error: Rule {input_rule} does not conform to the definition of chain rule.\")\n",
    "                            fout_error.write(\n",
    "                                f\"Error: Rule {input_rule} does not conform to the definition of chain rule.\\n\"\n",
    "                            )\n",
    "                            num_error = num_error + 1\n",
    "                            is_save = False\n",
    "                            break\n",
    "\n",
    "                        time_squeque.append(int(timestamp[1:]))\n",
    "\n",
    "                    if idx == len(regrex_list) - 1:\n",
    "                        if last_subject != final_object:\n",
    "                            print(f\"Error: Rule {input_rule} does not conform to the definition of chain rule.\")\n",
    "                            fout_error.write(\n",
    "                                f\"Error: Rule {input_rule} does not conform to the definition of chain rule.\\n\"\n",
    "                            )\n",
    "                            num_error = num_error + 1\n",
    "                            is_save = False\n",
    "                            break\n",
    "\n",
    "                else:\n",
    "                    print(f\"Error: rule {input_rule}\")\n",
    "                    fout_error.write(f\"Error: rule {input_rule}\\n\")\n",
    "                    num_error = num_error + 1\n",
    "                    is_save = False\n",
    "                    break\n",
    "\n",
    "            if is_check is True:\n",
    "                if all(time_squeque[i] <= time_squeque[i + 1] for i in range(len(time_squeque) - 1)) is False:\n",
    "                    print(f\"Error: Rule {input_rule} time_squeque is error.\")\n",
    "                    fout_error.write(f\"Error: Rule {input_rule} time_squeque is error.\\n\")\n",
    "                    num_error = num_error + 1\n",
    "                    is_save = False\n",
    "                elif final_time < time_squeque[-1]:\n",
    "                    print(f\"Error: Rule {input_rule} time_squeque is error.\")\n",
    "                    fout_error.write(f\"Error: Rule {input_rule} time_squeque is error.\\n\")\n",
    "                    num_error = num_error + 1\n",
    "                    is_save = False\n",
    "\n",
    "            if is_save:\n",
    "                correct_rule = \"&\".join(rule_list).strip().replace(\"&\", \"<-\", 1)\n",
    "                cleaned_rules.append(correct_rule)\n",
    "                fout_suc.write(correct_rule + \"\\n\")\n",
    "                num_suc = num_suc + 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Processing {input_rule} failed.\\n Error: {str(e)}\")\n",
    "            fout_error.write(f\"Processing {input_rule} failed.\\n Error: {str(e)}\\n\")\n",
    "            num_error = num_error + 1\n",
    "    return cleaned_rules, num_error, num_suc\n",
    "\n",
    "\n",
    "def clean_rules(summarized_file_path, all_rels, relation_regex, fout_error, fout_suc):\n",
    "    \"\"\"\n",
    "    Clean error rules and remove rules with error relation.\n",
    "    \"\"\"\n",
    "    num_error = 0\n",
    "    num_suc = 0\n",
    "    with open(summarized_file_path, \"r\") as f:\n",
    "        input_rules = [line.strip() for line in f]\n",
    "    cleaned_rules = list()\n",
    "    # Correct spelling error/grammar error for the relation in the rules and Remove rules with error relation.\n",
    "    for input_rule in input_rules:\n",
    "        if input_rule == \"\":\n",
    "            continue\n",
    "        rule_list = []\n",
    "        temp_rule = re.sub(r\"\\s*<-\\s*\", \"&\", input_rule)\n",
    "        regrex_list = temp_rule.split(\"&\")\n",
    "        last_subject = None\n",
    "        final_object = None\n",
    "        time_squeque = []\n",
    "        final_time = None\n",
    "        is_save = True\n",
    "        try:\n",
    "            for idx, regrex in enumerate(regrex_list):\n",
    "                match = re.search(relation_regex, regrex)\n",
    "                if match:\n",
    "                    relation_name = match[1].strip()\n",
    "                    subject = match[2].strip()\n",
    "                    object = match[3].strip()\n",
    "                    timestamp = match[4].strip()\n",
    "\n",
    "                    if timestamp[1:].isdigit() is False:\n",
    "                        print(f\"Error: Rule {input_rule}:{timestamp} is not digit\")\n",
    "                        fout_error.write(f\"Error: Rule {input_rule}:{timestamp} is not digit\\n\")\n",
    "                        num_error = num_error + 1\n",
    "                        is_save = False\n",
    "                        break\n",
    "\n",
    "                    if relation_name not in all_rels:\n",
    "                        best_match = get_close_matches(relation_name, all_rels, n=1)\n",
    "                        if not best_match:\n",
    "                            print(f\"Cannot correctify this rule, head not in relation:{input_rule}\\n\")\n",
    "                            fout_error.write(f\"Cannot correctify this rule, head not in relation:{input_rule}\\n\")\n",
    "                            is_save = False\n",
    "                            num_error = num_error + 1\n",
    "                            break\n",
    "                        relation_name = best_match[0].strip()\n",
    "\n",
    "                    rule_list.append(f\"{relation_name}({subject},{object},{timestamp})\")\n",
    "\n",
    "                    if idx == 0:\n",
    "                        head_subject = subject\n",
    "                        head_object = object\n",
    "                        head_subject = head_subject\n",
    "\n",
    "                        last_subject = head_subject\n",
    "                        final_object = head_object\n",
    "\n",
    "                        final_time = int(timestamp[1:])\n",
    "                    else:\n",
    "                        if last_subject == subject:\n",
    "                            last_subject = object\n",
    "                        else:\n",
    "                            print(f\"Error: Rule {input_rule} does not conform to the definition of chain rule.\")\n",
    "                            fout_error.write(\n",
    "                                f\"Error: Rule {input_rule} does not conform to the definition of chain rule.\\n\"\n",
    "                            )\n",
    "                            num_error = num_error + 1\n",
    "                            is_save = False\n",
    "                            break\n",
    "\n",
    "                        time_squeque.append(int(timestamp[1:]))\n",
    "\n",
    "                    if idx == len(regrex_list) - 1:\n",
    "                        if last_subject != final_object:\n",
    "                            print(f\"Error: Rule {input_rule} does not conform to the definition of chain rule.\")\n",
    "                            fout_error.write(\n",
    "                                f\"Error: Rule {input_rule} does not conform to the definition of chain rule.\\n\"\n",
    "                            )\n",
    "                            num_error = num_error + 1\n",
    "                            is_save = False\n",
    "                            break\n",
    "\n",
    "                else:\n",
    "                    print(f\"Error: rule {input_rule}\")\n",
    "                    fout_error.write(f\"Error: rule {input_rule}\\n\")\n",
    "                    num_error = num_error + 1\n",
    "                    is_save = False\n",
    "                    break\n",
    "\n",
    "            if all(time_squeque[i] <= time_squeque[i + 1] for i in range(len(time_squeque) - 1)) is False:\n",
    "                print(f\"Error: Rule {input_rule} time_squeque is error.\")\n",
    "                fout_error.write(f\"Error: Rule {input_rule} time_squeque is error.\\n\")\n",
    "                num_error = num_error + 1\n",
    "                is_save = False\n",
    "            elif final_time < time_squeque[-1]:\n",
    "                print(f\"Error: Rule {input_rule} time_squeque is error.\")\n",
    "                fout_error.write(f\"Error: Rule {input_rule} time_squeque is error.\\n\")\n",
    "                num_error = num_error + 1\n",
    "                is_save = False\n",
    "\n",
    "            if is_save:\n",
    "                correct_rule = \"&\".join(rule_list).strip().replace(\"&\", \"<-\", 1)\n",
    "                cleaned_rules.append(correct_rule)\n",
    "                fout_suc.write(correct_rule + \"\\n\")\n",
    "                num_suc = num_suc + 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Processing {input_rule} failed.\\n Error: {str(e)}\")\n",
    "            fout_error.write(f\"Processing {input_rule} failed.\\n Error: {str(e)}\\n\")\n",
    "            num_error = num_error + 1\n",
    "    return cleaned_rules, num_error, num_suc\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request Time out. Retrying in 30 seconds...\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "def parse_arguments():\n",
    "    parser = argparse.ArgumentParser(description=\"KGC rule generation parameters\")\n",
    "    parser.add_argument(\"--data_path\", type=str, default=\"datasets\", help=\"Data directory\")\n",
    "    parser.add_argument(\"--dataset\", \"-d\", type=str, default=\"icews14\", help=\"Dataset name\")\n",
    "    parser.add_argument(\"--sampled_paths\", type=str, default=\"sampled_path\", help=\"Sampled path directory\")\n",
    "    parser.add_argument(\"--prompt_paths\", type=str, default=\"prompt\", help=\"Sampled path directory\")\n",
    "    parser.add_argument(\"--rule_path\", type=str, default=\"gen_rules_iteration\", help=\"Path to rule file\")\n",
    "    parser.add_argument(\"--model_name\", type=str, default=\"gpt-4.1-nano\", help=\"Model name\")\n",
    "    parser.add_argument(\"--is_zero\", action=\"store_true\", help=\"Enable zero-shot rule generation\")\n",
    "    parser.add_argument(\"-k\", type=int, default=0, help=\"Number of generated rules, 0 denotes as much as possible\")\n",
    "    parser.add_argument(\"-f\", type=int, default=50, help=\"Few-shot number\")\n",
    "    parser.add_argument(\"-topk\", type=int, default=20, help=\"Top-k paths\")\n",
    "    parser.add_argument(\"-n\", type=int, default=10, help=\"Number of threads\")  # 변경했음\n",
    "    parser.add_argument(\"-l\", type=int, default=5, help=\"Sample times for generating k rules\")\n",
    "    parser.add_argument(\"--prefix\", type=str, default=\"\", help=\"Prefix for files\")\n",
    "    parser.add_argument(\"--dry_run\", action=\"store_true\", help=\"Dry run mode\")\n",
    "    parser.add_argument(\"--is_rel_name\", type=str_to_bool, default=\"yes\", help=\"Enable relation names\")\n",
    "    parser.add_argument(\"--select_with_confidence\", type=str_to_bool, default=\"no\", help=\"Select with confidence\")\n",
    "    parser.add_argument(\"--clean_only\", action=\"store_true\", help=\"Load summarized rules and clean rules only\")\n",
    "    # parser.add_argument(\"--force_summarize\", action=\"store_true\", help=\"Force summarize rules\")\n",
    "    parser.add_argument(\"--is_merge\", type=str_to_bool, default=\"no\", help=\"Enable merge\")\n",
    "    parser.add_argument(\"--transition_distr\", type=str, default=\"exp\", help=\"Transition distribution\")\n",
    "    parser.add_argument(\"--is_only_with_original_rules\", type=str_to_bool, default=\"no\", help=\"Use only original rules\")\n",
    "    parser.add_argument(\"--is_high\", type=str_to_bool, default=\"No\", help=\"Enable high mode\")\n",
    "    parser.add_argument(\"--min_conf\", type=float, default=0.01, help=\"Minimum confidence\")\n",
    "    parser.add_argument(\"--num_iter\", type=int, default=2, help=\"Number of iterations\")\n",
    "    parser.add_argument(\"-second\", type=int, default=3, help=\"Second sampling times for generating k rules\")\n",
    "    parser.add_argument(\n",
    "        \"--bgkg\",\n",
    "        type=str,\n",
    "        default=\"valid\",\n",
    "        choices=[\"train\", \"train_valid\", \"valid\", \"test\"],\n",
    "        help=\"Background knowledge graph\",\n",
    "    )\n",
    "    parser.add_argument(\"--based_rule_type\", type=str, default=\"low\", choices=[\"low\", \"high\"], help=\"Base rule type\")\n",
    "    parser.add_argument(\n",
    "        \"--rule_domain\", type=str, default=\"iteration\", choices=[\"iteration\", \"high\", \"all\"], help=\"Rule domain\"\n",
    "    )\n",
    "\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "    return args, parser\n",
    "\n",
    "registed_language_models = {\n",
    "    'gpt-4.1-nano': ChatGPT,\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "def get_registed_model(model_name) -> BaseLanguageModel:\n",
    "    for key, value in registed_language_models.items():\n",
    "        if key in model_name.lower():\n",
    "            return value\n",
    "    raise ValueError(f\"No registered model found for name {model_name}\")\n",
    "\n",
    "\n",
    "args, parser = parse_arguments()\n",
    "LLM = get_registed_model(args.model_name)\n",
    "LLM.add_args(parser)\n",
    "args, _ = parser.parse_known_args()\n",
    "main(args, LLM)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
