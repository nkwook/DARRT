{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Dynamic rule adaption (1) rule sampling\n",
    "\n",
    "## Input\n",
    "- files in /datasets\n",
    "\n",
    "## Output\n",
    "- files in /sampled_path\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Setup Requirements\n",
    "- python 3.12\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!pip install joblib>=1.5.0 \\\n",
    "    matplotlib \\\n",
    "    networkx==3.2.1 \\\n",
    "    numpy==1.26.3 \\\n",
    "    openai==1.79.0 \\\n",
    "    pandas==2.2.0 \\\n",
    "    python-dotenv==1.0.0 \\\n",
    "    scipy==1.12.0 \\\n",
    "    seaborn==0.13.1 \\\n",
    "    sentence-transformers>=3.0.1 \\\n",
    "    tiktoken==0.9.0 \\\n",
    "    torch==2.4.0 \\\n",
    "    tqdm==4.66.1 \\\n",
    "    transformers==4.35.2 \\\n",
    "    datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def load_json_data(file_path, default=None):\n",
    "    try:\n",
    "        if os.path.exists(file_path):\n",
    "            print(f\"Use cache from: {file_path}\")\n",
    "            with open(file_path, \"r\") as file:\n",
    "                return json.load(file)\n",
    "        else:\n",
    "            print(f\"File not found: {file_path}\")\n",
    "            return default\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading JSON data from {file_path}: {e}\")\n",
    "    return default\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grapher\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "class Grapher(object):\n",
    "    def __init__(self, dataset_dir, args=None, test_mask=None):\n",
    "        \"\"\"\n",
    "        Store information about the graph (train/valid/test set).\n",
    "        Add corresponding inverse quadruples to the data.\n",
    "\n",
    "        Parameters:\n",
    "            dataset_dir (str): path to the graph dataset directory\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "\n",
    "        self.args = args\n",
    "        self.dataset_dir = dataset_dir\n",
    "        self.entity2id = load_json_data(os.path.join(self.dataset_dir, \"entity2id.json\"))\n",
    "        self.relation2id_old = load_json_data(os.path.join(self.dataset_dir, \"relation2id.json\"))\n",
    "        self.relation2id = self.relation2id_old.copy()\n",
    "        counter = len(self.relation2id_old)\n",
    "        for relation in self.relation2id_old:\n",
    "            self.relation2id[\"inv_\" + relation] = counter  # Inverse relation\n",
    "            counter += 1\n",
    "        self.ts2id = load_json_data(os.path.join(self.dataset_dir, \"ts2id.json\"))\n",
    "        self.id2entity = dict([(v, k) for k, v in self.entity2id.items()])\n",
    "        self.id2relation = dict([(v, k) for k, v in self.relation2id.items()])\n",
    "        self.id2ts = dict([(v, k) for k, v in self.ts2id.items()])\n",
    "\n",
    "        self.inv_relation_id = dict()\n",
    "        num_relations = len(self.relation2id_old)\n",
    "        for i in range(num_relations):\n",
    "            self.inv_relation_id[i] = i + num_relations\n",
    "        for i in range(num_relations, num_relations * 2):\n",
    "            self.inv_relation_id[i] = i % num_relations\n",
    "\n",
    "        self.train_idx = self.create_store(\"train.txt\")\n",
    "        self.valid_idx = self.create_store(\"valid.txt\")\n",
    "        self.test_idx = self.create_store(\"test.txt\")\n",
    "\n",
    "        if test_mask is not None:\n",
    "            mask = (self.test_idx[:, 3] >= test_mask[0]) * (self.test_idx[:, 3] <= test_mask[1])\n",
    "            self.test_idx = self.test_idx[mask]\n",
    "\n",
    "        if self.args is not None:\n",
    "            if self.args[\"bgkg\"] == \"all\":\n",
    "                self.all_idx = np.vstack((self.train_idx, self.valid_idx, self.test_idx))\n",
    "            elif self.args[\"bgkg\"] == \"train\":\n",
    "                self.all_idx = self.train_idx\n",
    "            elif self.args[\"bgkg\"] == \"valid\":\n",
    "                self.all_idx = self.valid_idx\n",
    "            elif self.args[\"bgkg\"] == \"test\":\n",
    "                self.all_idx = self.test_idx\n",
    "            elif self.args[\"bgkg\"] == \"train_valid\":\n",
    "                self.all_idx = np.vstack((self.train_idx, self.valid_idx))\n",
    "            elif self.args[\"bgkg\"] == \"train_test\":\n",
    "                self.all_idx = np.vstack((self.train_idx, self.test_idx))\n",
    "            elif self.args[\"bgkg\"] == \"valid_test\":\n",
    "                self.all_idx = np.vstack((self.valid_idx, self.test_idx))\n",
    "\n",
    "        print(\"Grapher initialized.\")\n",
    "\n",
    "    def create_store(self, file):\n",
    "        \"\"\"\n",
    "        Store the quadruples from the file as indices.\n",
    "        The quadruples in the file should be in the format \"subject\\trelation\\tobject\\ttimestamp\\n\".\n",
    "\n",
    "        Parameters:\n",
    "            file (str): file name\n",
    "\n",
    "        Returns:\n",
    "            store_idx (np.ndarray): indices of quadruples\n",
    "        \"\"\"\n",
    "\n",
    "        with open(os.path.join(self.dataset_dir, file), \"r\", encoding=\"utf-8\") as f:\n",
    "            quads = f.readlines()\n",
    "        store = self.split_quads(quads)\n",
    "        store_idx = self.map_to_idx(store)\n",
    "        store_idx = self.add_inverses(store_idx)\n",
    "\n",
    "        return store_idx\n",
    "\n",
    "    def split_quads(self, quads):\n",
    "        \"\"\"\n",
    "        Split quadruples into a list of strings.\n",
    "\n",
    "        Parameters:\n",
    "            quads (list): list of quadruples\n",
    "                          Each quadruple has the form \"subject\\trelation\\tobject\\ttimestamp\\n\".\n",
    "\n",
    "        Returns:\n",
    "            split_q (list): list of quadruples\n",
    "                            Each quadruple has the form [subject, relation, object, timestamp].\n",
    "        \"\"\"\n",
    "\n",
    "        split_q = []\n",
    "        for quad in quads:\n",
    "            split_q.append(quad[:-1].split(\"\\t\"))\n",
    "\n",
    "        return split_q\n",
    "\n",
    "    def map_to_idx(self, quads):\n",
    "        \"\"\"\n",
    "        Map quadruples to their indices.\n",
    "\n",
    "        Parameters:\n",
    "            quads (list): list of quadruples\n",
    "                          Each quadruple has the form [subject, relation, object, timestamp].\n",
    "\n",
    "        Returns:\n",
    "            quads (np.ndarray): indices of quadruples\n",
    "        \"\"\"\n",
    "\n",
    "        subs = [self.entity2id[x[0]] for x in quads]\n",
    "        rels = [self.relation2id[x[1]] for x in quads]\n",
    "        objs = [self.entity2id[x[2]] for x in quads]\n",
    "        tss = [self.ts2id[x[3]] for x in quads]\n",
    "        quads = np.column_stack((subs, rels, objs, tss))\n",
    "\n",
    "        return quads\n",
    "\n",
    "    def add_inverses(self, quads_idx):\n",
    "        \"\"\"\n",
    "        Add the inverses of the quadruples as indices.\n",
    "\n",
    "        Parameters:\n",
    "            quads_idx (np.ndarray): indices of quadruples\n",
    "\n",
    "        Returns:\n",
    "            quads_idx (np.ndarray): indices of quadruples along with the indices of their inverses\n",
    "        \"\"\"\n",
    "\n",
    "        subs = quads_idx[:, 2]\n",
    "        rels = [self.inv_relation_id[x] for x in quads_idx[:, 1]]\n",
    "        objs = quads_idx[:, 0]\n",
    "        tss = quads_idx[:, 3]\n",
    "        inv_quads_idx = np.column_stack((subs, rels, objs, tss))\n",
    "        quads_idx = np.vstack((quads_idx, inv_quads_idx))\n",
    "\n",
    "        return quads_idx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def initialize_temporal_walk(version_id, data, transition_distr):\n",
    "    idx_map = {\n",
    "        \"all\": np.array(data.train_idx.tolist() + data.valid_idx.tolist() + data.test_idx.tolist()),\n",
    "        \"train_valid\": np.array(data.train_idx.tolist() + data.valid_idx.tolist()),\n",
    "        \"train\": np.array(data.train_idx.tolist()),\n",
    "        \"test\": np.array(data.test_idx.tolist()),\n",
    "        \"valid\": np.array(data.valid_idx.tolist()),\n",
    "    }\n",
    "    return Temporal_Walk(idx_map[version_id], data.inv_relation_id, transition_distr)\n",
    "\n",
    "def store_neighbors(quads):\n",
    "    \"\"\"\n",
    "    Store all neighbors (outgoing edges) for each node.\n",
    "\n",
    "    Parameters:\n",
    "        quads (np.ndarray): indices of quadruples\n",
    "\n",
    "    Returns:\n",
    "        neighbors (dict): neighbors for each node\n",
    "    \"\"\"\n",
    "\n",
    "    # 将 quads 转换为 DataFrame\n",
    "    df = pd.DataFrame(quads, columns=[\"head\", \"relation\", \"target\", \"timestamp\"])\n",
    "\n",
    "    # 按 'node' 列分组，并将每组转换为数组\n",
    "    neighbors = {node: group.values for node, group in df.groupby(\"head\")}\n",
    "\n",
    "    return neighbors\n",
    "\n",
    "\n",
    "def store_edges(quads):\n",
    "    \"\"\"\n",
    "    Store all edges for each relation.\n",
    "\n",
    "    Parameters:\n",
    "        quads (np.ndarray): indices of quadruples\n",
    "\n",
    "    Returns:\n",
    "        edges (dict): edges for each relation\n",
    "    \"\"\"\n",
    "\n",
    "    edges = dict()\n",
    "    relations = list(set(quads[:, 1]))\n",
    "    for rel in relations:\n",
    "        edges[rel] = quads[quads[:, 1] == rel]\n",
    "\n",
    "    return edges\n",
    "\n",
    "class Temporal_Walk(object):\n",
    "    def __init__(self, learn_data, inv_relation_id, transition_distr):\n",
    "        \"\"\"\n",
    "        Initialize temporal random walk object.\n",
    "\n",
    "        Parameters:\n",
    "            learn_data (np.ndarray): data on which the rules should be learned\n",
    "            inv_relation_id (dict): mapping of relation to inverse relation\n",
    "            transition_distr (str): transition distribution\n",
    "                                    \"unif\" - uniform distribution\n",
    "                                    \"exp\"  - exponential distribution\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "\n",
    "        self.learn_data = learn_data\n",
    "        self.inv_relation_id = inv_relation_id\n",
    "        self.transition_distr = transition_distr\n",
    "        self.neighbors = store_neighbors(learn_data)\n",
    "        self.edges = store_edges(learn_data)\n",
    "\n",
    "    def sample_start_edge(self, rel_idx):\n",
    "        \"\"\"\n",
    "        Define start edge distribution.\n",
    "\n",
    "        Parameters:\n",
    "            rel_idx (int): relation index\n",
    "\n",
    "        Returns:\n",
    "            start_edge (np.ndarray): start edge\n",
    "        \"\"\"\n",
    "\n",
    "        rel_edges = self.edges[rel_idx]\n",
    "        start_edge = rel_edges[np.random.choice(len(rel_edges))]\n",
    "\n",
    "        return start_edge\n",
    "\n",
    "    def sample_next_edge(self, filtered_edges, cur_ts):\n",
    "        \"\"\"\n",
    "        Define next edge distribution.\n",
    "\n",
    "        Parameters:\n",
    "            filtered_edges (np.ndarray): filtered (according to time) edges\n",
    "            cur_ts (int): current timestamp\n",
    "\n",
    "        Returns:\n",
    "            next_edge (np.ndarray): next edge\n",
    "        \"\"\"\n",
    "\n",
    "        if self.transition_distr == \"unif\":\n",
    "            next_edge = filtered_edges[np.random.choice(len(filtered_edges))]\n",
    "        elif self.transition_distr == \"exp\":\n",
    "            tss = filtered_edges[:, 3]\n",
    "            prob = np.exp(tss - cur_ts)\n",
    "            try:\n",
    "                prob = prob / np.sum(prob)\n",
    "                next_edge = filtered_edges[np.random.choice(range(len(filtered_edges)), p=prob)]\n",
    "            except ValueError:  # All timestamps are far away\n",
    "                next_edge = filtered_edges[np.random.choice(len(filtered_edges))]\n",
    "\n",
    "        return next_edge\n",
    "\n",
    "    def transition_step(self, cur_node, cur_ts, prev_edge, start_node, step, L, target_cur_ts=None):\n",
    "        \"\"\"\n",
    "        Sample a neighboring edge given the current node and timestamp.\n",
    "        In the second step (step == 1), the next timestamp should be smaller than the current timestamp.\n",
    "        In the other steps, the next timestamp should be smaller than or equal to the current timestamp.\n",
    "        In the last step (step == L-1), the edge should connect to the source of the walk (cyclic walk).\n",
    "        It is not allowed to go back using the inverse edge.\n",
    "\n",
    "        Parameters:\n",
    "            cur_node (int): current node\n",
    "            cur_ts (int): current timestamp\n",
    "            prev_edge (np.ndarray): previous edge\n",
    "            start_node (int): start node\n",
    "            step (int): number of current step\n",
    "            L (int): length of random walk\n",
    "            target_cur_ts (int, optional): target current timestamp for relaxed time. Defaults to cur_ts.\n",
    "\n",
    "        Returns:\n",
    "            next_edge (np.ndarray): next edge\n",
    "        \"\"\"\n",
    "\n",
    "        next_edges = self.neighbors[cur_node]\n",
    "        if target_cur_ts is None:\n",
    "            target_cur_ts = cur_ts\n",
    "\n",
    "        if step == 1:  # The next timestamp should be smaller than the current timestamp\n",
    "            filtered_edges = next_edges[next_edges[:, 3] < target_cur_ts]\n",
    "        else:  # The next timestamp should be smaller than or equal to the current timestamp\n",
    "            filtered_edges = next_edges[next_edges[:, 3] <= target_cur_ts]\n",
    "            # Delete inverse edge\n",
    "            inv_edge = [\n",
    "                cur_node,\n",
    "                self.inv_relation_id[prev_edge[1]],\n",
    "                prev_edge[0],\n",
    "                cur_ts,\n",
    "            ]\n",
    "            row_idx = np.where(np.all(filtered_edges == inv_edge, axis=1))\n",
    "            filtered_edges = np.delete(filtered_edges, row_idx, axis=0)\n",
    "\n",
    "        if step == L - 1:  # Find an edge that connects to the source of the walk\n",
    "            filtered_edges = filtered_edges[filtered_edges[:, 2] == start_node]\n",
    "\n",
    "        if len(filtered_edges):\n",
    "            next_edge = self.sample_next_edge(filtered_edges, cur_ts)\n",
    "        else:\n",
    "            next_edge = []\n",
    "\n",
    "        return next_edge\n",
    "\n",
    "    def transition_step_with_relax_time(self, cur_node, cur_ts, prev_edge, start_node, step, L, target_cur_ts):\n",
    "        \"\"\"\n",
    "        Wrapper for transition_step with relaxed time handling.\n",
    "\n",
    "        Parameters:\n",
    "            cur_node (int): current node\n",
    "            cur_ts (int): current timestamp\n",
    "            prev_edge (np.ndarray): previous edge\n",
    "            start_node (int): start node\n",
    "            step (int): number of current step\n",
    "            L (int): length of random walk\n",
    "            target_cur_ts (int): target current timestamp for relaxed time\n",
    "\n",
    "        Returns:\n",
    "            next_edge (np.ndarray): next edge\n",
    "        \"\"\"\n",
    "        return self.transition_step(cur_node, cur_ts, prev_edge, start_node, step, L, target_cur_ts)\n",
    "\n",
    "    def sample_walk(self, L, rel_idx, use_relax_time=False):\n",
    "        \"\"\"\n",
    "        Try to sample a cyclic temporal random walk of length L (for a rule of length L-1).\n",
    "\n",
    "        Parameters:\n",
    "            L (int): length of random walk\n",
    "            rel_idx (int): relation index\n",
    "            use_relax_time (bool): whether to use relaxed time sampling\n",
    "\n",
    "        Returns:\n",
    "            walk_successful (bool): if a cyclic temporal random walk has been successfully sampled\n",
    "            walk (dict): information about the walk (entities, relations, timestamps)\n",
    "        \"\"\"\n",
    "\n",
    "        walk_successful = True\n",
    "        walk = dict()\n",
    "        prev_edge = self.sample_start_edge(rel_idx)\n",
    "        start_node = prev_edge[0]\n",
    "        cur_node = prev_edge[2]\n",
    "        cur_ts = prev_edge[3]\n",
    "        target_cur_ts = cur_ts\n",
    "        walk[\"entities\"] = [start_node, cur_node]\n",
    "        walk[\"relations\"] = [prev_edge[1]]\n",
    "        walk[\"timestamps\"] = [cur_ts]\n",
    "\n",
    "        for step in range(1, L):\n",
    "            if use_relax_time:\n",
    "                next_edge = self.transition_step_with_relax_time(\n",
    "                    cur_node, cur_ts, prev_edge, start_node, step, L, target_cur_ts\n",
    "                )\n",
    "            else:\n",
    "                next_edge = self.transition_step(cur_node, cur_ts, prev_edge, start_node, step, L)\n",
    "\n",
    "            if len(next_edge):\n",
    "                cur_node = next_edge[2]\n",
    "                cur_ts = next_edge[3]\n",
    "                walk[\"relations\"].append(next_edge[1])\n",
    "                walk[\"entities\"].append(cur_node)\n",
    "                walk[\"timestamps\"].append(cur_ts)\n",
    "                prev_edge = next_edge\n",
    "            else:  # No valid neighbors (due to temporal or cyclic constraints)\n",
    "                walk_successful = False\n",
    "                break\n",
    "\n",
    "        return walk_successful, walk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rule learner \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import itertools\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "import copy\n",
    "import re\n",
    "import traceback\n",
    "\n",
    "def save_json_data(data, file_path):\n",
    "    try:\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            json.dump(data, file, indent=4, ensure_ascii=False)\n",
    "        print(f\"Data has been converted to JSON and saved to {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving JSON data to {file_path}: {e}\")\n",
    "\n",
    "\n",
    "def write_to_file(content, path):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as fout:\n",
    "        fout.write(content)\n",
    "\n",
    "class Rule_Learner(object):\n",
    "    def __init__(self, edges, id2relation, inv_relation_id, dataset):\n",
    "        \"\"\"\n",
    "        Initialize rule learner object.\n",
    "\n",
    "        Parameters:\n",
    "            edges (dict): edges for each relation\n",
    "            id2relation (dict): mapping of index to relation\n",
    "            inv_relation_id (dict): mapping of relation to inverse relation\n",
    "            dataset (str): dataset name\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "\n",
    "        self.edges = edges\n",
    "        self.id2relation = id2relation\n",
    "        self.inv_relation_id = inv_relation_id\n",
    "        self.num_individual = 0\n",
    "        self.num_shared = 0\n",
    "        self.num_original = 0\n",
    "\n",
    "        self.found_rules = []\n",
    "        self.rule2confidence_dict = {}\n",
    "        self.original_found_rules = []\n",
    "        self.rules_dict = dict()\n",
    "        self.output_dir = \"./sampled_path/\" + dataset + \"/\"\n",
    "        if not os.path.exists(self.output_dir):\n",
    "            os.makedirs(self.output_dir)\n",
    "\n",
    "    def create_rule(self, walk, confidence=0, use_relax_time=False):\n",
    "        \"\"\"\n",
    "        Create a rule given a cyclic temporal random walk.\n",
    "        The rule contains information about head relation, body relations,\n",
    "        variable constraints, confidence, rule support, and body support.\n",
    "        A rule is a dictionary with the content\n",
    "        {\"head_rel\": int, \"body_rels\": list, \"var_constraints\": list,\n",
    "         \"conf\": float, \"rule_supp\": int, \"body_supp\": int}\n",
    "\n",
    "        Parameters:\n",
    "            walk (dict): cyclic temporal random walk\n",
    "                         {\"entities\": list, \"relations\": list, \"timestamps\": list}\n",
    "            confidence (float): confidence value\n",
    "            use_relax_time (bool): whether the rule is created with relaxed time\n",
    "\n",
    "        Returns:\n",
    "            rule (dict): created rule\n",
    "        \"\"\"\n",
    "\n",
    "        rule = dict()\n",
    "        rule[\"head_rel\"] = int(walk[\"relations\"][0])\n",
    "        rule[\"body_rels\"] = [self.inv_relation_id[x] for x in walk[\"relations\"][1:][::-1]]\n",
    "        rule[\"var_constraints\"] = self.define_var_constraints(walk[\"entities\"][1:][::-1])\n",
    "\n",
    "        if rule not in self.found_rules:\n",
    "            self.found_rules.append(rule.copy())\n",
    "            (\n",
    "                rule[\"conf\"],\n",
    "                rule[\"rule_supp\"],\n",
    "                rule[\"body_supp\"],\n",
    "            ) = self.estimate_confidence(rule, is_relax_time=use_relax_time)\n",
    "\n",
    "            rule[\"llm_confidence\"] = confidence\n",
    "\n",
    "            if rule[\"conf\"] or confidence:\n",
    "                self.update_rules_dict(rule)\n",
    "\n",
    "    def create_rule_for_merge(\n",
    "        self, walk, confidence=0, rule_without_confidence=\"\", rules_var_dict=None, is_merge=False, is_relax_time=False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Create a rule given a cyclic temporal random walk.\n",
    "        The rule contains information about head relation, body relations,\n",
    "        variable constraints, confidence, rule support, and body support.\n",
    "        A rule is a dictionary with the content\n",
    "        {\"head_rel\": int, \"body_rels\": list, \"var_constraints\": list,\n",
    "         \"conf\": float, \"rule_supp\": int, \"body_supp\": int}\n",
    "\n",
    "        Parameters:\n",
    "            walk (dict): cyclic temporal random walk\n",
    "                         {\"entities\": list, \"relations\": list, \"timestamps\": list}\n",
    "\n",
    "        Returns:\n",
    "            rule (dict): created rule\n",
    "        \"\"\"\n",
    "\n",
    "        rule = dict()\n",
    "        rule[\"head_rel\"] = int(walk[\"relations\"][0])\n",
    "        rule[\"body_rels\"] = [self.inv_relation_id[x] for x in walk[\"relations\"][1:][::-1]]\n",
    "        rule[\"var_constraints\"] = self.define_var_constraints(walk[\"entities\"][1:][::-1])\n",
    "\n",
    "        if is_merge is True:\n",
    "            if rules_var_dict.get(rule_without_confidence) is None:\n",
    "                if rule not in self.found_rules:\n",
    "                    self.found_rules.append(rule.copy())\n",
    "                    (\n",
    "                        rule[\"conf\"],\n",
    "                        rule[\"rule_supp\"],\n",
    "                        rule[\"body_supp\"],\n",
    "                    ) = self.estimate_confidence(rule)\n",
    "\n",
    "                    rule[\"llm_confidence\"] = confidence\n",
    "\n",
    "                    if rule[\"conf\"] or confidence:\n",
    "                        self.num_individual += 1\n",
    "                        self.update_rules_dict(rule)\n",
    "\n",
    "            else:\n",
    "                rule_var = rules_var_dict[rule_without_confidence]\n",
    "                rule_var[\"llm_confidence\"] = confidence\n",
    "                temp_var = {}\n",
    "                temp_var[\"head_rel\"] = rule_var[\"head_rel\"]\n",
    "                temp_var[\"body_rels\"] = rule_var[\"body_rels\"]\n",
    "                temp_var[\"var_constraints\"] = rule_var[\"var_constraints\"]\n",
    "                if temp_var not in self.original_found_rules:\n",
    "                    self.original_found_rules.append(temp_var.copy())\n",
    "                    self.update_rules_dict(rule_var)\n",
    "                    self.num_shared += 1\n",
    "        else:\n",
    "            if rule not in self.found_rules:\n",
    "                self.found_rules.append(rule.copy())\n",
    "                (\n",
    "                    rule[\"conf\"],\n",
    "                    rule[\"rule_supp\"],\n",
    "                    rule[\"body_supp\"],\n",
    "                ) = self.estimate_confidence(rule, is_relax_time=is_relax_time)\n",
    "\n",
    "                # if rule[\"body_supp\"] == 0:\n",
    "                #     rule[\"body_supp\"] = 2\n",
    "\n",
    "                rule[\"llm_confidence\"] = confidence\n",
    "\n",
    "                if rule[\"conf\"] or confidence:\n",
    "                    self.update_rules_dict(rule)\n",
    "\n",
    "    def create_rule_for_merge_for_iteration(\n",
    "        self, walk, llm_confidence=0, rule_without_confidence=\"\", rules_var_dict=None, is_merge=False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Create a rule given a cyclic temporal random walk.\n",
    "        The rule contains information about head relation, body relations,\n",
    "        variable constraints, confidence, rule support, and body support.\n",
    "        A rule is a dictionary with the content\n",
    "        {\"head_rel\": int, \"body_rels\": list, \"var_constraints\": list,\n",
    "         \"conf\": float, \"rule_supp\": int, \"body_supp\": int}\n",
    "\n",
    "        Parameters:\n",
    "            walk (dict): cyclic temporal random walk\n",
    "                         {\"entities\": list, \"relations\": list, \"timestamps\": list}\n",
    "\n",
    "        Returns:\n",
    "            rule (dict): created rule\n",
    "        \"\"\"\n",
    "\n",
    "        rule = dict()\n",
    "        rule[\"head_rel\"] = int(walk[\"relations\"][0])\n",
    "        rule[\"body_rels\"] = [self.inv_relation_id[x] for x in walk[\"relations\"][1:][::-1]]\n",
    "        rule[\"var_constraints\"] = self.define_var_constraints(walk[\"entities\"][1:][::-1])\n",
    "\n",
    "        rule_with_confidence = \"\"\n",
    "\n",
    "        if is_merge is True:\n",
    "            if rules_var_dict.get(rule_without_confidence) is None:\n",
    "                if rule not in self.found_rules:\n",
    "                    self.found_rules.append(rule.copy())\n",
    "                    (\n",
    "                        rule[\"conf\"],\n",
    "                        rule[\"rule_supp\"],\n",
    "                        rule[\"body_supp\"],\n",
    "                    ) = self.estimate_confidence(rule)\n",
    "\n",
    "                    tuple_key = str(rule)\n",
    "                    self.rule2confidence_dict[tuple_key] = rule[\"conf\"]\n",
    "                    rule_with_confidence = rule_without_confidence + \"&\" + str(rule[\"conf\"])\n",
    "\n",
    "                    rule[\"llm_confidence\"] = llm_confidence\n",
    "\n",
    "                    if rule[\"conf\"] or llm_confidence:\n",
    "                        self.num_individual += 1\n",
    "                        self.update_rules_dict(rule)\n",
    "                else:\n",
    "                    tuple_key = tuple(rule)\n",
    "                    confidence = self.rule2confidence_dict[tuple_key]\n",
    "                    rule_with_confidence = rule_without_confidence + \"&\" + confidence\n",
    "\n",
    "            else:\n",
    "                rule_var = rules_var_dict[rule_without_confidence]\n",
    "                rule_var[\"llm_confidence\"] = llm_confidence\n",
    "                temp_var = {}\n",
    "                temp_var[\"head_rel\"] = rule_var[\"head_rel\"]\n",
    "                temp_var[\"body_rels\"] = rule_var[\"body_rels\"]\n",
    "                temp_var[\"var_constraints\"] = rule_var[\"var_constraints\"]\n",
    "                if temp_var not in self.original_found_rules:\n",
    "                    self.original_found_rules.append(temp_var.copy())\n",
    "                    self.update_rules_dict(rule_var)\n",
    "                    self.num_shared += 1\n",
    "        else:\n",
    "            if rule not in self.found_rules:\n",
    "                tuple_key = str(rule)\n",
    "                self.found_rules.append(rule.copy())\n",
    "                (\n",
    "                    rule[\"conf\"],\n",
    "                    rule[\"rule_supp\"],\n",
    "                    rule[\"body_supp\"],\n",
    "                ) = self.estimate_confidence(rule)\n",
    "\n",
    "                self.rule2confidence_dict[tuple_key] = rule[\"conf\"]\n",
    "                rule_with_confidence = rule_without_confidence + \"&\" + str(rule[\"conf\"])\n",
    "\n",
    "                if rule[\"body_supp\"] == 0:\n",
    "                    rule[\"body_supp\"] = 2\n",
    "\n",
    "                rule[\"llm_confidence\"] = llm_confidence\n",
    "\n",
    "                if rule[\"conf\"] or llm_confidence:\n",
    "                    self.update_rules_dict(rule)\n",
    "            else:\n",
    "                tuple_key = str(rule)\n",
    "                confidence = self.rule2confidence_dict[tuple_key]\n",
    "                rule_with_confidence = rule_without_confidence + \"&\" + str(confidence)\n",
    "\n",
    "        return rule_with_confidence\n",
    "\n",
    "    def define_var_constraints(self, entities):\n",
    "        \"\"\"\n",
    "        Define variable constraints, i.e., state the indices of reoccurring entities in a walk.\n",
    "\n",
    "        Parameters:\n",
    "            entities (list): entities in the temporal walk\n",
    "\n",
    "        Returns:\n",
    "            var_constraints (list): list of indices for reoccurring entities\n",
    "        \"\"\"\n",
    "\n",
    "        var_constraints = []\n",
    "        for ent in set(entities):\n",
    "            all_idx = [idx for idx, x in enumerate(entities) if x == ent]\n",
    "            var_constraints.append(all_idx)\n",
    "        var_constraints = [x for x in var_constraints if len(x) > 1]\n",
    "\n",
    "        return sorted(var_constraints)\n",
    "\n",
    "    def estimate_confidence(self, rule, num_samples=2000, is_relax_time=False):\n",
    "        \"\"\"\n",
    "        Estimate the confidence of the rule by sampling bodies and checking the rule support.\n",
    "\n",
    "        Parameters:\n",
    "            rule (dict): rule\n",
    "                         {\"head_rel\": int, \"body_rels\": list, \"var_constraints\": list}\n",
    "            num_samples (int): number of samples\n",
    "\n",
    "        Returns:\n",
    "            confidence (float): confidence of the rule, rule_support/body_support\n",
    "            rule_support (int): rule support\n",
    "            body_support (int): body support\n",
    "        \"\"\"\n",
    "\n",
    "        if any(body_rel not in self.edges for body_rel in rule[\"body_rels\"]):\n",
    "            return 0, 0, 0\n",
    "\n",
    "        if rule[\"head_rel\"] not in self.edges:\n",
    "            return 0, 0, 0\n",
    "\n",
    "        all_bodies = []\n",
    "        for _ in range(num_samples):\n",
    "            sample_successful, body_ents_tss = self.sample_body(\n",
    "                rule[\"body_rels\"], rule[\"var_constraints\"], is_relax_time\n",
    "            )\n",
    "            if sample_successful:\n",
    "                all_bodies.append(body_ents_tss)\n",
    "\n",
    "        all_bodies.sort()\n",
    "        unique_bodies = list(x for x, _ in itertools.groupby(all_bodies))\n",
    "        body_support = len(unique_bodies)\n",
    "\n",
    "        confidence, rule_support = 0, 0\n",
    "        if body_support:\n",
    "            rule_support = self.calculate_rule_support(unique_bodies, rule[\"head_rel\"])\n",
    "            confidence = round(rule_support / body_support, 6)\n",
    "\n",
    "        return confidence, rule_support, body_support\n",
    "\n",
    "    def sample_body(self, body_rels, var_constraints, use_relax_time=False):\n",
    "        \"\"\"\n",
    "        Sample a walk according to the rule body.\n",
    "        The sequence of timesteps should be non-decreasing.\n",
    "\n",
    "        Parameters:\n",
    "            body_rels (list): relations in the rule body\n",
    "            var_constraints (list): variable constraints for the entities\n",
    "            use_relax_time (bool): whether to use relaxed time sampling\n",
    "\n",
    "        Returns:\n",
    "            sample_successful (bool): if a body has been successfully sampled\n",
    "            body_ents_tss (list): entities and timestamps (alternately entity and timestamp)\n",
    "                                  of the sampled body\n",
    "        \"\"\"\n",
    "\n",
    "        sample_successful = True\n",
    "        body_ents_tss = []\n",
    "        cur_rel = body_rels[0]\n",
    "        rel_edges = self.edges[cur_rel]\n",
    "        next_edge = rel_edges[np.random.choice(len(rel_edges))]\n",
    "        cur_ts = next_edge[3]\n",
    "        cur_node = next_edge[2]\n",
    "        body_ents_tss.append(next_edge[0])\n",
    "        body_ents_tss.append(cur_ts)\n",
    "        body_ents_tss.append(cur_node)\n",
    "\n",
    "        for cur_rel in body_rels[1:]:\n",
    "            next_edges = self.edges[cur_rel]\n",
    "            if use_relax_time:\n",
    "                mask = next_edges[:, 0] == cur_node\n",
    "            else:\n",
    "                mask = (next_edges[:, 0] == cur_node) * (next_edges[:, 3] >= cur_ts)\n",
    "\n",
    "            filtered_edges = next_edges[mask]\n",
    "\n",
    "            if len(filtered_edges):\n",
    "                next_edge = filtered_edges[np.random.choice(len(filtered_edges))]\n",
    "                cur_ts = next_edge[3]\n",
    "                cur_node = next_edge[2]\n",
    "                body_ents_tss.append(cur_ts)\n",
    "                body_ents_tss.append(cur_node)\n",
    "            else:\n",
    "                sample_successful = False\n",
    "                break\n",
    "\n",
    "        if sample_successful and var_constraints:\n",
    "            # Check variable constraints\n",
    "            body_var_constraints = self.define_var_constraints(body_ents_tss[::2])\n",
    "            if body_var_constraints != var_constraints:\n",
    "                sample_successful = False\n",
    "\n",
    "        return sample_successful, body_ents_tss\n",
    "\n",
    "    def calculate_rule_support(self, unique_bodies, head_rel):\n",
    "        \"\"\"\n",
    "        Calculate the rule support. Check for each body if there is a timestamp\n",
    "        (larger than the timestamps in the rule body) for which the rule head holds.\n",
    "\n",
    "        Parameters:\n",
    "            unique_bodies (list): bodies from self.sample_body\n",
    "            head_rel (int): head relation\n",
    "\n",
    "        Returns:\n",
    "            rule_support (int): rule support\n",
    "        \"\"\"\n",
    "\n",
    "        rule_support = 0\n",
    "        try:\n",
    "            head_rel_edges = self.edges[head_rel]\n",
    "        except Exception as e:\n",
    "            print(head_rel)\n",
    "        for body in unique_bodies:\n",
    "            mask = (\n",
    "                (head_rel_edges[:, 0] == body[0])\n",
    "                * (head_rel_edges[:, 2] == body[-1])\n",
    "                * (head_rel_edges[:, 3] > body[-2])\n",
    "            )\n",
    "\n",
    "            if True in mask:\n",
    "                rule_support += 1\n",
    "\n",
    "        return rule_support\n",
    "\n",
    "    def update_rules_dict(self, rule):\n",
    "        \"\"\"\n",
    "        Update the rules if a new rule has been found.\n",
    "\n",
    "        Parameters:\n",
    "            rule (dict): generated rule from self.create_rule\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            self.rules_dict[rule[\"head_rel\"]].append(rule)\n",
    "        except KeyError:\n",
    "            self.rules_dict[rule[\"head_rel\"]] = [rule]\n",
    "\n",
    "    def sort_rules_dict(self):\n",
    "        \"\"\"\n",
    "        Sort the found rules for each head relation by decreasing confidence.\n",
    "\n",
    "        Parameters:\n",
    "            None\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "\n",
    "        for rel in self.rules_dict:\n",
    "            self.rules_dict[rel] = sorted(self.rules_dict[rel], key=lambda x: x[\"conf\"], reverse=True)\n",
    "\n",
    "    def save_rules(self, dt, rule_lengths, num_walks, transition_distr, seed):\n",
    "        \"\"\"\n",
    "        Save all rules.\n",
    "\n",
    "        Parameters:\n",
    "            dt (str): time now\n",
    "            rule_lengths (list): rule lengths\n",
    "            num_walks (int): number of walks\n",
    "            transition_distr (str): transition distribution\n",
    "            seed (int): random seed\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "\n",
    "        rules_dict = {int(k): v for k, v in self.rules_dict.items()}\n",
    "        filename = \"{0}_r{1}_n{2}_{3}_s{4}_rules.json\".format(dt, rule_lengths, num_walks, transition_distr, seed)\n",
    "        filename = filename.replace(\" \", \"\")\n",
    "        with open(self.output_dir + filename, \"w\", encoding=\"utf-8\") as fout:\n",
    "            json.dump(rules_dict, fout)\n",
    "\n",
    "    def save_rules_verbalized(self, dt, rule_lengths, num_walks, transition_distr, seed, rel2idx, relation_regex):\n",
    "        \"\"\"\n",
    "        Save all rules in a human-readable format.\n",
    "\n",
    "        Parameters:\n",
    "            dt (str): time now\n",
    "            rule_lengths (list): rule lengths\n",
    "            num_walks (int): number of walks\n",
    "            transition_distr (str): transition distribution\n",
    "            seed (int): random seed\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "\n",
    "        output_original_dir = os.path.join(self.output_dir, \"original/\")\n",
    "        os.makedirs(output_original_dir, exist_ok=True)\n",
    "\n",
    "        rules_str, rules_var = self.verbalize_rules()\n",
    "        save_json_data(rules_var, output_original_dir + \"rules_var.json\")\n",
    "\n",
    "        filename = self.generate_filename(dt, rule_lengths, num_walks, transition_distr, seed, \"rules.txt\")\n",
    "        write_to_file(rules_str, self.output_dir + filename)\n",
    "\n",
    "        original_rule_txt = self.output_dir + filename\n",
    "        remove_filename = self.generate_filename(\n",
    "            dt, rule_lengths, num_walks, transition_distr, seed, \"remove_rules.txt\"\n",
    "        )\n",
    "\n",
    "        rule_id_content = self.remove_first_three_columns(self.output_dir + filename, self.output_dir + remove_filename)\n",
    "\n",
    "        self.parse_and_save_rules(remove_filename, list(rel2idx.keys()), relation_regex, \"closed_rel_paths.jsonl\")\n",
    "        self.parse_and_save_rules_with_names(\n",
    "            remove_filename, rel2idx, relation_regex, \"rules_name.json\", rule_id_content\n",
    "        )\n",
    "        self.parse_and_save_rules_with_ids(rule_id_content, rel2idx, relation_regex, \"rules_id.json\")\n",
    "\n",
    "        self.save_rule_name_with_confidence(\n",
    "            original_rule_txt,\n",
    "            relation_regex,\n",
    "            self.output_dir + \"relation_name_with_confidence.json\",\n",
    "            list(rel2idx.keys()),\n",
    "        )\n",
    "\n",
    "    def verbalize_rules(self):\n",
    "        rules_str = \"\"\n",
    "        rules_var = {}\n",
    "        for rel in self.rules_dict:\n",
    "            for rule in self.rules_dict[rel]:\n",
    "                single_rule = verbalize_rule(rule, self.id2relation) + \"\\n\"\n",
    "                part = re.split(r\"\\s+\", single_rule.strip())\n",
    "                rule_with_confidence = f\"{part[-1]}\"\n",
    "                rules_var[rule_with_confidence] = rule\n",
    "                rules_str += single_rule\n",
    "        return rules_str, rules_var\n",
    "\n",
    "    def generate_filename(self, dt, rule_lengths, num_walks, transition_distr, seed, suffix):\n",
    "        filename = f\"{dt}_r{rule_lengths}_n{num_walks}_{transition_distr}_s{seed}_{suffix}\"\n",
    "        return filename.replace(\" \", \"\")\n",
    "\n",
    "    def remove_first_three_columns(self, input_path, output_path):\n",
    "        rule_id_content = []\n",
    "        with open(input_path, \"r\") as input_file, open(output_path, \"w\", encoding=\"utf-8\") as output_file:\n",
    "            for line in input_file:\n",
    "                columns = line.split()\n",
    "                new_line = \" \".join(columns[3:])\n",
    "                new_line_for_rule_id = \" \".join(columns[3:]) + \"&\" + columns[0] + \"\\n\"\n",
    "                rule_id_content.append(new_line_for_rule_id)\n",
    "                output_file.write(new_line + \"\\n\")\n",
    "        return rule_id_content\n",
    "\n",
    "    def parse_and_save_rules(self, remove_filename, keys, relation_regex, output_filename):\n",
    "        output_file_path = os.path.join(self.output_dir, output_filename)\n",
    "        with open(self.output_dir + remove_filename, \"r\") as file:\n",
    "            lines = file.readlines()\n",
    "            converted_rules = parse_rules_for_path(lines, keys, relation_regex)\n",
    "        with open(output_file_path, \"w\") as file:\n",
    "            for head, paths in converted_rules.items():\n",
    "                json.dump({\"head\": head, \"paths\": paths}, file)\n",
    "                file.write(\"\\n\")\n",
    "        print(f\"Rules have been converted and saved to {output_file_path}\")\n",
    "        return converted_rules\n",
    "\n",
    "    def parse_and_save_rules_with_names(\n",
    "        self, remove_filename, rel2idx, relation_regex, output_filename, rule_id_content\n",
    "    ):\n",
    "        input_file_path = os.path.join(self.output_dir, remove_filename)\n",
    "        output_file_path = os.path.join(self.output_dir, output_filename)\n",
    "        with open(input_file_path, \"r\") as file:\n",
    "            rules_content = file.readlines()\n",
    "            rules_name_dict = parse_rules_for_name(rules_content, list(rel2idx.keys()), relation_regex)\n",
    "        with open(output_file_path, \"w\") as file:\n",
    "            json.dump(rules_name_dict, file, indent=4)\n",
    "        print(f\"Rules have been converted and saved to {output_file_path}\")\n",
    "\n",
    "    def parse_and_save_rules_with_ids(self, rule_id_content, rel2idx, relation_regex, output_filename):\n",
    "        output_file_path = os.path.join(self.output_dir, output_filename)\n",
    "        rules_id_dict = parse_rules_for_id(rule_id_content, rel2idx, relation_regex)\n",
    "        with open(output_file_path, \"w\") as file:\n",
    "            json.dump(rules_id_dict, file, indent=4)\n",
    "        print(f\"Rules have been converted and saved to {output_file_path}\")\n",
    "\n",
    "    def save_rule_name_with_confidence(self, file_path, relation_regex, out_file_path, relations):\n",
    "        rules_dict = {}\n",
    "        with open(file_path, \"r\") as fin:\n",
    "            rules = fin.readlines()\n",
    "            for rule in rules:\n",
    "                # Split the string by spaces to get the columns\n",
    "                columns = rule.split()\n",
    "\n",
    "                # Extract the first and fourth columns\n",
    "                first_column = columns[0]\n",
    "                fourth_column = \"\".join(columns[3:])\n",
    "                output = f\"{fourth_column}&{first_column}\"\n",
    "\n",
    "                regrex_list = fourth_column.split(\"<-\")\n",
    "                match = re.search(relation_regex, regrex_list[0])\n",
    "                if match:\n",
    "                    head = match[1].strip()\n",
    "                    if head not in relations:\n",
    "                        raise ValueError(f\"Not exist relation:{head}\")\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "                if head not in rules_dict:\n",
    "                    rules_dict[head] = []\n",
    "                rules_dict[head].append(output)\n",
    "        save_json_data(rules_dict, out_file_path)\n",
    "\n",
    "\n",
    "def parse_rules_for_path(lines, relations, relation_regex):\n",
    "    converted_rules = {}\n",
    "    for line in lines:\n",
    "        rule = line.strip()\n",
    "        if not rule:\n",
    "            continue\n",
    "        temp_rule = re.sub(r\"\\s*<-\\s*\", \"&\", rule)\n",
    "        regrex_list = temp_rule.split(\"&\")\n",
    "\n",
    "        head = \"\"\n",
    "        body_list = []\n",
    "        for idx, regrex_item in enumerate(regrex_list):\n",
    "            match = re.search(relation_regex, regrex_item)\n",
    "            if match:\n",
    "                rel_name = match.group(1).strip()\n",
    "                if rel_name not in relations:\n",
    "                    raise ValueError(f\"Not exist relation:{rel_name}\")\n",
    "                if idx == 0:\n",
    "                    head = rel_name\n",
    "                    paths = converted_rules.setdefault(head, [])\n",
    "                else:\n",
    "                    body_list.append(rel_name)\n",
    "\n",
    "        path = \"|\".join(body_list)\n",
    "        paths.append(path)\n",
    "\n",
    "    return converted_rules\n",
    "\n",
    "\n",
    "def parse_rules_for_name(lines, relations, relation_regex):\n",
    "    rules_dict = {}\n",
    "    for rule in lines:\n",
    "        temp_rule = re.sub(r\"\\s*<-\\s*\", \"&\", rule)\n",
    "        regrex_list = temp_rule.split(\"&\")\n",
    "        match = re.search(relation_regex, regrex_list[0])\n",
    "        if match:\n",
    "            head = match[1].strip()\n",
    "            if head not in relations:\n",
    "                raise ValueError(f\"Not exist relation:{head}\")\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        if head not in rules_dict:\n",
    "            rules_dict[head] = []\n",
    "        rules_dict[head].append(rule)\n",
    "\n",
    "    return rules_dict\n",
    "\n",
    "\n",
    "def parse_rules_for_id(rules, rel2idx, relation_regex):\n",
    "    rules_dict = {}\n",
    "    for rule in rules:\n",
    "        temp_rule = re.sub(r\"\\s*<-\\s*\", \"&\", rule)\n",
    "        regrex_list = temp_rule.split(\"&\")\n",
    "        match = re.search(relation_regex, regrex_list[0])\n",
    "        if match:\n",
    "            head = match[1].strip()\n",
    "            if head not in rel2idx:\n",
    "                raise ValueError(f\"Relation '{head}' not found in rel2idx\")\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        rule_id = rule2id(rule.rsplit(\"&\", 1)[0], rel2idx, relation_regex)\n",
    "        rule_id = rule_id + \"&\" + rule.rsplit(\"&\", 1)[1].strip()\n",
    "        rules_dict.setdefault(head, []).append(rule_id)\n",
    "    return rules_dict\n",
    "\n",
    "\n",
    "def rule2id(rule, relation2id, relation_regex):\n",
    "    temp_rule = copy.deepcopy(rule)\n",
    "    temp_rule = re.sub(r\"\\s*<-\\s*\", \"&\", temp_rule)\n",
    "    temp_rule = temp_rule.split(\"&\")\n",
    "    rule2id_str = \"\"\n",
    "\n",
    "    try:\n",
    "        for idx, _ in enumerate(temp_rule):\n",
    "            match = re.search(relation_regex, temp_rule[idx])\n",
    "            rel_name = match[1].strip()\n",
    "            subject = match[2].strip()\n",
    "            object = match[3].strip()\n",
    "            timestamp = match[4].strip()\n",
    "            rel_id = relation2id[rel_name]\n",
    "            full_id = f\"{rel_id}({subject},{object},{timestamp})\"\n",
    "            if idx == 0:\n",
    "                full_id = f\"{full_id}<-\"\n",
    "            else:\n",
    "                full_id = f\"{full_id}&\"\n",
    "\n",
    "            rule2id_str += f\"{full_id}\"\n",
    "    except KeyError as keyerror:\n",
    "        # 捕获异常并打印调用栈信息\n",
    "        traceback.print_exc()\n",
    "        raise ValueError(f\"KeyError: {keyerror}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"An error occurred: {rule}\")\n",
    "\n",
    "    return rule2id_str[:-1]\n",
    "\n",
    "\n",
    "def verbalize_rule(rule, id2relation):\n",
    "    \"\"\"\n",
    "    Verbalize the rule to be in a human-readable format.\n",
    "\n",
    "    Parameters:\n",
    "        rule (dict): rule from Rule_Learner.create_rule\n",
    "        id2relation (dict): mapping of index to relation\n",
    "\n",
    "    Returns:\n",
    "        rule_str (str): human-readable rule\n",
    "    \"\"\"\n",
    "\n",
    "    if rule[\"var_constraints\"]:\n",
    "        var_constraints = rule[\"var_constraints\"]\n",
    "        constraints = [x for sublist in var_constraints for x in sublist]\n",
    "        for i in range(len(rule[\"body_rels\"]) + 1):\n",
    "            if i not in constraints:\n",
    "                var_constraints.append([i])\n",
    "        var_constraints = sorted(var_constraints)\n",
    "    else:\n",
    "        var_constraints = [[x] for x in range(len(rule[\"body_rels\"]) + 1)]\n",
    "\n",
    "    rule_str = \"{0:8.6f}  {1:4}  {2:4}  {3}(X0,X{4},T{5})<-\"\n",
    "    obj_idx = [idx for idx in range(len(var_constraints)) if len(rule[\"body_rels\"]) in var_constraints[idx]][0]\n",
    "    rule_str = rule_str.format(\n",
    "        rule[\"conf\"],\n",
    "        rule[\"rule_supp\"],\n",
    "        rule[\"body_supp\"],\n",
    "        id2relation[rule[\"head_rel\"]],\n",
    "        obj_idx,\n",
    "        len(rule[\"body_rels\"]),\n",
    "    )\n",
    "\n",
    "    for i in range(len(rule[\"body_rels\"])):\n",
    "        sub_idx = [idx for idx in range(len(var_constraints)) if i in var_constraints[idx]][0]\n",
    "        obj_idx = [idx for idx in range(len(var_constraints)) if i + 1 in var_constraints[idx]][0]\n",
    "        rule_str += \"{0}(X{1},X{2},T{3})&\".format(id2relation[rule[\"body_rels\"][i]], sub_idx, obj_idx, i)\n",
    "\n",
    "    return rule_str[:-1]\n",
    "\n",
    "\n",
    "def rules_statistics(rules_dict):\n",
    "    \"\"\"\n",
    "    Show statistics of the rules.\n",
    "\n",
    "    Parameters:\n",
    "        rules_dict (dict): rules\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Number of relations with rules: \", len(rules_dict))  # Including inverse relations\n",
    "    print(\"Total number of rules: \", sum([len(v) for k, v in rules_dict.items()]))\n",
    "\n",
    "    lengths = []\n",
    "    for rel in rules_dict:\n",
    "        lengths += [len(x[\"body_rels\"]) for x in rules_dict[rel]]\n",
    "    rule_lengths = [(k, v) for k, v in Counter(lengths).items()]\n",
    "    print(\"Number of rules by length: \", sorted(rule_lengths))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def select_similary_relations(relation2id, output_dir):\n",
    "    id2relation = dict([(v, k) for k, v in relation2id.items()])\n",
    "\n",
    "    save_json_data(id2relation, os.path.join(output_dir, \"transfomers_id2rel.json\"))\n",
    "    save_json_data(relation2id, os.path.join(output_dir, \"transfomers_rel2id.json\"))\n",
    "\n",
    "    all_rels = list(relation2id.keys())\n",
    "    # 사전 훈련된 모델 로드\n",
    "    model = SentenceTransformer(\"bert-base-nli-mean-tokens\")\n",
    "\n",
    "    # 문장 정의\n",
    "    sentences_A = all_rels\n",
    "    sentences_B = all_rels\n",
    "\n",
    "    # 모델을 사용하여 문장 인코딩\n",
    "    embeddings_A = model.encode(sentences_A)\n",
    "    embeddings_B = model.encode(sentences_B)\n",
    "\n",
    "    # 문장 간 코사인 유사도 계산\n",
    "    similarity_matrix = cosine_similarity(embeddings_A, embeddings_B)\n",
    "\n",
    "    np.fill_diagonal(similarity_matrix, 0)\n",
    "\n",
    "    np.save(os.path.join(output_dir, \"matrix.npy\"), similarity_matrix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "def main(parsed):\n",
    "    dataset = parsed[\"dataset\"]\n",
    "    rule_lengths = parsed[\"max_path_len\"]\n",
    "    rule_lengths = (torch.arange(rule_lengths) + 1).tolist()\n",
    "    num_walks = parsed[\"num_walks\"]\n",
    "    transition_distr = parsed[\"transition_distr\"]\n",
    "    num_processes = parsed[\"cores\"]\n",
    "    seed = parsed[\"seed\"]\n",
    "    version_id = parsed[\"version\"]\n",
    "\n",
    "    dataset_dir = \"./datasets/\" + dataset + \"/\"\n",
    "    data = Grapher(dataset_dir)\n",
    "\n",
    "    temporal_walk = initialize_temporal_walk(version_id, data, transition_distr)\n",
    "\n",
    "    rl = Rule_Learner(temporal_walk.edges, data.id2relation, data.inv_relation_id, dataset)\n",
    "    all_relations = sorted(temporal_walk.edges)  # 모든 관계에 대해 학습\n",
    "    all_relations = [int(item) for item in all_relations]\n",
    "    rel2idx = data.relation2id\n",
    "\n",
    "    select_similary_relations(data.relation2id, rl.output_dir)\n",
    "\n",
    "    constant_config = load_json_data(\"./Config/constant.json\")\n",
    "    relation_regex = constant_config[\"relation_regex\"][dataset]\n",
    "\n",
    "    def learn_rules(i, num_relations, use_relax_time=False):\n",
    "        \"\"\"\n",
    "        선택적 완화 시간(멀티프로세싱 가능)으로 규칙을 학습합니다.\n",
    "\n",
    "        매개변수:\n",
    "            i (int): 프로세스 번호\n",
    "            num_relations (int): 각 프로세스에 대한 최소 관계 수\n",
    "            use_relax_time (bool): 샘플링 시 완화 시간 사용 여부\n",
    "\n",
    "        반환값:\n",
    "            rl.rules_dict (dict): 규칙 사전\n",
    "        \"\"\"\n",
    "\n",
    "        set_seed_if_provided()\n",
    "        relations_idx = calculate_relations_idx(i, num_relations)\n",
    "        num_rules = [0]\n",
    "\n",
    "        for k in relations_idx:\n",
    "            rel = all_relations[k]\n",
    "            for length in rule_lengths:\n",
    "                it_start = time.time()\n",
    "                process_rules_for_relation(rel, length, use_relax_time)\n",
    "                it_end = time.time()\n",
    "                it_time = round(it_end - it_start, 6)\n",
    "                num_rules.append(sum([len(v) for k, v in rl.rules_dict.items()]) // 2)\n",
    "                num_new_rules = num_rules[-1] - num_rules[-2]\n",
    "\n",
    "                print(\n",
    "                    f\"Process {i}: relation {k - relations_idx[0] + 1}/{len(relations_idx)}, length {length}: {it_time} sec, {num_new_rules} rules\"\n",
    "                )\n",
    "\n",
    "        return rl.rules_dict\n",
    "\n",
    "    def set_seed_if_provided():\n",
    "        if seed:\n",
    "            np.random.seed(seed)\n",
    "\n",
    "    def calculate_relations_idx(i, num_relations):\n",
    "        if i < num_processes - 1:\n",
    "            return range(i * num_relations, (i + 1) * num_relations)\n",
    "        else:\n",
    "            return range(i * num_relations, len(all_relations))\n",
    "\n",
    "    def process_rules_for_relation(rel, length, use_relax_time):\n",
    "        for _ in range(num_walks):\n",
    "            walk_successful, walk = temporal_walk.sample_walk(length + 1, rel, use_relax_time)\n",
    "            if walk_successful:\n",
    "                rl.create_rule(walk, use_relax_time)\n",
    "\n",
    "    start = time.time()\n",
    "    num_relations = len(all_relations) // num_processes\n",
    "    output = Parallel(n_jobs=num_processes)(\n",
    "        delayed(learn_rules)(i, num_relations, parsed[\"is_relax_time\"]) for i in range(num_processes)\n",
    "    )\n",
    "    end = time.time()\n",
    "    all_graph = output[0]\n",
    "    for i in range(1, num_processes):\n",
    "        all_graph.update(output[i])\n",
    "\n",
    "    total_time = round(end - start, 6)\n",
    "    print(\"학습 완료: {} 초\".format(total_time))\n",
    "\n",
    "    rl.rules_dict = all_graph\n",
    "    rl.sort_rules_dict()\n",
    "    dt = datetime.now()\n",
    "    dt = dt.strftime(\"%d%m%y%H%M%S\")\n",
    "    rl.save_rules(dt, rule_lengths, num_walks, transition_distr, seed)\n",
    "    save_json_data(rl.rules_dict, rl.output_dir + \"confidence.json\")\n",
    "    rules_statistics(rl.rules_dict)\n",
    "    rl.save_rules_verbalized(dt, rule_lengths, num_walks, transition_distr, seed, rel2idx, relation_regex)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uv run rule_sampler.py -d icews14 -m 3 -n 200 -p 16 -s 12 --is_relax_time No \n",
    "\n",
    "\n",
    "parsed = {\n",
    "    \"data_path\": \"datasets\",\n",
    "    \"dataset\": \"icews14\",\n",
    "    \"max_path_len\": 3,\n",
    "    \"anchor\": 5,\n",
    "    \"output_path\": \"sampled_path\",\n",
    "    \"sparsity\": 1,\n",
    "    \"cores\": 20,\n",
    "    \"num_walks\": 100,\n",
    "    \"transition_distr\": \"exp\",\n",
    "    \"seed\": None,\n",
    "    \"window\": 0,\n",
    "    \"version\": \"train\",\n",
    "    \"is_relax_time\": \"no\",\n",
    "}\n",
    "\n",
    "main(parsed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
