{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# LLM Reasoning (1) Fact Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "## Input\n",
    "- Ranked rules(ranked_rules/icews14/confidence.json)\n",
    "\n",
    "## Output\n",
    "- Retrieved facts(retrieved_facts/icews14/prediction_icews14.txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Setup requirements\n",
    "- python 3.12\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: 2: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets==2.13.1 torch==2.4.0 fairscale==0.4.13 fire==0.5.0 numpy<2 tokenizers transformers==4.35.2 tqdm wandb==0.16.0 sentencepiece peft==0.10 PyYAML==6.0.1 setuptools bitsandbytes==0.41.0 scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uv run ./data_utils/retrieve.py --name_of_rules_file confidence.json --dataset icews14 -s ./data/processed_rule2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def read_txt_as_list(path_txt):\n",
    "    with open(path_txt, 'r', encoding='utf-8-sig') as file:\n",
    "        data = file.readlines()\n",
    "    return data\n",
    "\n",
    "\n",
    "def read_json(json_dir):\n",
    "    with open(json_dir, \"r\", encoding=\"utf-8\") as f:\n",
    "        json_data = json.load(f)\n",
    "    return json_data\n",
    "\n",
    "def write_txt(txt_dir, out_list, head='\\t'):\n",
    "    with open(txt_dir, 'w', encoding='utf-8') as txtfile:\n",
    "        for sublist in out_list:\n",
    "            txtfile.write(head.join(map(str, sublist)) + '\\n')\n",
    "\n",
    "def flip_dict(original_dict):\n",
    "    return {v: k for k, v in original_dict.items()}\n",
    "\n",
    "def str_dict(original_dict):\n",
    "    return {str(k): str(v) for k, v in original_dict.items()}\n",
    "\n",
    "\n",
    "def convert(str_in, dict_in):\n",
    "    return dict_in[str_in]\n",
    "\n",
    "def id_words(li, dict_ent, dict_r, dict_t, end=str(0), period=1):\n",
    "    li_new = []\n",
    "    for line in li:\n",
    "        try:\n",
    "            columns = line.strip().split(\"\\t\")\n",
    "    \n",
    "            # columns[0] = str(convert(columns[0], dict_ent))\n",
    "            # columns[1] = str(convert(columns[1], dict_r))\n",
    "            # columns[2] = str(convert(columns[2], dict_ent))\n",
    "            columns[0] = str(columns[0])\n",
    "            columns[1] = str(columns[1])\n",
    "            columns[2] = str(columns[2])\n",
    "            # columns[3] = str(convert(str(int(columns[3]) * period), dict_t))\n",
    "            columns[3] = str(columns[3])\n",
    "            line = \"\\t\".join([columns[0], columns[1], columns[2], columns[3], end])\n",
    "            li_new.append(line)\n",
    "        except Exception as e:\n",
    "            print(f\"Error converting line: {line}\")\n",
    "            print(f\"Error: {e}\")\n",
    "            raise e\n",
    "    return li_new\n",
    "\n",
    "\n",
    "def convert_dataset(li_to_convert, path_workspace, end=str(0), period=1):\n",
    "    relations = read_json(path_workspace + \"relation2id.json\")\n",
    "    entities = read_json(path_workspace + \"entity2id.json\")\n",
    "    times_id = read_json(path_workspace + \"ts2id.json\")\n",
    "    test_ans = id_words(\n",
    "        li_to_convert,\n",
    "        # str_dict(flip_dict(entities)),\n",
    "        # str_dict(flip_dict(relations)),\n",
    "        # str_dict(flip_dict(times_id)),\n",
    "        str_dict(entities),\n",
    "        str_dict(relations),\n",
    "        str_dict(times_id),\n",
    "        end,\n",
    "        period,\n",
    "    )  # convert list in ids to list in words\n",
    "    return test_ans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time as ti\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class Retriever:\n",
    "    def __init__(\n",
    "        self,\n",
    "        test,\n",
    "        all_facts,\n",
    "        entities,\n",
    "        relations,\n",
    "        times_id,\n",
    "        num_relations,\n",
    "        chains,\n",
    "        rel_keys,\n",
    "        dataset,\n",
    "        retrieve_type=\"TLogic\",\n",
    "    ):\n",
    "        self.retrieve_type = retrieve_type\n",
    "        self.dataset = dataset\n",
    "        self.test = test\n",
    "        self.all_facts = all_facts\n",
    "\n",
    "        self.entities = entities\n",
    "        self.relations = relations\n",
    "        self.times_id = times_id\n",
    "        self.num_relations = num_relations\n",
    "        self.chains = chains\n",
    "\n",
    "        self.entities_flip = flip_dict(self.entities)\n",
    "        self.relations_flip = flip_dict(self.relations)\n",
    "        col_sub = []\n",
    "        col_rel = []\n",
    "        col_obj = []\n",
    "        col_time = []\n",
    "        for row in all_facts:\n",
    "            row = row.strip().split(\"\\t\")\n",
    "            col_sub.append(row[0])  # take sub\n",
    "            col_rel.append(row[1])  # Get the relation column of all facts\n",
    "            col_obj.append(row[2])  # Take obj\n",
    "            col_time.append(row[3])  # Time in Str form\n",
    "        self.col_obj = np.array(col_obj)  # To get cand and then search for facts use\n",
    "        self.col_sub = np.array(col_sub)  # To get cand and then search for facts use\n",
    "        self.col_time = np.array(col_time)  # time, str form\n",
    "        self.col_rel = np.array(col_rel)  # relation, str form\n",
    "        all_facts_array = np.array(all_facts)\n",
    "\n",
    "        self.rel_keys = np.array(rel_keys)\n",
    "\n",
    "    def prepare_bs(self, i):\n",
    "        sub, rel, _, time, _ = self.test[i].strip().split(\"\\t\")\n",
    "        idx_t = np.where(self.col_time < time)[0]  # cannot be equal to\n",
    "        s_t = set(idx_t)\n",
    "        idx0 = np.where(self.col_sub == sub)[0]\n",
    "        s0 = set(idx0)\n",
    "        idx = list(s0 & s_t)\n",
    "        idx.sort(reverse=True)\n",
    "        time = self.times_id[time]\n",
    "        return time, sub, rel, idx\n",
    "\n",
    "    def build_bs(self):\n",
    "        # Pure Entity mode\n",
    "        test_text = []\n",
    "        test_idx = []\n",
    "\n",
    "        for i in tqdm(range(0, len(self.test))):  # csv has a header, txt has no header. len(self.test)\n",
    "            num_facts = 50  # 20 or 100\n",
    "            time, sub, rel, idx = self.prepare_bs(i)\n",
    "\n",
    "            facts = []\n",
    "            idx = idx[0:num_facts]\n",
    "            for k in idx:\n",
    "                facts.append(self.all_facts[k])  # Get the facts where sub and rel are the same\n",
    "\n",
    "            if len(facts) < num_facts:\n",
    "                num_facts = len(facts)\n",
    "\n",
    "            histories = self.collect_hist(i, facts, num_facts)\n",
    "            history_query = self.build_history_query(time, sub, rel, histories=histories)\n",
    "\n",
    "            test_idx.append(idx)\n",
    "            test_text.append(history_query)\n",
    "        return test_idx, test_text\n",
    "\n",
    "    def tlogic_prepro(self, i):\n",
    "        test_sub, test_rel, _, test_time, _ = self.test[i].strip().split(\"\\t\")\n",
    "        # First of all, there must be a time premise of retrieve s_t\n",
    "        # Here we need to find out the idx of test in all_facts so that it can be removed\n",
    "        idx_test = len(self.all_facts) - (len(self.test) - 1) + i - 1\n",
    "        # #The major premise is that retrieval must be performed from those ranges earlier than test_time\n",
    "        idx_t = np.where(self.col_time < test_time)[0]\n",
    "        s_t = set(idx_t)\n",
    "        if idx_test in s_t:\n",
    "            s_t.remove(idx_test)  # Remove the test item itself\n",
    "            # Second, the search for the beginning of the chain needs to be restricted to rel==test_sub\n",
    "        idx_test_sub = np.where(self.col_sub == test_sub)[0]\n",
    "        s_test_sub = set(idx_test_sub)\n",
    "        s_0 = s_t & s_test_sub  # Get: major premise\n",
    "        head_rel = self.relations[test_rel]  # Get the idx corresponding to test_relation: 0,1,2,...\n",
    "        time = self.times_id[\n",
    "            test_time\n",
    "        ]  # To move forward the time according to the id corresponding relationship of time. Get int from str\n",
    "        return s_0, s_t, head_rel, int(time), test_sub, test_rel\n",
    "\n",
    "    def build_tl(self):\n",
    "        test_text = []\n",
    "        test_idx = []\n",
    "\n",
    "        for i in tqdm(range(0, len(self.test))):  # Starting from 1 because there is a header. 1, len(test)\n",
    "\n",
    "            num_facts = 50  # Set it again for each question, as the following may change.\n",
    "            s_0, s_t, head_rel, time, test_sub, test_rel = self.tlogic_prepro(i)\n",
    "            facts = []\n",
    "            idx = []\n",
    "\n",
    "            if not str(head_rel) in self.chains:  # If the test relation has no chain, nothing will be done.\n",
    "                # l = ['Just repeat \"No Chains.\"\\n']\n",
    "                history_query = [\n",
    "                    str(int(time / 24)) + \": [\" + test_sub + \", \" + test_rel + \",\\n\"\n",
    "                ]  # TODO : 24로 나누는 부분 추가함\n",
    "                test_idx.append([])  # At this time idx is a blank line\n",
    "                test_text.append(history_query)\n",
    "                # print(i, 'no chain in this line')\n",
    "                continue\n",
    "            s_0 = np.array(list(s_0))  # Convert collection to NumPy array\n",
    "            # After the above preparations, start searching for facts by chain.\n",
    "            idx_chain = []\n",
    "            for k in range(0, len(self.chains[str(head_rel)])):  # There are len(chains[str(head_rel)]) chains\n",
    "                body_rel_len = len(self.chains[str(head_rel)][k][\"body_rels\"])  # The length of this chain\n",
    "                if body_rel_len == 1:\n",
    "                    idx_chain.append(k)\n",
    "            for k in idx_chain:  # TLogic (or TLogic-3), as long as the shortest len=1 chain\n",
    "                idx_case = []\n",
    "                body_rel_len = len(self.chains[str(head_rel)][k][\"body_rels\"])  # The length of this chain\n",
    "                rel = self.chains[str(head_rel)][k][\"body_rels\"][-1] % self.num_relations\n",
    "                idx_rel = np.where(self.col_rel == self.rel_keys[rel])[0]\n",
    "                idx_rel = np.intersect1d(idx_rel, s_0)  # Using NumPy functions for intersection operations\n",
    "                idx_case = idx_rel\n",
    "                idx_case.tolist()\n",
    "                if len(idx_case) != 0:  # If it is not empty, retrieve it.\n",
    "                    idx_case = list(set(idx_case))\n",
    "                    idx = list(set(idx + idx_case))\n",
    "                else:\n",
    "                    continue  # If no such chain exists, jump to the next chain\n",
    "                if len(idx) >= num_facts:\n",
    "                    break  # Break out of the loop on chains and go to the next test\n",
    "            # time reordering\n",
    "            idx.sort(reverse=True)\n",
    "            # Idx with chain.sort(reverse=true)\n",
    "            if len(idx) > num_facts:\n",
    "                idx = idx[0:num_facts]\n",
    "                for a in idx:\n",
    "                    facts.append(self.all_facts[a])\n",
    "            else:\n",
    "                for a in idx:\n",
    "                    facts.append(self.all_facts[a])\n",
    "            test_idx.append(idx)\n",
    "            if len(facts) == 0:  # If the test relation has no chain, nothing will be done.\n",
    "                history_query = self.build_history_query(time, test_sub, test_rel)  # Self.times id[time]\n",
    "                test_text.append(history_query)\n",
    "                # test_idx.append([]) The above line has been appended\n",
    "                continue\n",
    "            if num_facts >= len(facts):\n",
    "                num_facts = len(facts)\n",
    "\n",
    "            histories = self.collect_hist(i, facts, num_facts)\n",
    "            history_query = self.build_history_query(time, test_sub, test_rel, histories=histories)\n",
    "            test_text.append(history_query)\n",
    "            ti.sleep(0.001)\n",
    "        return test_idx, test_text\n",
    "\n",
    "    def collect_hist(self, i, facts, num_facts):\n",
    "        period = 1\n",
    "        if self.dataset == \"icews14\" or self.dataset == \"icews18\":\n",
    "            period = 24\n",
    "        histories = []\n",
    "        facts = facts[0:num_facts]  #\n",
    "        facts.reverse()  # Replace the order so that the last output is the one closest in time.\n",
    "        for b in range(num_facts):\n",
    "            fact = facts[b].strip().split(\"\\t\")\n",
    "            time_in_id = self.times_id[fact[3]]\n",
    "            sub_in_word = fact[0]\n",
    "            rel_in_word = fact[1]\n",
    "\n",
    "            obj_in_word = fact[2]\n",
    "            id_obj = self.entities[obj_in_word]\n",
    "            histories = histories + [\n",
    "                str(int(int(time_in_id) / int(period)))\n",
    "                + \": [\"\n",
    "                + sub_in_word\n",
    "                + \", \"\n",
    "                + rel_in_word\n",
    "                + \", \"\n",
    "                + str(id_obj)\n",
    "                + \".\"\n",
    "                + obj_in_word\n",
    "                + \"] \\n\"\n",
    "            ]\n",
    "        return histories\n",
    "\n",
    "    def build_history_query(self, time, test_sub, test_rel, histories=\"\"):\n",
    "        period = 1\n",
    "        if self.dataset == \"icews14\" or self.dataset == \"icews18\":\n",
    "            period = 24\n",
    "        # time_in_id = self.times_id[time]  # TODO: 이미 id형태의 time이 나와서 로직 변경함.\n",
    "        time_in_id = time\n",
    "\n",
    "        return [\n",
    "            \"\".join(histories)\n",
    "            + str(int(int(time_in_id) / int(period)))\n",
    "            + \": [\"\n",
    "            + test_sub\n",
    "            + \", \"\n",
    "            + test_rel\n",
    "            + \",\\n\"  # times id[time]\n",
    "        ]\n",
    "\n",
    "    def call_function(self, func_name):\n",
    "        func = getattr(self, func_name)\n",
    "        if func and callable(func):\n",
    "            test_idx, test_text = func()\n",
    "        else:\n",
    "            print(\"Retrieve function not found\")\n",
    "        return test_idx, test_text\n",
    "\n",
    "    def get_output(self):\n",
    "        type_retr = \"bs\" if self.retrieve_type == \"bs\" else \"tl\"\n",
    "        test_idx, test_text = self.call_function(\"build_\" + type_retr)\n",
    "\n",
    "        return test_idx, test_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./ranked_rules/icews14/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7371/7371 [01:15<00:00, 97.51it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as  ./retrieved_facts/icews14/test/history_facts/history_facts_icews14.txt and  ./retrieved_facts/icews14/test/history_facts/history_facts_icews14_idx_fine_tune_all.txt\n",
      "saved as  ./retrieved_facts/icews14/test/test_answers/test_answers_icews14.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os, glob\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--dataset\", \"-d\", default=\"icews14\", type=str)\n",
    "parser.add_argument(\"--retrieve_type\", \"-t\", default=\"TLogic-3\", type=str)\n",
    "parser.add_argument(\"--name_of_rules_file\", \"-r\", default=\"confidence.json\", type=str)\n",
    "parser.add_argument(\"--path_save\", \"-s\", default=\"./retrieved_facts/\", type=str)\n",
    "\n",
    "parsed, _ = parser.parse_known_args()\n",
    "parsed = vars(parsed)\n",
    "\n",
    "retrieve_type = parsed[\"retrieve_type\"]\n",
    "type_dataset = parsed[\"dataset\"]\n",
    "name_rules = parsed[\"name_of_rules_file\"]\n",
    "\n",
    "path_workspace = \"./datasets/\" + type_dataset + \"/\"  \n",
    "path_out_tl = \"./ranked_rules/\" + type_dataset + \"/\"\n",
    "print(path_out_tl)\n",
    "\n",
    "path_save = parsed[\"path_save\"] + type_dataset + \"/\"\n",
    "if not os.path.exists(path_save):\n",
    "    os.makedirs(path_save)\n",
    "\n",
    "period = 1\n",
    "if type_dataset == \"icews18\":\n",
    "    num_relations = 256  # for ICEWS18 #set before np.array\n",
    "    period = 24\n",
    "elif type_dataset == \"icews14\":\n",
    "    num_relations = 230\n",
    "    # period = 24 # 데이터 바꾸면서 필요없어짐\n",
    "elif type_dataset == \"GDELT\":\n",
    "    num_relations = 238  # GDELT and\n",
    "else:\n",
    "    num_relations = 24  # YAGO\n",
    "\n",
    "test_ans = []\n",
    "li_files = [\"test\"]  #  ['train','valid','test'] or  ['test'] when only test set is needed\n",
    "\n",
    "for files in li_files:\n",
    "    # print(\"existing rules:\", glob.glob(path_out_tl + \"*rules.json\"))\n",
    "    dir_rules = glob.glob(path_out_tl + \"*rules.json\")[0] if name_rules == \"\" else path_out_tl + name_rules\n",
    "    # print(\"files\", files)\n",
    "    test_ans = read_txt_as_list(path_workspace + files + \".txt\")\n",
    "\n",
    "    relations = read_json(path_workspace + \"relation2id.json\")\n",
    "    entities = read_json(path_workspace + \"entity2id.json\")\n",
    "    times_id = read_json(path_workspace + \"ts2id.json\")\n",
    "\n",
    "    test_ans = convert_dataset(test_ans, path_workspace, period=period)\n",
    "\n",
    "    chains = read_json(path_out_tl + name_rules)\n",
    "    rel_keys = list(relations.keys())\n",
    "    ent_idx = list(entities.keys())  # [0, 1, ...]\n",
    "    times_id_keys = list(times_id.keys())\n",
    "    all_facts = []\n",
    "    with open(path_workspace + \"all_facts.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "        all_facts = f.readlines()\n",
    "\n",
    "    rtr = Retriever(\n",
    "        test_ans, all_facts, entities, relations, times_id, num_relations, chains, rel_keys, dataset=type_dataset\n",
    "    )\n",
    "    test_idx, test_text = rtr.get_output()\n",
    "\n",
    "    path_file = (\n",
    "        path_save + files + \"/history_facts/\" + \"history_facts_\" + type_dataset\n",
    "    )  # \"history_facts_\"+retrieve_type+type_dataset\n",
    "    path_file_word = path_file + \".txt\"\n",
    "    path_file_id = path_file + \"_idx_fine_tune_all.txt\"\n",
    "\n",
    "    if not os.path.exists(path_save + files + \"/history_facts/\"):\n",
    "        os.makedirs(path_save + files + \"/history_facts/\")\n",
    "    write_txt(path_file_id, test_text)\n",
    "    with open(path_file_word, \"w\", encoding=\"utf-8\") as f:\n",
    "        for i in range(len(test_text)):\n",
    "            f.write(test_text[i][0] + \"\\n\")\n",
    "    print(\"saved as \", path_file_word, \"and \", path_file_id)\n",
    "\n",
    "    path_answer = path_save + files + \"/test_answers/\" + \"test_answers_\" + type_dataset + \".txt\"\n",
    "    if not os.path.exists(path_save + files + \"/test_answers/\"):\n",
    "        os.makedirs(path_save + files + \"/test_answers/\")\n",
    "    write_txt(path_answer, test_ans, head=\"\")\n",
    "    print(\"saved as \", path_answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
