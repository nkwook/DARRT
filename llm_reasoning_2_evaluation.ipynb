{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# LLM Reasoning (2) - Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uv run inference.py --LORA_CHECKPOINT_DIR \"./model/icews14\" --output_file \"./results/llmda_new_rule/prediction_icews14.txt\"  --input_file \"./data/processed_rule2/icews14/test/history_facts/history_facts_icews14.txt\"  --test_ans_file \"./data/processed_rule2/icews14/test/test_answers/test_answers_icews14.txt\" >> logs/test-llmda-rule3.log 2>&1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Input\n",
    "- retrieved facts by rule (retrieved_facts/icews14/test/history_facts/history_facts_icews14.txt)\n",
    "- test answers (retrieved_facts/icews14/test/test_answers/test_answers_icews14.txt)\n",
    "\n",
    "## Output\n",
    "- prediction (results/prediction_icews14.txt)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup requirements\n",
    "- python 3.12\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!pip install datasets==2.13.1 torch==2.4.0 fairscale==0.4.13 fire==0.5.0 \"numpy<2\" tokenizers transformers==4.35.2 tqdm wandb==0.16.0 sentencepiece peft==0.10 PyYAML==6.0.1 setuptools bitsandbytes==0.41.0 scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "\n",
    "def read_json(json_dir):\n",
    "    with open(json_dir, \"r\", encoding=\"utf-8\") as f:\n",
    "        json_data = json.load(f)\n",
    "    return json_data\n",
    "\n",
    "def read_txt_as_list(path_txt):\n",
    "    with open(path_txt, 'r', encoding='utf-8-sig') as file:\n",
    "        data = file.readlines()\n",
    "    return data\n",
    "\n",
    "def read_results(path_results, divider=', \\n'): #, \\n for 18 yago llama2 or \\n\n",
    "    with open(path_results, 'r', encoding='utf-8') as file:\n",
    "        content = file.read() #\n",
    "    tests = content.split(', \\n\\n') #, \\n\\n for 18 yago llama2 or \\n\\n\n",
    "    return [re.split(divider, test) for test in tests] \n",
    "\n",
    "def read_num_and_li_results(test):\n",
    "    pattern_end = r'(.+?)( \\n|\"]})'   #'(.+?)( \\n|\"]})' for 18 yago llama2 or (.+?)(, |\"]})\n",
    "    num_Test = re.search(r'\\d+', test[0]).group() if re.search(r'\\d+', test[0]) else \"\"\n",
    "    li_results = [re.match(pattern_end, answer).group(1) if re.match(pattern_end, answer) else answer \\\n",
    "                for answer in test[1:]] \n",
    "    return num_Test, li_results\n",
    "    \n",
    "\n",
    "def get_file_extension(file_path):\n",
    "    _, extension = os.path.splitext(file_path)\n",
    "    return extension    \n",
    "\n",
    "def read_test_an(pth_ans, col=2):\n",
    "    file_type = get_file_extension(pth_ans)\n",
    "    test_ans = []\n",
    "    if file_type == \".csv\":\n",
    "        with open(pth_ans, \"r\", encoding=\"utf-8\") as f:\n",
    "            reader = csv.reader(f)\n",
    "            test_ans = [row1[col] for row1 in reader]  # take obj from [sub, rel, obj, time] as ans\n",
    "            test_ans = test_ans[1:]  #\n",
    "    else:\n",
    "        with open(pth_ans, \"r\", encoding=\"utf-8\") as f:\n",
    "            lines = f.readlines()\n",
    "        for i in range(len(lines)):\n",
    "            test_ans.append(lines[i].split(\"\\t\")[col])\n",
    "    return test_ans\n",
    "\n",
    "def read_test_and_divide(path):  # also in evaler.py\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as file:\n",
    "        content = file.read()\n",
    "    tests = content.split(\"\\n\\n\")\n",
    "    return tests\n",
    "\n",
    "def read_last_metric(last_metric):\n",
    "    if last_metric != \"\":\n",
    "        with open(last_metric, \"r\") as file:\n",
    "\n",
    "            lines = file.readlines()\n",
    "\n",
    "        last_c_k = lines[-4:-1]  # [-5:-2]\n",
    "\n",
    "        last_c_k = [int(line.strip()) for line in last_c_k]\n",
    "\n",
    "        c1 = int(last_c_k[0])\n",
    "        c3 = int(last_c_k[1])\n",
    "        c10 = int(last_c_k[2])\n",
    "\n",
    "    else:  #\n",
    "        c1 = 0  #\n",
    "        c3 = 0\n",
    "        c10 = 0\n",
    "    # print('initial c1: ', c1)\n",
    "    # print('initial c3: ', c3)\n",
    "    # print('initial c10: ', c10 )\n",
    "\n",
    "    return {\"c1\": c1, \"c3\": c3, \"c10\": c10}  # c\n",
    "\n",
    "from transformers import LlamaForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "import torch \n",
    "\n",
    "def decide_model(args):\n",
    "    if args.BIT_8:  #\n",
    "        model = LlamaForCausalLM.from_pretrained(\n",
    "            args.MODEL_NAME,\n",
    "            # load_in_8bit_fp32_cpu_offload=True,\n",
    "            load_in_8bit=True,\n",
    "            # torch_dtype=torch.load_in_8bit,\n",
    "            # device_map=\"auto\",\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "    elif args.BIT_4:\n",
    "        quant_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        )\n",
    "        model = LlamaForCausalLM.from_pretrained(\n",
    "            args.MODEL_NAME,\n",
    "            quantization_config=quant_config,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "    else:\n",
    "        model = LlamaForCausalLM.from_pretrained(\n",
    "            args.MODEL_NAME,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "    return PeftModel.from_pretrained(model, args.LORA_CHECKPOINT_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "## Evaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from transformers import GenerationConfig, LogitsProcessorList, StoppingCriteriaList\n",
    "from typing import TYPE_CHECKING, Any, Callable, Dict, List, Optional, Tuple, Union\n",
    "from transformers.generation.stopping_criteria import (\n",
    "    MaxLengthCriteria,\n",
    "    MaxTimeCriteria,\n",
    "    StoppingCriteria,\n",
    "    StoppingCriteriaList,\n",
    "    validate_stopping_criteria,\n",
    ")\n",
    "import numpy as np\n",
    "import copy\n",
    "import inspect\n",
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.distributed as dist\n",
    "\n",
    "from transformers.deepspeed import is_deepspeed_zero3_enabled\n",
    "from transformers.utils import logging\n",
    "from transformers.generation.beam_constraints import DisjunctiveConstraint, PhrasalConstraint\n",
    "from transformers.generation.beam_search import BeamScorer, BeamSearchScorer, ConstrainedBeamSearchScorer\n",
    "from transformers.generation.configuration_utils import GenerationConfig\n",
    "from transformers.generation.utils import (\n",
    "    GreedySearchEncoderDecoderOutput,\n",
    "    GreedySearchDecoderOnlyOutput,\n",
    "    BeamSearchEncoderDecoderOutput,\n",
    "    BeamSearchDecoderOnlyOutput,\n",
    ")\n",
    "from transformers.generation.logits_process import LogitsProcessorList\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import List, Literal, Optional, Tuple, TypedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from fairscale.nn.model_parallel.initialize import (\n",
    "    get_model_parallel_rank,\n",
    "    initialize_model_parallel,\n",
    "    model_parallel_is_initialized,\n",
    ")\n",
    "from llama2_ori_repo.llama.model import ModelArgs, Transformer\n",
    "from llama2_ori_repo.llama.tokenizer import Tokenizer\n",
    "\n",
    "Role = Literal[\"system\", \"user\", \"assistant\"]\n",
    "\n",
    "if TYPE_CHECKING:\n",
    "    from transformers.modeling_utils import PreTrainedModel\n",
    "    from transformers.generation.streamers import BaseStreamer\n",
    "\n",
    "\n",
    "logger = logging.get_logger(__name__)\n",
    "\n",
    "\n",
    "class Evaler:\n",
    "    def __init__(\n",
    "        self,\n",
    "        topk,\n",
    "        tests,\n",
    "        test_ans,\n",
    "        eval_txt_path,\n",
    "        args,\n",
    "        model=None,\n",
    "        tokenizer=None,\n",
    "        patterns=None,\n",
    "        early_stop_chars=None,\n",
    "        obligations=[],\n",
    "    ):\n",
    "        model_name = args.MODEL_NAME\n",
    "        if not model or not tokenizer:\n",
    "            if \"llama\" in model_name:\n",
    "                self.llama = 1\n",
    "                llama2_directory = model_name.split(\"/models/\")[0]\n",
    "                tokenizer_path = os.path.join(llama2_directory, \"tokenizer.model\")\n",
    "                self.model, self.tokenizer = self.build(\n",
    "                    ckpt_dir=model_name,\n",
    "                    tokenizer_path=tokenizer_path,\n",
    "                    max_seq_len=args.max_seq_len,\n",
    "                    max_batch_size=args.max_batch_size,\n",
    "                )\n",
    "        else:\n",
    "            self.llama = 1\n",
    "            self.model = model\n",
    "            self.tokenizer = tokenizer\n",
    "        self.patterns = patterns\n",
    "        self.tests = tests\n",
    "        self.test_ans = test_ans\n",
    "        self.eval_txt_path = eval_txt_path\n",
    "        self.topk = topk\n",
    "\n",
    "        self.args = args\n",
    "\n",
    "        self.obligations = obligations\n",
    "        self.constraints = []\n",
    "        self.zone_zero = (\n",
    "            early_stop_chars  # in tensors. 0 is '\\n', 29962 is ']'; self.tokenizer.encode(char) gives only a list\n",
    "        )\n",
    "\n",
    "        self.first_check = 0\n",
    "        self.top = 1\n",
    "\n",
    "    def restrict_list_hard(self, tokens, prev_pos, min_prompt_len, input_text_mask, eos_reached, m=0):\n",
    "        logits = self.model.forward(tokens[:, prev_pos:min_prompt_len], prev_pos)\n",
    "        logits_last = logits[:, -1]\n",
    "        # Get the index of the ten tokens of numbers\n",
    "        top_10_indices = torch.topk(logits_last, k=logits.shape[-1], dim=-1).indices\n",
    "        values_to_extract = [29900, 29896, 29906, 29941, 29946, 29945, 29953, 29955, 29947, 29929]  # 0-9 token\n",
    "        top_10_indices_np = top_10_indices.cpu().numpy()\n",
    "        mask = np.isin(top_10_indices_np, values_to_extract)\n",
    "        extracted_elements = top_10_indices_np[mask][:10]\n",
    "        # Convert back to Tensor type\n",
    "        top_10_indices = torch.tensor(extracted_elements)\n",
    "\n",
    "        # print('top 10 idx:', top_10_indices) # [:,0:10]\n",
    "        # Get the token with the m-th highest probability\n",
    "        next_token = top_10_indices[m]\n",
    "        next_token = next_token.reshape(-1)\n",
    "        # print('next token1: ', next_token)\n",
    "\n",
    "        next_token = torch.where(input_text_mask[:, min_prompt_len], tokens[:, min_prompt_len], next_token)\n",
    "\n",
    "        # In addition to getting the token, everything related to last_layer must be updated for the mode.forward of the next cycle\n",
    "        tokens[:, min_prompt_len] = next_token\n",
    "        eos_reached |= (~input_text_mask[:, min_prompt_len]) & (next_token == self.tokenizer.eos_id)\n",
    "\n",
    "        self.first_check = 1  # skip first_check\n",
    "        return next_token, eos_reached\n",
    "\n",
    "    def first_checking(self, next_tokens, next_tokens_scores):\n",
    "        this_peer_finished = False\n",
    "        if self.first_check == 0:  # first check\n",
    "            if self.obligations and (next_tokens not in self.obligations):\n",
    "                this_peer_finished = True\n",
    "                # need to force regenerate/reset next tokens to avoid the constraints\n",
    "                self.first_check = -1  # not begin with nums\n",
    "\n",
    "            if self.constraints and (next_tokens in self.constraints):\n",
    "                self.top += 1\n",
    "                # force regenerate/reset next tokens to avoid the constraints\n",
    "                next_tokens = torch.argsort(next_tokens_scores, dim=-1, descending=True)[:, self.top - 1]\n",
    "                self.constraints.append(next_tokens)\n",
    "                self.first_check = -1  # breach of obligs\n",
    "            else:\n",
    "                self.constraints.append(next_tokens)\n",
    "                self.first_check = 1  # check sign passed\n",
    "        return this_peer_finished, next_tokens\n",
    "\n",
    "    def gen_set_ans(self, tests=\"\", dir_full_test=\"\", dir_time2id=\"\"):\n",
    "        \"\"\"add non-duplicate answer for duplicate queries (no duplicates in sets);\n",
    "        not require order a-z within one timestamp anymore\n",
    "            (to be used in Gdelt & Yago), but may need more time and space\"\"\"\n",
    "        if tests == \"\":\n",
    "            tests = self.tests\n",
    "        dict_qu_ans = {}\n",
    "\n",
    "        if dir_full_test == \"\":\n",
    "            full_test_ans = self.test_ans  # dense and time-well-divided dataset; icews14\n",
    "            for i in tqdm(range(0, len(tests) - 1)):\n",
    "                query = tests[i].split(\"\\n\")[-1]\n",
    "                if query == \"\":\n",
    "                    break\n",
    "                if dict_qu_ans.get(query) == None:\n",
    "                    dict_qu_ans[query] = set()\n",
    "                dict_qu_ans[query].add(full_test_ans[i])  # add answers to the set\n",
    "                time.sleep(0.001)\n",
    "        else:\n",
    "            dict_t2id = {}\n",
    "            if dir_time2id != \"\":\n",
    "                dict_t2id = read_json(dir_time2id)\n",
    "            else:\n",
    "                print(\"Attention: icews18 needs its ts2id file to convert time into time_id\")\n",
    "            fulltest = read_txt_as_list(dir_full_test)  # only load essentially\n",
    "            li_queries = [test.split(\"\\n\")[-1] for test in tests]\n",
    "            # build sets\n",
    "            for i in range(0, len(li_queries) - 1):\n",
    "                query = li_queries[i]\n",
    "                if query == \"\":\n",
    "                    break\n",
    "                if dict_qu_ans.get(query) is None:\n",
    "                    dict_qu_ans[query] = set()\n",
    "            end_time = li_queries[-3].split(\":\")[0]\n",
    "            for line in fulltest:\n",
    "                quadruple = line.strip().split(\"\\t\")\n",
    "                time_quadruple = dict_t2id[quadruple[3]] if dir_time2id != \"\" else quadruple[3]\n",
    "                if int(time_quadruple) > int(end_time):\n",
    "                    break\n",
    "                built_query = f\"{time_quadruple}: [{quadruple[0]}, {quadruple[1]},\"\n",
    "                if dict_qu_ans.get(built_query) is not None:\n",
    "                    dict_qu_ans[built_query].add(quadruple[2])  # add answers to the set\n",
    "            print(\"duplicate answers checked\")\n",
    "        return dict_qu_ans\n",
    "\n",
    "    def generate_extra_answers(self, m_inloop, k_inloop):\n",
    "        if self.args.ft == 1:\n",
    "            raw_answers, answer_regs = self.model_calling(m_inloop)  # call for more generated ans\n",
    "        elif self.llama == 1:  # icl llama2\n",
    "            answer_regs = self.text_completion(\n",
    "                m_inloop,\n",
    "                str(self.args.PROMPT),\n",
    "                max_gen_len=self.args.max_gen_len,\n",
    "                temperature=self.args.TEMPERATURE,\n",
    "                # top_p=top_p,\n",
    "            )\n",
    "            answer_regs = [answer_reg[\"generation\"] for answer_reg in answer_regs]\n",
    "            raw_answers = answer_regs\n",
    "        \n",
    "        return raw_answers, answer_regs\n",
    "\n",
    "    def build(\n",
    "        self,\n",
    "        ckpt_dir: str,\n",
    "        tokenizer_path: str,\n",
    "        max_seq_len: int,\n",
    "        max_batch_size: int,\n",
    "        model_parallel_size: Optional[int] = None,\n",
    "    ):\n",
    "        os.environ[\"RANK\"] = \"0\"  # Set for torch.distributed.init_process_group\n",
    "\n",
    "        # os.environ[\"WORLD_SIZE\"] = \"4\" #\n",
    "        if not torch.distributed.is_initialized():\n",
    "            torch.distributed.init_process_group(\"nccl\")\n",
    "        if not model_parallel_is_initialized():\n",
    "            if model_parallel_size is None:\n",
    "                model_parallel_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n",
    "            initialize_model_parallel(model_parallel_size)\n",
    "\n",
    "        local_rank = int(os.environ.get(\"LOCAL_RANK\", 0))  # TODO change\n",
    "        torch.cuda.set_device(local_rank)\n",
    "\n",
    "        # seed must be the same in all processes\n",
    "        torch.manual_seed(1)\n",
    "\n",
    "        if local_rank > 0:\n",
    "            sys.stdout = open(os.devnull, \"w\")\n",
    "\n",
    "        start_time = time.time()\n",
    "        checkpoints = sorted(Path(ckpt_dir).glob(\"*.pth\"))\n",
    "        assert len(checkpoints) > 0, f\"no checkpoint files found in {ckpt_dir}\"\n",
    "        assert model_parallel_size == len(\n",
    "            checkpoints\n",
    "        ), f\"Loading a checkpoint for MP={len(checkpoints)} but world size is {model_parallel_size}\"\n",
    "        ckpt_path = checkpoints[get_model_parallel_rank()]\n",
    "        checkpoint = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "        with open(Path(ckpt_dir) / \"params.json\", \"r\") as f:\n",
    "            params = json.loads(f.read())\n",
    "\n",
    "        model_args: ModelArgs = ModelArgs(\n",
    "            max_seq_len=max_seq_len,\n",
    "            max_batch_size=max_batch_size,\n",
    "            **params,\n",
    "        )\n",
    "        tokenizer = Tokenizer(model_path=tokenizer_path)\n",
    "        model_args.vocab_size = tokenizer.n_words\n",
    "        torch.set_default_tensor_type(torch.cuda.HalfTensor)\n",
    "        model = Transformer(model_args)\n",
    "        model.load_state_dict(checkpoint, strict=False)\n",
    "        print(f\"Loaded in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "        return model, tokenizer  # Llama(model, tokenizer)\n",
    "\n",
    "    def bs_generate(\n",
    "        self,\n",
    "        m,\n",
    "        prompt_tokens: List[List[int]],\n",
    "        max_gen_len: int,\n",
    "        temperature: float = 0.6,\n",
    "        top_p: float = 0.9,\n",
    "        logprobs: bool = False,\n",
    "        echo: bool = False,\n",
    "    ) -> Tuple[List[List[int]], Optional[List[List[float]]]]:\n",
    "        torch.no_grad()  # replace @torch.inference_mode()\n",
    "\n",
    "        params = self.model.params\n",
    "        bsz = len(prompt_tokens)\n",
    "        assert bsz <= params.max_batch_size, (bsz, params.max_batch_size)\n",
    "\n",
    "        min_prompt_len = min(len(t) for t in prompt_tokens)\n",
    "        max_prompt_len = max(len(t) for t in prompt_tokens)\n",
    "        assert max_prompt_len <= params.max_seq_len\n",
    "        total_len = min(params.max_seq_len, max_gen_len + max_prompt_len)\n",
    "\n",
    "        pad_id = self.tokenizer.pad_id\n",
    "        tokens = torch.full((bsz, total_len), pad_id, dtype=torch.long, device=\"cuda\")\n",
    "        for k, t in enumerate(prompt_tokens):\n",
    "            tokens[k, : len(t)] = torch.tensor(t, dtype=torch.long, device=\"cuda\")\n",
    "        if logprobs:\n",
    "            token_logprobs = torch.zeros_like(tokens, dtype=torch.float)\n",
    "\n",
    "        prev_pos = 0\n",
    "        eos_reached = torch.tensor([False] * bsz, device=\"cuda\")\n",
    "        input_text_mask = tokens != pad_id\n",
    "        if min_prompt_len == total_len:\n",
    "            logits = self.model.forward(tokens, prev_pos)\n",
    "            token_logprobs = -F.cross_entropy(\n",
    "                input=logits.transpose(1, 2),\n",
    "                target=tokens,\n",
    "                reduction=\"none\",\n",
    "                ignore_index=pad_id,\n",
    "            )\n",
    "        if self.top <= 10 and m < 10:\n",
    "            next_token, eos_reached = self.restrict_list_hard(\n",
    "                tokens, prev_pos, min_prompt_len, input_text_mask, eos_reached, m\n",
    "            )\n",
    "\n",
    "        prev_pos = min_prompt_len\n",
    "        torch.set_printoptions(profile=\"full\")\n",
    "        tokens = torch.where(tokens == -1, torch.tensor(0), tokens)\n",
    "\n",
    "        for cur_pos in range(min_prompt_len + 1, total_len):\n",
    "            logits = self.model.forward(tokens[:, prev_pos:cur_pos], prev_pos)\n",
    "\n",
    "            if logprobs:\n",
    "                token_logprobs[:, prev_pos + 1 : cur_pos + 1] = -F.cross_entropy(\n",
    "                    input=logits.transpose(1, 2),\n",
    "                    target=tokens[:, prev_pos + 1 : cur_pos + 1],\n",
    "                    reduction=\"none\",\n",
    "                    ignore_index=pad_id,\n",
    "                )\n",
    "\n",
    "            top_sign = self.top - 1 if self.first_check == 0 else 0  # first check, or to generate the rest\n",
    "            next_token = torch.argsort(logits[:, -1], dim=-1, descending=True)[:, top_sign]\n",
    "\n",
    "            this_peer_finished, next_token = self.first_checking(next_token, logits[:, -1])\n",
    "\n",
    "            if next_token in self.zone_zero:\n",
    "                this_peer_finished = True\n",
    "            ## modification ends\n",
    "            next_token = next_token.reshape(-1)\n",
    "\n",
    "            # only replace token if prompt has already been generated\n",
    "            next_token = torch.where(input_text_mask[:, cur_pos], tokens[:, cur_pos], next_token)\n",
    "            tokens[:, cur_pos] = next_token\n",
    "            eos_reached |= (~input_text_mask[:, cur_pos]) & (next_token == self.tokenizer.eos_id)\n",
    "            prev_pos = cur_pos\n",
    "            if all(eos_reached) or this_peer_finished:  # added this_peer_finished\n",
    "                break\n",
    "\n",
    "        if logprobs:\n",
    "            token_logprobs = token_logprobs.tolist()\n",
    "        out_tokens, out_logprobs = [], []\n",
    "        for i, toks in enumerate(tokens.tolist()):\n",
    "            # cut to max gen len\n",
    "            start = 0 if echo else len(prompt_tokens[i])\n",
    "            toks = toks[start : len(prompt_tokens[i]) + max_gen_len]\n",
    "            probs = None\n",
    "            if logprobs:\n",
    "                probs = token_logprobs[i][start : len(prompt_tokens[i]) + max_gen_len]\n",
    "            # cut to eos tok if any\n",
    "            if self.tokenizer.eos_id in toks:\n",
    "                eos_idx = toks.index(self.tokenizer.eos_id)\n",
    "                toks = toks[:eos_idx]\n",
    "                probs = probs[:eos_idx] if logprobs else None\n",
    "            out_tokens.append(toks)\n",
    "            out_logprobs.append(probs)\n",
    "        return (out_tokens, out_logprobs if logprobs else None)\n",
    "\n",
    "    def text_completion(\n",
    "        self,\n",
    "        m,\n",
    "        prompts: List[str],\n",
    "        temperature: float = 0,\n",
    "        top_p: float = 0.1,\n",
    "        max_gen_len: Optional[int] = None,\n",
    "        logprobs: bool = False,\n",
    "        echo: bool = False,\n",
    "    ):  # -> List[CompletionPrediction]:\n",
    "        if max_gen_len is None:\n",
    "            max_gen_len = self.model.params.max_seq_len - 1\n",
    "        prompt_tokens = [self.tokenizer.encode(prompts, bos=True, eos=False)]\n",
    "        \"\"\"\n",
    "        for x in prompts:\n",
    "            print(x)\n",
    "            prompt_tokens.append(self.tokenizer.encode(x, bos=False, eos=False))\"\"\"\n",
    "        generation_tokens, generation_logprobs = self.bs_generate(\n",
    "            m,\n",
    "            prompt_tokens,\n",
    "            max_gen_len,\n",
    "            temperature,\n",
    "            top_p,\n",
    "            logprobs,\n",
    "            echo,\n",
    "        )\n",
    "        if logprobs:\n",
    "            return [\n",
    "                {\n",
    "                    \"generation\": self.tokenizer.decode(t),\n",
    "                    \"tokens\": [self.tokenizer.decode(x) for x in t],\n",
    "                    \"logprobs\": logprobs_i,\n",
    "                }\n",
    "                for t, logprobs_i in zip(generation_tokens, generation_logprobs)\n",
    "            ]\n",
    "        return [{\"generation\": self.tokenizer.decode(t)} for t in generation_tokens]\n",
    "\n",
    "    def my_generate_top10(self, model_instance, m, gen_length, **kwargs):\n",
    "        base_model = model_instance.base_model\n",
    "\n",
    "        # original prepare_inputs_for_generation and generation_config\n",
    "        original_prepare_inputs_for_generation = base_model.prepare_inputs_for_generation\n",
    "        original_generation_config = getattr(base_model, \"generation_config\", None)\n",
    "\n",
    "        # prepare_inputs_for_generation and generation_config\n",
    "        base_model.prepare_inputs_for_generation = model_instance.prepare_inputs_for_generation\n",
    "        if hasattr(base_model, \"model\"):\n",
    "            base_model.model.generation_config = model_instance.generation_config\n",
    "        else:\n",
    "            base_model.generation_config = model_instance.generation_config\n",
    "\n",
    "        try:\n",
    "            # base_model generate_top10\n",
    "            outputs = self.my_utils_generate_top10(base_model, m, gen_length, **kwargs)\n",
    "        except Exception as e:\n",
    "            # prepare_inputs_for_generation\n",
    "            base_model.prepare_inputs_for_generation = original_prepare_inputs_for_generation\n",
    "            # recover generation_config\n",
    "            if original_generation_config is not None:\n",
    "                base_model.generation_config = original_generation_config\n",
    "            raise e\n",
    "        else:\n",
    "            # recover prepare_inputs_for_generation\n",
    "            base_model.prepare_inputs_for_generation = original_prepare_inputs_for_generation\n",
    "            # recover generation_config\n",
    "            if original_generation_config is not None:\n",
    "                base_model.generation_config = original_generation_config\n",
    "            return outputs\n",
    "\n",
    "    # adopted from \"generate\" in /transformers/generation/utils.py\n",
    "    @torch.no_grad()\n",
    "    def my_utils_generate_top10(\n",
    "        self,\n",
    "        model_instance,\n",
    "        m,\n",
    "        gen_length,\n",
    "        inputs: Optional[torch.Tensor] = None,\n",
    "        generation_config: Optional[GenerationConfig] = None,\n",
    "        logits_processor: Optional[LogitsProcessorList] = None,\n",
    "        stopping_criteria: Optional[StoppingCriteriaList] = None,\n",
    "        # max_length=max_length,\n",
    "        prefix_allowed_tokens_fn: Optional[Callable[[int, torch.Tensor], List[int]]] = None,\n",
    "        synced_gpus: Optional[bool] = None,\n",
    "        assistant_model: Optional[\"PreTrainedModel\"] = None,\n",
    "        streamer: Optional[\"BaseStreamer\"] = None,\n",
    "        **kwargs,\n",
    "    ):  # -> Union[GenerateOutput, torch.LongTensor]:\n",
    "\n",
    "        if synced_gpus is None:\n",
    "            if is_deepspeed_zero3_enabled() and dist.get_world_size() > 1:\n",
    "                synced_gpus = True\n",
    "            else:\n",
    "                synced_gpus = False\n",
    "\n",
    "        # 1. Handle `generation_config` and kwargs that might update it, and validate the `.generate()` call\n",
    "        model_instance._validate_model_class()\n",
    "\n",
    "        # priority: `generation_config` argument > `model.generation_config` (the default generation config)\n",
    "        if generation_config is None:\n",
    "            # legacy: users may modify the model configuration to control generation -- update the generation config\n",
    "            # model attribute accordingly, if it was created from the model config\n",
    "            if model_instance.generation_config._from_model_config:\n",
    "                new_generation_config = GenerationConfig.from_model_config(model_instance.config)\n",
    "                if new_generation_config != model_instance.generation_config:\n",
    "                    warnings.warn(\n",
    "                        \"You have modified the pretrained model configuration to control generation. This is a\"\n",
    "                        \" deprecated strategy to control generation and will be removed soon, in a future version.\"\n",
    "                        \" Please use a generation configuration file (see\"\n",
    "                        \" https://huggingface.co/docs/transformers/main_classes/text_generation)\"\n",
    "                    )\n",
    "                    model_instance.generation_config = new_generation_config\n",
    "            generation_config = model_instance.generation_config\n",
    "\n",
    "        generation_config = copy.deepcopy(generation_config)\n",
    "        model_kwargs = generation_config.update(**kwargs)  # All unused kwargs must be model kwargs\n",
    "        generation_config.validate()\n",
    "        model_instance._validate_model_kwargs(model_kwargs.copy())\n",
    "\n",
    "        # 2. Set generation parameters if not already defined\n",
    "        logits_processor = logits_processor if logits_processor is not None else LogitsProcessorList()\n",
    "        stopping_criteria = stopping_criteria if stopping_criteria is not None else StoppingCriteriaList()\n",
    "\n",
    "        if generation_config.pad_token_id is None and generation_config.eos_token_id is not None:\n",
    "            if model_kwargs.get(\"attention_mask\", None) is None:\n",
    "                logger.warning(\n",
    "                    \"The attention mask and the pad token id were not set. As a consequence, you may observe \"\n",
    "                    \"unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\"\n",
    "                )\n",
    "            eos_token_id = generation_config.eos_token_id\n",
    "            if isinstance(eos_token_id, list):\n",
    "                eos_token_id = eos_token_id[0]\n",
    "            logger.warning(f\"Setting `pad_token_id` to `eos_token_id`:{eos_token_id} for open-end generation.\")\n",
    "            generation_config.pad_token_id = eos_token_id\n",
    "\n",
    "        # 3. Define model inputs\n",
    "        # inputs_tensor has to be defined\n",
    "        # model_input_name is defined if model-specific keyword input is passed\n",
    "        # otherwise model_input_name is None\n",
    "        # all model-specific keyword inputs are removed from `model_kwargs`\n",
    "        inputs_tensor, model_input_name, model_kwargs = model_instance._prepare_model_inputs(\n",
    "            inputs, generation_config.bos_token_id, model_kwargs\n",
    "        )\n",
    "        batch_size = inputs_tensor.shape[0]\n",
    "\n",
    "        # 4. Define other model kwargs\n",
    "        model_kwargs[\"output_attentions\"] = generation_config.output_attentions\n",
    "        model_kwargs[\"output_hidden_states\"] = generation_config.output_hidden_states\n",
    "        model_kwargs[\"use_cache\"] = generation_config.use_cache\n",
    "\n",
    "        accepts_attention_mask = \"attention_mask\" in set(inspect.signature(model_instance.forward).parameters.keys())\n",
    "        requires_attention_mask = \"encoder_outputs\" not in model_kwargs\n",
    "\n",
    "        if model_kwargs.get(\"attention_mask\", None) is None and requires_attention_mask and accepts_attention_mask:\n",
    "            model_kwargs[\"attention_mask\"] = model_instance._prepare_attention_mask_for_generation(\n",
    "                inputs_tensor, generation_config.pad_token_id, generation_config.eos_token_id\n",
    "            )\n",
    "\n",
    "        # decoder-only models should use left-padding for generation\n",
    "        if not model_instance.config.is_encoder_decoder:\n",
    "            # If `input_ids` was given, check if the last id in any sequence is `pad_token_id`\n",
    "            # Note: If using, `inputs_embeds` this check does not work, because we want to be more hands-off.\n",
    "            if (\n",
    "                generation_config.pad_token_id is not None\n",
    "                and len(inputs_tensor.shape) == 2\n",
    "                and torch.sum(inputs_tensor[:, -1] == generation_config.pad_token_id) > 0\n",
    "            ):\n",
    "                logger.warning(\n",
    "                    \"A decoder-only architecture is being used, but right-padding was detected! For correct \"\n",
    "                    \"generation results, please set `padding_side='left'` when initializing the tokenizer.\"\n",
    "                )\n",
    "\n",
    "        if model_instance.config.is_encoder_decoder and \"encoder_outputs\" not in model_kwargs:\n",
    "            # if model is encoder decoder encoder_outputs are created\n",
    "            # and added to `model_kwargs`\n",
    "            model_kwargs = model_instance._prepare_encoder_decoder_kwargs_for_generation(\n",
    "                inputs_tensor, model_kwargs, model_input_name\n",
    "            )\n",
    "\n",
    "        # 5. Prepare `input_ids` which will be used for auto-regressive generation\n",
    "        if model_instance.config.is_encoder_decoder:\n",
    "            input_ids, model_kwargs = model_instance._prepare_decoder_input_ids_for_generation(\n",
    "                batch_size=batch_size,\n",
    "                model_input_name=model_input_name,\n",
    "                model_kwargs=model_kwargs,\n",
    "                decoder_start_token_id=generation_config.decoder_start_token_id,\n",
    "                bos_token_id=generation_config.bos_token_id,\n",
    "                device=inputs_tensor.device,\n",
    "            )\n",
    "        else:\n",
    "            input_ids = inputs_tensor if model_input_name == \"input_ids\" else model_kwargs.pop(\"input_ids\")\n",
    "\n",
    "        if streamer is not None:\n",
    "            streamer.put(input_ids.cpu())\n",
    "\n",
    "        # 6. Prepare `max_length` depending on other stopping criteria.\n",
    "        input_ids_seq_length = input_ids.shape[-1]\n",
    "        has_default_max_length = kwargs.get(\"max_length\") is None and generation_config.max_length is not None\n",
    "        if has_default_max_length and generation_config.max_new_tokens is None:\n",
    "            warnings.warn(\n",
    "                f\"Using `max_length`'s default ({generation_config.max_length}) to control the generation length. \"\n",
    "                \"This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we\"\n",
    "                \" recommend using `max_new_tokens` to control the maximum length of the generation.\",\n",
    "                UserWarning,\n",
    "            )\n",
    "        elif generation_config.max_new_tokens is not None:\n",
    "            if not has_default_max_length:\n",
    "                logger.warning(\n",
    "                    f\"Both `max_new_tokens` (={generation_config.max_new_tokens}) and `max_length`(=\"\n",
    "                    f\"{generation_config.max_length}) seem to have been set. `max_new_tokens` will take precedence. \"\n",
    "                    \"Please refer to the documentation for more information. \"\n",
    "                    \"(https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\"\n",
    "                )\n",
    "            generation_config.max_length = generation_config.max_new_tokens + input_ids_seq_length\n",
    "\n",
    "        if generation_config.min_length is not None and generation_config.min_length > generation_config.max_length:\n",
    "            raise ValueError(\n",
    "                f\"Unfeasible length constraints: the minimum length ({generation_config.min_length}) is larger than\"\n",
    "                f\" the maximum length ({generation_config.max_length})\"\n",
    "            )\n",
    "        if input_ids_seq_length >= generation_config.max_length:\n",
    "            input_ids_string = \"decoder_input_ids\" if model_instance.config.is_encoder_decoder else \"input_ids\"\n",
    "            logger.warning(\n",
    "                f\"Input length of {input_ids_string} is {input_ids_seq_length}, but `max_length` is set to\"\n",
    "                f\" {generation_config.max_length}. This can lead to unexpected behavior. You should consider\"\n",
    "                \" increasing `max_new_tokens`.\"\n",
    "            )\n",
    "\n",
    "        # 7. determine generation mode\n",
    "        is_constraint_gen_mode = (\n",
    "            generation_config.constraints is not None or generation_config.force_words_ids is not None\n",
    "        )\n",
    "\n",
    "        is_contrastive_search_gen_mode = (\n",
    "            (generation_config.num_beams == 1)\n",
    "            and generation_config.top_k is not None\n",
    "            and generation_config.top_k > 1\n",
    "            and generation_config.do_sample is False\n",
    "            and generation_config.penalty_alpha is not None\n",
    "            and generation_config.penalty_alpha > 0\n",
    "        )\n",
    "\n",
    "        is_greedy_gen_mode = (\n",
    "            (generation_config.num_beams == 1)\n",
    "            and (generation_config.num_beam_groups == 1)\n",
    "            and generation_config.do_sample is False\n",
    "            and not is_constraint_gen_mode\n",
    "            and not is_contrastive_search_gen_mode\n",
    "        )\n",
    "        is_sample_gen_mode = (\n",
    "            (generation_config.num_beams == 1)\n",
    "            and (generation_config.num_beam_groups == 1)\n",
    "            and generation_config.do_sample is True\n",
    "            and not is_constraint_gen_mode\n",
    "            and not is_contrastive_search_gen_mode\n",
    "        )\n",
    "        is_beam_gen_mode = (\n",
    "            (generation_config.num_beams > 1)\n",
    "            and (generation_config.num_beam_groups == 1)\n",
    "            and generation_config.do_sample is False\n",
    "            and not is_constraint_gen_mode\n",
    "            and not is_contrastive_search_gen_mode\n",
    "        )\n",
    "        is_beam_sample_gen_mode = (\n",
    "            (generation_config.num_beams > 1)\n",
    "            and (generation_config.num_beam_groups == 1)\n",
    "            and generation_config.do_sample is True\n",
    "            and not is_constraint_gen_mode\n",
    "            and not is_contrastive_search_gen_mode\n",
    "        )\n",
    "        is_group_beam_gen_mode = (\n",
    "            (generation_config.num_beams > 1)\n",
    "            and (generation_config.num_beam_groups > 1)\n",
    "            and not is_constraint_gen_mode\n",
    "            and not is_contrastive_search_gen_mode\n",
    "        )\n",
    "        is_assisted_gen_mode = False\n",
    "        if assistant_model is not None:\n",
    "            if not (is_greedy_gen_mode or is_sample_gen_mode):\n",
    "                raise ValueError(\n",
    "                    \"You've set `assistant_model`, which triggers assisted generate. Currently, assisted generate \"\n",
    "                    \"is only supported with Greedy Search and Sample.\"\n",
    "                )\n",
    "            is_assisted_gen_mode = True\n",
    "\n",
    "        if generation_config.num_beam_groups > generation_config.num_beams:\n",
    "            raise ValueError(\"`num_beam_groups` has to be smaller or equal to `num_beams`\")\n",
    "        if is_group_beam_gen_mode and generation_config.do_sample is True:\n",
    "            raise ValueError(\n",
    "                \"Diverse beam search cannot be used in sampling mode. Make sure that `do_sample` is set to `False`.\"\n",
    "            )\n",
    "\n",
    "        if streamer is not None and (generation_config.num_beams > 1):\n",
    "            raise ValueError(\n",
    "                \"`streamer` cannot be used with beam search (yet!). Make sure that `num_beams` is set to 1.\"\n",
    "            )\n",
    "\n",
    "        if model_instance.device.type != input_ids.device.type:\n",
    "            warnings.warn(\n",
    "                \"You are calling .generate() with the `input_ids` being on a device type different\"\n",
    "                f\" than your model's device. `input_ids` is on {input_ids.device.type}, whereas the model\"\n",
    "                f\" is on {model_instance.device.type}. You may experience unexpected behaviors or slower generation.\"\n",
    "                \" Please make sure that you have put `input_ids` to the\"\n",
    "                f\" correct device by calling for example input_ids = input_ids.to('{model_instance.device.type}') before\"\n",
    "                \" running `.generate()`.\",\n",
    "                UserWarning,\n",
    "            )\n",
    "\n",
    "        # 8. prepare distribution pre_processing samplers\n",
    "        logits_processor = model_instance._get_logits_processor(\n",
    "            generation_config=generation_config,\n",
    "            input_ids_seq_length=input_ids_seq_length,\n",
    "            encoder_input_ids=inputs_tensor,\n",
    "            prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,\n",
    "            logits_processor=logits_processor,\n",
    "        )\n",
    "\n",
    "        # 9. prepare stopping criteria\n",
    "        stopping_criteria = model_instance._get_stopping_criteria(\n",
    "            generation_config=generation_config, stopping_criteria=stopping_criteria\n",
    "        )\n",
    "        # 10. go into different generation modes\n",
    "        if is_assisted_gen_mode:\n",
    "            if generation_config.num_return_sequences > 1:\n",
    "                raise ValueError(\n",
    "                    \"num_return_sequences has to be 1 when doing assisted generate, \"\n",
    "                    f\"but is {generation_config.num_return_sequences}.\"\n",
    "                )\n",
    "            if batch_size > 1:\n",
    "                raise ValueError(\"assisted generate is only supported for batch_size = 1\")\n",
    "            if not model_kwargs[\"use_cache\"]:\n",
    "                raise ValueError(\"assisted generate requires `use_cache=True`\")\n",
    "\n",
    "            # 11. If the assistant model is an encoder-decoder, prepare its encoder outputs\n",
    "            if assistant_model.config.is_encoder_decoder:\n",
    "                assistant_model_kwargs = copy.deepcopy(model_kwargs)\n",
    "                inputs_tensor, model_input_name, assistant_model_kwargs = assistant_model._prepare_model_inputs(\n",
    "                    inputs_tensor, assistant_model.generation_config.bos_token_id, assistant_model_kwargs\n",
    "                )\n",
    "                assistant_model_kwargs = assistant_model._prepare_encoder_decoder_kwargs_for_generation(\n",
    "                    inputs_tensor, assistant_model_kwargs, model_input_name\n",
    "                )\n",
    "                model_kwargs[\"assistant_encoder_outputs\"] = assistant_model_kwargs[\"encoder_outputs\"]\n",
    "\n",
    "            # 12. run assisted generate\n",
    "            return model_instance.assisted_decoding(\n",
    "                input_ids,\n",
    "                assistant_model=assistant_model,\n",
    "                do_sample=generation_config.do_sample,\n",
    "                logits_processor=logits_processor,\n",
    "                logits_warper=(\n",
    "                    model_instance._get_logits_warper(generation_config) if generation_config.do_sample else None\n",
    "                ),\n",
    "                stopping_criteria=stopping_criteria,\n",
    "                pad_token_id=generation_config.pad_token_id,\n",
    "                eos_token_id=generation_config.eos_token_id,\n",
    "                output_scores=generation_config.output_scores,\n",
    "                return_dict_in_generate=generation_config.return_dict_in_generate,\n",
    "                synced_gpus=synced_gpus,\n",
    "                streamer=streamer,\n",
    "                **model_kwargs,\n",
    "            )\n",
    "        if is_greedy_gen_mode:\n",
    "            if generation_config.num_return_sequences > 1:\n",
    "                raise ValueError(\n",
    "                    \"num_return_sequences has to be 1 when doing greedy search, \"\n",
    "                    f\"but is {generation_config.num_return_sequences}.\"\n",
    "                )\n",
    "            # 11. run greedy search\n",
    "            return self.my_utils_greedy_search_top10(\n",
    "                model_instance,  # my_utils_greedy_search_top10\n",
    "                m,  # check m\n",
    "                gen_length,\n",
    "                input_ids,\n",
    "                logits_processor=logits_processor,\n",
    "                stopping_criteria=stopping_criteria,\n",
    "                # max_length=20,\n",
    "                pad_token_id=generation_config.pad_token_id,\n",
    "                eos_token_id=generation_config.eos_token_id,\n",
    "                output_scores=generation_config.output_scores,\n",
    "                return_dict_in_generate=generation_config.return_dict_in_generate,\n",
    "                synced_gpus=synced_gpus,\n",
    "                streamer=streamer,\n",
    "                **model_kwargs,\n",
    "            )\n",
    "\n",
    "        elif is_contrastive_search_gen_mode:\n",
    "            if generation_config.num_return_sequences > 1:\n",
    "                raise ValueError(\n",
    "                    \"num_return_sequences has to be 1 when doing contrastive search, \"\n",
    "                    f\"but is {generation_config.num_return_sequences}.\"\n",
    "                )\n",
    "            if not model_kwargs[\"use_cache\"]:\n",
    "                raise ValueError(\"Contrastive search requires `use_cache=True`\")\n",
    "\n",
    "            return model_instance.contrastive_search(\n",
    "                input_ids,\n",
    "                top_k=generation_config.top_k,\n",
    "                penalty_alpha=generation_config.penalty_alpha,\n",
    "                logits_processor=logits_processor,\n",
    "                stopping_criteria=stopping_criteria,\n",
    "                pad_token_id=generation_config.pad_token_id,\n",
    "                eos_token_id=generation_config.eos_token_id,\n",
    "                output_scores=generation_config.output_scores,\n",
    "                return_dict_in_generate=generation_config.return_dict_in_generate,\n",
    "                synced_gpus=synced_gpus,\n",
    "                streamer=streamer,\n",
    "                **model_kwargs,\n",
    "            )\n",
    "\n",
    "        elif is_sample_gen_mode:\n",
    "            # 11. prepare logits warper\n",
    "            logits_warper = model_instance._get_logits_warper(generation_config)\n",
    "\n",
    "            # 12. expand input_ids with `num_return_sequences` additional sequences per batch\n",
    "            input_ids, model_kwargs = model_instance._expand_inputs_for_generation(\n",
    "                input_ids=input_ids,\n",
    "                expand_size=generation_config.num_return_sequences,\n",
    "                is_encoder_decoder=model_instance.config.is_encoder_decoder,\n",
    "                **model_kwargs,\n",
    "            )\n",
    "\n",
    "            # 13. run sample\n",
    "            return model_instance.sample(\n",
    "                input_ids,\n",
    "                logits_processor=logits_processor,\n",
    "                logits_warper=logits_warper,\n",
    "                stopping_criteria=stopping_criteria,\n",
    "                pad_token_id=generation_config.pad_token_id,\n",
    "                eos_token_id=generation_config.eos_token_id,\n",
    "                output_scores=generation_config.output_scores,\n",
    "                return_dict_in_generate=generation_config.return_dict_in_generate,\n",
    "                synced_gpus=synced_gpus,\n",
    "                streamer=streamer,\n",
    "                **model_kwargs,\n",
    "            )\n",
    "\n",
    "        elif is_beam_gen_mode:\n",
    "            if generation_config.num_return_sequences > generation_config.num_beams:\n",
    "                raise ValueError(\"`num_return_sequences` has to be smaller or equal to `num_beams`.\")\n",
    "\n",
    "            if stopping_criteria.max_length is None:\n",
    "                raise ValueError(\"`max_length` needs to be a stopping_criteria for now.\")\n",
    "\n",
    "            # 11. prepare beam search scorer\n",
    "            beam_scorer = BeamSearchScorer(\n",
    "                batch_size=batch_size,\n",
    "                num_beams=generation_config.num_beams,\n",
    "                device=inputs_tensor.device,\n",
    "                length_penalty=generation_config.length_penalty,\n",
    "                do_early_stopping=generation_config.early_stopping,\n",
    "                num_beam_hyps_to_keep=generation_config.num_return_sequences,\n",
    "                max_length=generation_config.max_length,\n",
    "            )\n",
    "            # 12. interleave input_ids with `num_beams` additional sequences per batch\n",
    "            input_ids, model_kwargs = model_instance._expand_inputs_for_generation(\n",
    "                input_ids=input_ids,\n",
    "                expand_size=generation_config.num_beams,\n",
    "                is_encoder_decoder=model_instance.config.is_encoder_decoder,\n",
    "                **model_kwargs,\n",
    "            )\n",
    "            # 13. run beam search\n",
    "            return model_instance.beam_search(\n",
    "                input_ids,\n",
    "                beam_scorer,\n",
    "                logits_processor=logits_processor,\n",
    "                stopping_criteria=stopping_criteria,\n",
    "                pad_token_id=generation_config.pad_token_id,\n",
    "                eos_token_id=generation_config.eos_token_id,\n",
    "                output_scores=generation_config.output_scores,\n",
    "                return_dict_in_generate=generation_config.return_dict_in_generate,\n",
    "                synced_gpus=synced_gpus,\n",
    "                **model_kwargs,\n",
    "            )\n",
    "\n",
    "        elif is_beam_sample_gen_mode:\n",
    "            # 11. prepare logits warper\n",
    "            logits_warper = model_instance._get_logits_warper(generation_config)\n",
    "\n",
    "            if stopping_criteria.max_length is None:\n",
    "                raise ValueError(\"`max_length` needs to be a stopping_criteria for now.\")\n",
    "            # 12. prepare beam search scorer\n",
    "            beam_scorer = BeamSearchScorer(\n",
    "                batch_size=batch_size * generation_config.num_return_sequences,\n",
    "                num_beams=generation_config.num_beams,\n",
    "                device=inputs_tensor.device,\n",
    "                length_penalty=generation_config.length_penalty,\n",
    "                do_early_stopping=generation_config.early_stopping,\n",
    "                max_length=generation_config.max_length,\n",
    "            )\n",
    "\n",
    "            # 13. interleave input_ids with `num_beams` additional sequences per batch\n",
    "            input_ids, model_kwargs = model_instance._expand_inputs_for_generation(\n",
    "                input_ids=input_ids,\n",
    "                expand_size=generation_config.num_beams * generation_config.num_return_sequences,\n",
    "                is_encoder_decoder=model_instance.config.is_encoder_decoder,\n",
    "                **model_kwargs,\n",
    "            )\n",
    "\n",
    "            # 14. run beam sample\n",
    "            return model_instance.beam_sample(\n",
    "                input_ids,\n",
    "                beam_scorer,\n",
    "                logits_processor=logits_processor,\n",
    "                logits_warper=logits_warper,\n",
    "                stopping_criteria=stopping_criteria,\n",
    "                pad_token_id=generation_config.pad_token_id,\n",
    "                eos_token_id=generation_config.eos_token_id,\n",
    "                output_scores=generation_config.output_scores,\n",
    "                return_dict_in_generate=generation_config.return_dict_in_generate,\n",
    "                synced_gpus=synced_gpus,\n",
    "                **model_kwargs,\n",
    "            )\n",
    "\n",
    "        elif is_group_beam_gen_mode:\n",
    "            if generation_config.num_return_sequences > generation_config.num_beams:\n",
    "                raise ValueError(\"`num_return_sequences` has to be smaller or equal to `num_beams`.\")\n",
    "\n",
    "            if generation_config.num_beams % generation_config.num_beam_groups != 0:\n",
    "                raise ValueError(\"`num_beams` should be divisible by `num_beam_groups` for group beam search.\")\n",
    "\n",
    "            if stopping_criteria.max_length is None:\n",
    "                raise ValueError(\"`max_length` needs to be a stopping_criteria for now.\")\n",
    "\n",
    "            has_default_typical_p = kwargs.get(\"typical_p\") is None and generation_config.typical_p == 1.0\n",
    "            if not has_default_typical_p:\n",
    "                raise ValueError(\"Decoder argument `typical_p` is not supported with beam groups.\")\n",
    "\n",
    "            # 11. prepare beam search scorer\n",
    "            beam_scorer = BeamSearchScorer(\n",
    "                batch_size=batch_size,\n",
    "                num_beams=generation_config.num_beams,\n",
    "                device=inputs_tensor.device,\n",
    "                length_penalty=generation_config.length_penalty,\n",
    "                do_early_stopping=generation_config.early_stopping,\n",
    "                num_beam_hyps_to_keep=generation_config.num_return_sequences,\n",
    "                num_beam_groups=generation_config.num_beam_groups,\n",
    "                max_length=generation_config.max_length,\n",
    "            )\n",
    "            # 12. interleave input_ids with `num_beams` additional sequences per batch\n",
    "            input_ids, model_kwargs = model_instance._expand_inputs_for_generation(\n",
    "                input_ids=input_ids,\n",
    "                expand_size=generation_config.num_beams,\n",
    "                is_encoder_decoder=model_instance.config.is_encoder_decoder,\n",
    "                **model_kwargs,\n",
    "            )\n",
    "            # 13. run beam search\n",
    "            return model_instance.group_beam_search(\n",
    "                input_ids,\n",
    "                beam_scorer,\n",
    "                logits_processor=logits_processor,\n",
    "                stopping_criteria=stopping_criteria,\n",
    "                pad_token_id=generation_config.pad_token_id,\n",
    "                eos_token_id=generation_config.eos_token_id,\n",
    "                output_scores=generation_config.output_scores,\n",
    "                return_dict_in_generate=generation_config.return_dict_in_generate,\n",
    "                synced_gpus=synced_gpus,\n",
    "                **model_kwargs,\n",
    "            )\n",
    "\n",
    "        elif is_constraint_gen_mode:\n",
    "            if generation_config.num_return_sequences > generation_config.num_beams:\n",
    "                raise ValueError(\"`num_return_sequences` has to be smaller or equal to `num_beams`.\")\n",
    "\n",
    "            if stopping_criteria.max_length is None:\n",
    "                raise ValueError(\"`max_length` needs to be a stopping_criteria for now.\")\n",
    "\n",
    "            if generation_config.num_beams <= 1:\n",
    "                raise ValueError(\"`num_beams` needs to be greater than 1 for constrained generation.\")\n",
    "\n",
    "            if generation_config.do_sample:\n",
    "                raise ValueError(\"`do_sample` needs to be false for constrained generation.\")\n",
    "\n",
    "            if generation_config.num_beam_groups is not None and generation_config.num_beam_groups > 1:\n",
    "                raise ValueError(\"`num_beam_groups` not supported yet for constrained generation.\")\n",
    "\n",
    "            final_constraints = []\n",
    "            if generation_config.constraints is not None:\n",
    "                final_constraints = generation_config.constraints\n",
    "\n",
    "            if generation_config.force_words_ids is not None:\n",
    "\n",
    "                def typeerror():\n",
    "                    raise ValueError(\n",
    "                        \"`force_words_ids` has to either be a `List[List[List[int]]]` or `List[List[int]]`\"\n",
    "                        f\"of positive integers, but is {generation_config.force_words_ids}.\"\n",
    "                    )\n",
    "\n",
    "                if (\n",
    "                    not isinstance(generation_config.force_words_ids, list)\n",
    "                    or len(generation_config.force_words_ids) == 0\n",
    "                ):\n",
    "                    typeerror()\n",
    "\n",
    "                for word_ids in generation_config.force_words_ids:\n",
    "                    if isinstance(word_ids[0], list):\n",
    "                        if not isinstance(word_ids, list) or len(word_ids) == 0:\n",
    "                            typeerror()\n",
    "                        if any(not isinstance(token_ids, list) for token_ids in word_ids):\n",
    "                            typeerror()\n",
    "                        if any(\n",
    "                            any((not isinstance(token_id, int) or token_id < 0) for token_id in token_ids)\n",
    "                            for token_ids in word_ids\n",
    "                        ):\n",
    "                            typeerror()\n",
    "\n",
    "                        constraint = DisjunctiveConstraint(word_ids)\n",
    "                    else:\n",
    "                        if not isinstance(word_ids, list) or len(word_ids) == 0:\n",
    "                            typeerror()\n",
    "                        if any((not isinstance(token_id, int) or token_id < 0) for token_id in word_ids):\n",
    "                            typeerror()\n",
    "\n",
    "                        constraint = PhrasalConstraint(word_ids)\n",
    "                    final_constraints.append(constraint)\n",
    "\n",
    "            # 11. prepare beam search scorer\n",
    "            constrained_beam_scorer = ConstrainedBeamSearchScorer(\n",
    "                constraints=final_constraints,\n",
    "                batch_size=batch_size,\n",
    "                num_beams=generation_config.num_beams,\n",
    "                device=inputs_tensor.device,\n",
    "                length_penalty=generation_config.length_penalty,\n",
    "                do_early_stopping=generation_config.early_stopping,\n",
    "                num_beam_hyps_to_keep=generation_config.num_return_sequences,\n",
    "                max_length=generation_config.max_length,\n",
    "            )\n",
    "            # 12. interleave input_ids with `num_beams` additional sequences per batch\n",
    "            input_ids, model_kwargs = model_instance._expand_inputs_for_generation(\n",
    "                input_ids=input_ids,\n",
    "                expand_size=generation_config.num_beams,\n",
    "                is_encoder_decoder=model_instance.config.is_encoder_decoder,\n",
    "                **model_kwargs,\n",
    "            )\n",
    "            # 13. run beam search\n",
    "            return model_instance.constrained_beam_search(\n",
    "                input_ids,\n",
    "                constrained_beam_scorer=constrained_beam_scorer,\n",
    "                logits_processor=logits_processor,\n",
    "                stopping_criteria=stopping_criteria,\n",
    "                pad_token_id=generation_config.pad_token_id,\n",
    "                eos_token_id=generation_config.eos_token_id,\n",
    "                output_scores=generation_config.output_scores,\n",
    "                return_dict_in_generate=generation_config.return_dict_in_generate,\n",
    "                synced_gpus=synced_gpus,\n",
    "                **model_kwargs,\n",
    "            )\n",
    "\n",
    "    # adopted from \"greedy_search\" in /transformers/generation/utils.py\n",
    "    def my_utils_greedy_search_top10(\n",
    "        self,\n",
    "        model_instance,\n",
    "        m_inloop,\n",
    "        gen_length,\n",
    "        input_ids: torch.LongTensor,\n",
    "        logits_processor: Optional[LogitsProcessorList] = None,\n",
    "        stopping_criteria: Optional[StoppingCriteriaList] = None,\n",
    "        max_length: Optional[int] = None,\n",
    "        pad_token_id: Optional[int] = None,\n",
    "        eos_token_id: Optional[Union[int, List[int]]] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        output_scores: Optional[bool] = None,\n",
    "        return_dict_in_generate: Optional[bool] = None,\n",
    "        synced_gpus: bool = False,\n",
    "        streamer: Optional[\"BaseStreamer\"] = None,\n",
    "        **model_kwargs,\n",
    "    ):  # -> Union[GreedySearchOutput, torch.LongTensor]:\n",
    "        r\"\"\"\n",
    "        [\"It might be possible to get a better understanding of the nature of the problem, but it's not\"]\n",
    "        ```\"\"\"\n",
    "        # init values\n",
    "        stopping_criteria = StoppingCriteriaList([MaxLengthCriteria(max_length=gen_length + input_ids.shape[1])])\n",
    "        logits_processor = logits_processor if logits_processor is not None else LogitsProcessorList()\n",
    "        stopping_criteria = stopping_criteria if stopping_criteria is not None else StoppingCriteriaList()\n",
    "        if max_length is not None:\n",
    "            warnings.warn(\n",
    "                \"`max_length` is deprecated in this function, use\"\n",
    "                \" `stopping_criteria=StoppingCriteriaList([MaxLengthCriteria(max_length=max_length)])` instead.\",\n",
    "                UserWarning,\n",
    "            )\n",
    "            stopping_criteria = validate_stopping_criteria(stopping_criteria, max_length)\n",
    "        pad_token_id = pad_token_id if pad_token_id is not None else model_instance.generation_config.pad_token_id\n",
    "        eos_token_id = eos_token_id if eos_token_id is not None else model_instance.generation_config.eos_token_id\n",
    "        if isinstance(eos_token_id, int):\n",
    "            eos_token_id = [eos_token_id]\n",
    "        eos_token_id_tensor = torch.tensor(eos_token_id).to(input_ids.device) if eos_token_id is not None else None\n",
    "        output_scores = output_scores if output_scores is not None else model_instance.generation_config.output_scores\n",
    "        output_attentions = (\n",
    "            output_attentions if output_attentions is not None else model_instance.generation_config.output_attentions\n",
    "        )\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states\n",
    "            if output_hidden_states is not None\n",
    "            else model_instance.generation_config.output_hidden_states\n",
    "        )\n",
    "        return_dict_in_generate = (\n",
    "            return_dict_in_generate\n",
    "            if return_dict_in_generate is not None\n",
    "            else model_instance.generation_config.return_dict_in_generate\n",
    "        )\n",
    "\n",
    "        # init attention / hidden states / scores tuples\n",
    "        scores = () if (return_dict_in_generate and output_scores) else None\n",
    "        decoder_attentions = () if (return_dict_in_generate and output_attentions) else None\n",
    "        cross_attentions = () if (return_dict_in_generate and output_attentions) else None\n",
    "        decoder_hidden_states = () if (return_dict_in_generate and output_hidden_states) else None\n",
    "\n",
    "        # if model is an encoder-decoder, retrieve encoder attention weights and hidden states\n",
    "        if return_dict_in_generate and model_instance.config.is_encoder_decoder:\n",
    "            encoder_attentions = model_kwargs[\"encoder_outputs\"].get(\"attentions\") if output_attentions else None\n",
    "            encoder_hidden_states = (\n",
    "                model_kwargs[\"encoder_outputs\"].get(\"hidden_states\") if output_hidden_states else None\n",
    "            )\n",
    "\n",
    "        # keep track of which sequences are already finished\n",
    "        unfinished_sequences = torch.ones(input_ids.shape[0], dtype=torch.long, device=input_ids.device)\n",
    "\n",
    "        this_peer_finished = False  # used by synced_gpus only\n",
    "\n",
    "        # prepare model initial inputs\n",
    "        model_inputs = model_instance.prepare_inputs_for_generation(input_ids, **model_kwargs)\n",
    "\n",
    "        # forward pass to get next token\n",
    "        outputs = model_instance(\n",
    "            **model_inputs,\n",
    "            return_dict=True,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "        )\n",
    "        next_token_logits = outputs.logits[:, -1, :]\n",
    "\n",
    "        # pre-process distribution\n",
    "        next_tokens_scores = logits_processor(input_ids, next_token_logits)\n",
    "\n",
    "        # Store scores, attentions and hidden_states when required\n",
    "        if return_dict_in_generate:\n",
    "            if output_scores:\n",
    "                scores += (next_tokens_scores,)\n",
    "            if output_attentions:\n",
    "                decoder_attentions += (\n",
    "                    (outputs.decoder_attentions,) if model_instance.config.is_encoder_decoder else (outputs.attentions,)\n",
    "                )\n",
    "                if model_instance.config.is_encoder_decoder:\n",
    "                    cross_attentions += (outputs.cross_attentions,)\n",
    "\n",
    "            if output_hidden_states:\n",
    "                decoder_hidden_states += (\n",
    "                    (outputs.decoder_hidden_states,)\n",
    "                    if model_instance.config.is_encoder_decoder\n",
    "                    else (outputs.hidden_states,)\n",
    "                )\n",
    "\n",
    "        next_tokens, _ = self.require_first_to_be(next_tokens_scores)\n",
    "\n",
    "        # finished sentences should have their next token be a padding token\n",
    "        if eos_token_id is not None:\n",
    "            if pad_token_id is None:\n",
    "                raise ValueError(\"If `eos_token_id` is defined, make sure that `pad_token_id` is defined.\")\n",
    "            next_tokens = next_tokens * unfinished_sequences + pad_token_id * (1 - unfinished_sequences)\n",
    "\n",
    "        # update generated ids, model inputs, and length for next step\n",
    "        input_ids = torch.cat([input_ids, next_tokens[:, None]], dim=-1)\n",
    "        if streamer is not None:\n",
    "            streamer.put(next_tokens.cpu())\n",
    "        model_kwargs = model_instance._update_model_kwargs_for_generation(\n",
    "            outputs, model_kwargs, is_encoder_decoder=model_instance.config.is_encoder_decoder\n",
    "        )\n",
    "\n",
    "        # if eos_token was found in one sentence, set sentence to finished\n",
    "        if eos_token_id_tensor is not None:\n",
    "            unfinished_sequences = unfinished_sequences.mul(\n",
    "                next_tokens.tile(eos_token_id_tensor.shape[0], 1).ne(eos_token_id_tensor.unsqueeze(1)).prod(dim=0)\n",
    "            )\n",
    "\n",
    "        while True:\n",
    "            if synced_gpus:\n",
    "                # Under synced_gpus the `forward` call must continue until all gpus complete their sequence.\n",
    "                # The following logic allows an early break if all peers finished generating their sequence\n",
    "                this_peer_finished_flag = torch.tensor(0.0 if this_peer_finished else 1.0).to(input_ids.device)\n",
    "                # send 0.0 if we finished, 1.0 otherwise\n",
    "                dist.all_reduce(this_peer_finished_flag, op=dist.ReduceOp.SUM)\n",
    "                # did all peers finish? the reduced sum will be 0.0 then\n",
    "                if this_peer_finished_flag.item() == 0.0:\n",
    "                    break\n",
    "\n",
    "            # prepare model inputs\n",
    "            model_inputs = model_instance.prepare_inputs_for_generation(input_ids, **model_kwargs)\n",
    "\n",
    "            # forward pass to get next token\n",
    "            outputs = model_instance(\n",
    "                **model_inputs,\n",
    "                return_dict=True,\n",
    "                output_attentions=output_attentions,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "            )\n",
    "\n",
    "            if synced_gpus and this_peer_finished:\n",
    "                continue  # don't waste resources running the code we don't need\n",
    "\n",
    "            next_token_logits = outputs.logits[:, -1, :]\n",
    "\n",
    "            # pre-process distribution\n",
    "            next_tokens_scores = logits_processor(input_ids, next_token_logits)\n",
    "\n",
    "            # Store scores, attentions and hidden_states when required\n",
    "            if return_dict_in_generate:\n",
    "                if output_scores:\n",
    "                    scores += (next_tokens_scores,)\n",
    "                if output_attentions:\n",
    "                    decoder_attentions += (\n",
    "                        (outputs.decoder_attentions,)\n",
    "                        if model_instance.config.is_encoder_decoder\n",
    "                        else (outputs.attentions,)\n",
    "                    )\n",
    "                    if model_instance.config.is_encoder_decoder:\n",
    "                        cross_attentions += (outputs.cross_attentions,)\n",
    "\n",
    "                if output_hidden_states:\n",
    "                    decoder_hidden_states += (\n",
    "                        (outputs.decoder_hidden_states,)\n",
    "                        if model_instance.config.is_encoder_decoder\n",
    "                        else (outputs.hidden_states,)\n",
    "                    )\n",
    "\n",
    "            top_sign = self.top - 1 if self.first_check == 0 else 0  # first check, or to generate the rest\n",
    "            next_tokens = torch.argsort(next_tokens_scores, dim=-1, descending=True)[:, top_sign]\n",
    "\n",
    "            this_peer_finished, next_tokens = self.first_checking(next_tokens, next_tokens_scores)\n",
    "\n",
    "            if next_tokens in self.zone_zero:\n",
    "                this_peer_finished = True\n",
    "\n",
    "            # finished sentences should have their next token be a padding token\n",
    "            if eos_token_id is not None:\n",
    "                if pad_token_id is None:\n",
    "                    raise ValueError(\"If `eos_token_id` is defined, make sure that `pad_token_id` is defined.\")\n",
    "                next_tokens = next_tokens * unfinished_sequences + pad_token_id * (1 - unfinished_sequences)\n",
    "\n",
    "            # update generated ids, model inputs, and length for next step\n",
    "            input_ids = torch.cat([input_ids, next_tokens[:, None]], dim=-1)\n",
    "            if streamer is not None:\n",
    "                streamer.put(next_tokens.cpu())\n",
    "            model_kwargs = model_instance._update_model_kwargs_for_generation(\n",
    "                outputs, model_kwargs, is_encoder_decoder=model_instance.config.is_encoder_decoder\n",
    "            )\n",
    "\n",
    "            # if eos_token was found in one sentence, set sentence to finished\n",
    "            if eos_token_id_tensor is not None:\n",
    "                unfinished_sequences = unfinished_sequences.mul(\n",
    "                    next_tokens.tile(eos_token_id_tensor.shape[0], 1).ne(eos_token_id_tensor.unsqueeze(1)).prod(dim=0)\n",
    "                )\n",
    "\n",
    "                # stop when each sentence is finished\n",
    "                if unfinished_sequences.max() == 0:\n",
    "                    this_peer_finished = True\n",
    "\n",
    "            # stop if we exceed the maximum length\n",
    "            if stopping_criteria(input_ids, scores):\n",
    "                this_peer_finished = True\n",
    "\n",
    "            if this_peer_finished and not synced_gpus:\n",
    "                break\n",
    "\n",
    "        if streamer is not None:\n",
    "            streamer.end()\n",
    "\n",
    "        if return_dict_in_generate:\n",
    "            if model_instance.config.is_encoder_decoder:\n",
    "                return GreedySearchEncoderDecoderOutput(\n",
    "                    sequences=input_ids,\n",
    "                    scores=scores,\n",
    "                    encoder_attentions=encoder_attentions,\n",
    "                    encoder_hidden_states=encoder_hidden_states,\n",
    "                    decoder_attentions=decoder_attentions,\n",
    "                    cross_attentions=cross_attentions,\n",
    "                    decoder_hidden_states=decoder_hidden_states,\n",
    "                )\n",
    "            else:\n",
    "                return GreedySearchDecoderOnlyOutput(\n",
    "                    sequences=input_ids,\n",
    "                    scores=scores,\n",
    "                    attentions=decoder_attentions,\n",
    "                    hidden_states=decoder_hidden_states,\n",
    "                )\n",
    "        else:\n",
    "            return input_ids\n",
    "\n",
    "    def require_first_to_be(self, next_tokens_scores, values_to_extract=[29871]):\n",
    "        top_k_indices = torch.topk(next_tokens_scores, k=next_tokens_scores.shape[-1], dim=-1).indices\n",
    "        top_k_indices_np = top_k_indices.cpu().numpy()\n",
    "        mask = np.isin(top_k_indices_np, values_to_extract)\n",
    "        top_k_indices = top_k_indices_np[mask][0]\n",
    "        top_k_indices = torch.tensor(top_k_indices)\n",
    "\n",
    "        next_tokens = top_k_indices.item()\n",
    "        next_tokens = torch.tensor(next_tokens).reshape(-1)\n",
    "        current_device = torch.cuda.current_device() if torch.cuda.is_available() else \"cpu\"\n",
    "        return next_tokens.to(current_device), top_k_indices.to(current_device)\n",
    "\n",
    "    def my_utils_greedy_search_top10_recursive(\n",
    "        self,\n",
    "        model_instance,\n",
    "        m_inloop,\n",
    "        gen_length,\n",
    "        input_ids: torch.LongTensor,\n",
    "        logits_processor: Optional[LogitsProcessorList] = None,\n",
    "        stopping_criteria: Optional[StoppingCriteriaList] = None,\n",
    "        max_length: Optional[int] = None,\n",
    "        pad_token_id: Optional[int] = None,\n",
    "        eos_token_id: Optional[Union[int, List[int]]] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        output_scores: Optional[bool] = None,\n",
    "        return_dict_in_generate: Optional[bool] = None,\n",
    "        synced_gpus: bool = False,\n",
    "        streamer: Optional[\"BaseStreamer\"] = None,\n",
    "        **model_kwargs,\n",
    "    ):\n",
    "        # init values\n",
    "        print(\"another day, another destiny\")\n",
    "        stopping_criteria = StoppingCriteriaList([MaxLengthCriteria(max_length=gen_length + input_ids.shape[1])])\n",
    "        logits_processor = logits_processor if logits_processor is not None else LogitsProcessorList()\n",
    "        stopping_criteria = stopping_criteria if stopping_criteria is not None else StoppingCriteriaList()\n",
    "        if max_length is not None:\n",
    "            warnings.warn(\n",
    "                \"`max_length` is deprecated in this function, use\"\n",
    "                \" `stopping_criteria=StoppingCriteriaList([MaxLengthCriteria(max_length=max_length)])` instead.\",\n",
    "                UserWarning,\n",
    "            )\n",
    "            stopping_criteria = validate_stopping_criteria(stopping_criteria, max_length)\n",
    "        pad_token_id = pad_token_id if pad_token_id is not None else model_instance.generation_config.pad_token_id\n",
    "        eos_token_id = eos_token_id if eos_token_id is not None else model_instance.generation_config.eos_token_id\n",
    "        if isinstance(eos_token_id, int):\n",
    "            eos_token_id = [eos_token_id]\n",
    "        eos_token_id_tensor = torch.tensor(eos_token_id).to(input_ids.device) if eos_token_id is not None else None\n",
    "        output_scores = output_scores if output_scores is not None else model_instance.generation_config.output_scores\n",
    "        output_attentions = (\n",
    "            output_attentions if output_attentions is not None else model_instance.generation_config.output_attentions\n",
    "        )\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states\n",
    "            if output_hidden_states is not None\n",
    "            else model_instance.generation_config.output_hidden_states\n",
    "        )\n",
    "        return_dict_in_generate = (\n",
    "            return_dict_in_generate\n",
    "            if return_dict_in_generate is not None\n",
    "            else model_instance.generation_config.return_dict_in_generate\n",
    "        )\n",
    "\n",
    "        scores = () if (return_dict_in_generate and output_scores) else None\n",
    "        decoder_attentions = () if (return_dict_in_generate and output_attentions) else None\n",
    "        cross_attentions = () if (return_dict_in_generate and output_attentions) else None\n",
    "        decoder_hidden_states = () if (return_dict_in_generate and output_hidden_states) else None\n",
    "        dict_parameter_return = {\n",
    "            \"scores\": scores,\n",
    "            \"decoder_attentions\": decoder_attentions,\n",
    "            \"cross_attentions\": cross_attentions,\n",
    "            \"decoder_hidden_states\": decoder_hidden_states,\n",
    "        }\n",
    "\n",
    "        if return_dict_in_generate and model_instance.config.is_encoder_decoder:\n",
    "            encoder_attentions = model_kwargs[\"encoder_outputs\"].get(\"attentions\") if output_attentions else None\n",
    "            encoder_hidden_states = (\n",
    "                model_kwargs[\"encoder_outputs\"].get(\"hidden_states\") if output_hidden_states else None\n",
    "            )\n",
    "\n",
    "        unfinished_sequences = torch.ones(input_ids.shape[0], dtype=torch.long, device=input_ids.device)\n",
    "        this_peer_finished = False\n",
    "        # end initialization\n",
    "\n",
    "        model_inputs = model_instance.prepare_inputs_for_generation(input_ids, **model_kwargs)\n",
    "        # most time consuming part:\n",
    "        outputs = model_instance(\n",
    "            **model_inputs,\n",
    "            return_dict=True,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "        )\n",
    "        next_token_logits = outputs.logits[:, -1, :]\n",
    "\n",
    "        next_tokens_scores = logits_processor(input_ids, next_token_logits)\n",
    "\n",
    "        if return_dict_in_generate:\n",
    "            if output_scores:\n",
    "                scores += (next_tokens_scores,)\n",
    "            if output_attentions:\n",
    "                decoder_attentions += (\n",
    "                    (outputs.decoder_attentions,) if model_instance.config.is_encoder_decoder else (outputs.attentions,)\n",
    "                )\n",
    "                if model_instance.config.is_encoder_decoder:\n",
    "                    cross_attentions += (outputs.cross_attentions,)\n",
    "\n",
    "            if output_hidden_states:\n",
    "                decoder_hidden_states += (\n",
    "                    (outputs.decoder_hidden_states,)\n",
    "                    if model_instance.config.is_encoder_decoder\n",
    "                    else (outputs.hidden_states,)\n",
    "                )\n",
    "\n",
    "        next_tokens, _ = self.require_first_to_be(next_tokens_scores)\n",
    "\n",
    "        if eos_token_id is not None:\n",
    "            if pad_token_id is None:\n",
    "                raise ValueError(\"If `eos_token_id` is defined, make sure that `pad_token_id` is defined.\")\n",
    "            next_tokens = next_tokens * unfinished_sequences + pad_token_id * (1 - unfinished_sequences)\n",
    "\n",
    "        input_ids = torch.cat([input_ids, next_tokens[:, None]], dim=-1)\n",
    "        if streamer is not None:\n",
    "            streamer.put(next_tokens.cpu())\n",
    "        model_kwargs = model_instance._update_model_kwargs_for_generation(\n",
    "            outputs, model_kwargs, is_encoder_decoder=model_instance.config.is_encoder_decoder\n",
    "        )\n",
    "\n",
    "        if eos_token_id_tensor is not None:\n",
    "            unfinished_sequences = unfinished_sequences.mul(\n",
    "                next_tokens.tile(eos_token_id_tensor.shape[0], 1).ne(eos_token_id_tensor.unsqueeze(1)).prod(dim=0)\n",
    "            )\n",
    "        # Time consume: 6-7s\n",
    "\n",
    "        def _recursive_greedy_search_top10(\n",
    "            input_ids,\n",
    "            model_kwargs,\n",
    "            unfinished_sequences,\n",
    "            this_peer_finished,\n",
    "            eos_token_id_tensor,\n",
    "            dict_parameter_return,\n",
    "        ):\n",
    "            def _get_outputs():\n",
    "                model_inputs = model_instance.prepare_inputs_for_generation(input_ids, **model_kwargs)\n",
    "                outputs = model_instance(\n",
    "                    **model_inputs,\n",
    "                    return_dict=True,\n",
    "                    output_attentions=output_attentions,\n",
    "                    output_hidden_states=output_hidden_states,\n",
    "                )\n",
    "                return outputs\n",
    "\n",
    "            # Base\n",
    "            if synced_gpus:\n",
    "                this_peer_finished_flag = torch.tensor(0.0 if this_peer_finished else 1.0).to(input_ids.device)\n",
    "                dist.all_reduce(this_peer_finished_flag, op=dist.ReduceOp.SUM)\n",
    "                if this_peer_finished_flag.item() == 0.0:\n",
    "                    return input_ids\n",
    "            if this_peer_finished and not synced_gpus:\n",
    "                return input_ids\n",
    "\n",
    "            outputs = _get_outputs()\n",
    "            next_token_logits = outputs.logits[:, -1, :]\n",
    "            next_tokens_scores = logits_processor(input_ids, next_token_logits)\n",
    "\n",
    "            if return_dict_in_generate:\n",
    "                if output_scores:\n",
    "                    dict_parameter_return[\"scores\"] += (next_tokens_scores,)\n",
    "                if output_attentions:\n",
    "                    dict_parameter_return[\"decoder_attentions\"] += (\n",
    "                        (outputs.decoder_attentions,)\n",
    "                        if model_instance.config.is_encoder_decoder\n",
    "                        else (outputs.attentions,)\n",
    "                    )\n",
    "                    if model_instance.config.is_encoder_decoder:\n",
    "                        dict_parameter_return[\"cross_attentions\"] += (outputs.cross_attentions,)\n",
    "\n",
    "                if output_hidden_states:\n",
    "                    dict_parameter_return[\"decoder_hidden_states\"] += (\n",
    "                        (outputs.decoder_hidden_states,)\n",
    "                        if model_instance.config.is_encoder_decoder\n",
    "                        else (outputs.hidden_states,)\n",
    "                    )\n",
    "\n",
    "            top_sign = self.top - 1 if self.first_check == 0 else 0\n",
    "            next_tokens = torch.argsort(next_tokens_scores, dim=-1, descending=True)[:, top_sign]\n",
    "\n",
    "            this_peer_finished, next_tokens = self.first_checking(next_tokens, next_tokens_scores)\n",
    "\n",
    "            if next_tokens in self.zone_zero:\n",
    "                this_peer_finished = True\n",
    "\n",
    "            # finished sentences should have their next token be a padding token\n",
    "            if eos_token_id is not None:\n",
    "                if pad_token_id is None:\n",
    "                    raise ValueError(\"If `eos_token_id` is defined, make sure that `pad_token_id` is defined.\")\n",
    "                next_tokens = next_tokens * unfinished_sequences + pad_token_id * (1 - unfinished_sequences)\n",
    "\n",
    "            # update generated ids, model inputs, and length for next step\n",
    "            input_ids = torch.cat([input_ids, next_tokens[:, None]], dim=-1)\n",
    "            if streamer is not None:\n",
    "                streamer.put(next_tokens.cpu())\n",
    "            model_kwargs = model_instance._update_model_kwargs_for_generation(\n",
    "                outputs, model_kwargs, is_encoder_decoder=model_instance.config.is_encoder_decoder\n",
    "            )\n",
    "\n",
    "            if eos_token_id_tensor is not None:\n",
    "                unfinished_sequences = unfinished_sequences.mul(\n",
    "                    next_tokens.tile(eos_token_id_tensor.shape[0], 1).ne(eos_token_id_tensor.unsqueeze(1)).prod(dim=0)\n",
    "                )\n",
    "\n",
    "                if unfinished_sequences.max() == 0:\n",
    "                    this_peer_finished = True\n",
    "\n",
    "            if stopping_criteria(input_ids, dict_parameter_return[\"scores\"]):\n",
    "                this_peer_finished = True\n",
    "            # recursion\n",
    "            return _recursive_greedy_search_top10(\n",
    "                input_ids,\n",
    "                model_kwargs,\n",
    "                unfinished_sequences,\n",
    "                this_peer_finished,\n",
    "                eos_token_id_tensor,\n",
    "                dict_parameter_return,\n",
    "            )\n",
    "\n",
    "        # main cur begins\n",
    "        input_ids = _recursive_greedy_search_top10(\n",
    "            input_ids,\n",
    "            model_kwargs,\n",
    "            unfinished_sequences,\n",
    "            this_peer_finished,\n",
    "            eos_token_id_tensor,\n",
    "            dict_parameter_return,\n",
    "        )\n",
    "\n",
    "        if streamer is not None:\n",
    "            streamer.end()\n",
    "\n",
    "        if return_dict_in_generate:\n",
    "            if model_instance.config.is_encoder_decoder:\n",
    "                return GreedySearchEncoderDecoderOutput(\n",
    "                    sequences=input_ids,\n",
    "                    scores=scores,\n",
    "                    encoder_attentions=encoder_attentions,\n",
    "                    encoder_hidden_states=encoder_hidden_states,\n",
    "                    decoder_attentions=decoder_attentions,\n",
    "                    cross_attentions=cross_attentions,\n",
    "                    decoder_hidden_states=decoder_hidden_states,\n",
    "                )\n",
    "            else:\n",
    "                return GreedySearchDecoderOnlyOutput(\n",
    "                    sequences=input_ids,\n",
    "                    scores=scores,\n",
    "                    attentions=decoder_attentions,\n",
    "                    hidden_states=decoder_hidden_states,\n",
    "                )\n",
    "        else:\n",
    "            return input_ids\n",
    "\n",
    "    def model_calling(self, m_inloop):\n",
    "        ids = self.tokenizer.encode(self.args.PROMPT)\n",
    "        input_ids = torch.LongTensor([ids]).to(\"cuda\")\n",
    "        self.first_check = 0  # set check sign = 0 first\n",
    "        out = self.my_generate_top10(\n",
    "            model_instance=self.model,\n",
    "            m=m_inloop,  # m_inloop useless here now\n",
    "            input_ids=input_ids,\n",
    "            max_length=self.args.CONTEXT_LEN,\n",
    "            gen_length=36,\n",
    "            do_sample=False,  # with temperature unset\n",
    "            # temperature=self.args.TEMPERATURE\n",
    "        )\n",
    "        out_text = self.tokenizer.decode(out[0])\n",
    "        answer = out_text.replace(self.args.PROMPT, \"\").replace(\"\\nEND\", \"\").strip()\n",
    "        answer = answer.replace(\"\\n\", \"\")\n",
    "\n",
    "        answer_regs = [\n",
    "            re.match(pattern, answer).group(1) if re.match(pattern, answer) else answer for pattern in self.patterns\n",
    "        ]\n",
    "        return answer, answer_regs\n",
    "\n",
    "    # @blockPrinting\n",
    "    def eval(self, c, cnt=0, path_results=None, filter_yes=True):\n",
    "        query = \"\"\n",
    "        c1 = c[\"c1\"]\n",
    "        c3 = c[\"c3\"]\n",
    "        c10 = c[\"c10\"]\n",
    "\n",
    "        if path_results is not None:  # for processing result files\n",
    "            test_results = read_results(path_results)\n",
    "        # preprocess multiple answers\n",
    "        dict_qu_ans = self.gen_set_ans(dir_full_test=self.args.testfull, dir_time2id=self.args.time2id)\n",
    "        set_checked_qu = set()\n",
    "        num_infer = len(self.tests)  #\n",
    "\n",
    "        valid_cnt = 0\n",
    "        for i in tqdm(range(cnt, num_infer)):  # cnt, len(self.tests[:-1])\n",
    "            his_query = self.tests[i]\n",
    "            query = his_query.split(\"\\n\")[-1]\n",
    "            # if len(his_query.split(\"\\n\")) < 5:\n",
    "            #     continue  # TODO:  retrieval     \n",
    "            valid_cnt += 1\n",
    "            # truncated history\n",
    "            val_trunc = -1\n",
    "            if len(his_query) - 1 > val_trunc and val_trunc != -1:\n",
    "                li_his_trunc = his_query.split(\"\\n\")[-val_trunc - 1 : -1]  # backward\n",
    "                li_his_trunc.append(query)\n",
    "                his_query = \"\\n\".join(li_his_trunc)\n",
    "\n",
    "            delete = False\n",
    "            if delete == True:\n",
    "                his_query = re.sub(r\"\\d+:\\s\", \"\", his_query)\n",
    "\n",
    "            ins = \"\"\"<s>[INST] <<SYS>> \\\n",
    "            You must be able to correctly predict the next {object_label} from \\\n",
    "            a given text consisting of multiple quadruplets in the form of \"{time}:[{subject}, {relation}, {object_label}.{object}]\" \\\n",
    "            and the query in the form of \"{time}:[{subject}, {relation},\" in the end.\\n\\\n",
    "            You must generate {object_label}.{object}\\n\\n<</SYS>>\"\"\"\n",
    "            self.args.PROMPT = ins + his_query + \"[/INST]\" if self.args.instruct_yes else his_query\n",
    "\n",
    "            if query not in set_checked_qu:\n",
    "                set_checked_qu.add(query)\n",
    "                hello = \"For\"\n",
    "\n",
    "            else:\n",
    "                hello = \"Duplicate query:\"\n",
    "            print(hello, query)\n",
    "            if query == \"\":  # probably the end\n",
    "                continue\n",
    "            print(\"Given answers\", dict_qu_ans[query], \"with\", self.test_ans[i], \"as the gt\")\n",
    "\n",
    "            content_to_write = []\n",
    "            content_to_write2 = []\n",
    "            m_inloop = -1\n",
    "            filter_m_count = -1\n",
    "            k_inloop = self.topk  # k\n",
    "            self.constraints = []\n",
    "            self.top = 1  # reset top\n",
    "            exist_num = 0\n",
    "            if path_results is not None:  # for processing result files\n",
    "                num_Test, li_results = read_num_and_li_results(test_results[i])\n",
    "                exist_num = len(li_results)\n",
    "                if int(num_Test) != i:\n",
    "                    print(num_Test, i)\n",
    "                    raise ValueError(\"Test id and i do not match.\")\n",
    "            while m_inloop < k_inloop - 1:  # Use while to allow changing \"m\"\n",
    "                m_inloop += 1\n",
    "                filter_m_count += 1\n",
    "                with torch.no_grad():  # loops for one history_query\n",
    "                    if path_results is None:  # or self.args.ft==1\n",
    "                        raw_ans, answer_regs = self.model_calling(m_inloop)\n",
    "                        print(str(m_inloop) + \"-th time, I would say, \", answer_regs)\n",
    "                    else:\n",
    "\n",
    "                        if m_inloop >= exist_num:\n",
    "                            if not filter_yes:\n",
    "                                break\n",
    "                            else:\n",
    "                                print(\"call of duty\")\n",
    "                                raw_ans, answer_regs = self.generate_extra_answers(m_inloop, k_inloop)\n",
    "                                print(str(m_inloop) + \"-th time, I would say, \", answer_regs)\n",
    "                        else:\n",
    "                            # existing results\n",
    "                            raw_ans = answer_regs = [li_results[m_inloop]]\n",
    "                            pattern = re.compile(r\".*?[\\d:@][._](.*)\\]\")  #'\\s*(\\d+)\\.(.*?)\\]')\n",
    "                            answer_regs = (\n",
    "                                [re.match(pattern, answer_regs[0]).group(2).strip()]\n",
    "                                if re.match(pattern, answer_regs[0])\n",
    "                                else answer_regs\n",
    "                            )\n",
    "                            print(str(m_inloop) + \" read \", answer_regs)\n",
    "                            self.top += 1\n",
    "\n",
    "                    content_to_write.append(\"\\n\" + str(answer_regs))\n",
    "                    content_to_write2.append(\"\\n\" + str(raw_ans))\n",
    "\n",
    "                    # check multiple regex\n",
    "                    bingo = False\n",
    "                    dict_qu_ans_lower = [ans.lower() for ans in dict_qu_ans[query]]\n",
    "                    for answer in answer_regs:\n",
    "                        answerlow = answer.lower()\n",
    "                        gtlow = self.test_ans[i].lower()\n",
    "                        if answer == \"\":\n",
    "                            content_to_write.append(\"(none string; removed)\")\n",
    "                            k_inloop += 1\n",
    "                            filter_m_count -= 1\n",
    "                            print(\"increased k: \" + str(k_inloop))\n",
    "                            break\n",
    "                        if (\n",
    "                            answerlow != gtlow and answerlow in dict_qu_ans_lower\n",
    "                        ) and filter_yes:  # first_check = -1 if to check breach of obligation\n",
    "                            print(\"Got another answer: \" + answer + \", ignored.\")\n",
    "                            content_to_write.append(\"(ignored gt)\")\n",
    "                            k_inloop += 1\n",
    "                            filter_m_count -= 1\n",
    "                            print(\"increased k: \" + str(k_inloop))\n",
    "                            break\n",
    "                        elif answerlow == gtlow:\n",
    "                            bingo = True\n",
    "                            if filter_m_count == 0:\n",
    "                                c1 += 1\n",
    "                                c3 += 1\n",
    "                                c10 += 1\n",
    "                            elif 0 < filter_m_count < 3:\n",
    "                                c3 += 1\n",
    "                                c10 += 1\n",
    "                            elif 3 <= filter_m_count < 10:\n",
    "                                c10 += 1\n",
    "                            print(\n",
    "                                \"Bingo! Line: \",\n",
    "                                i,\n",
    "                                \"count after filtering: \",\n",
    "                                filter_m_count + 1,\n",
    "                                \"all count: \",\n",
    "                                m_inloop + 1,\n",
    "                                \"answer: \",\n",
    "                                answer,\n",
    "                                \"gt: \",\n",
    "                                self.test_ans[i],\n",
    "                            )\n",
    "                            break\n",
    "                    if bingo:\n",
    "                        break\n",
    "\n",
    "            hits_1 = c1 / (i + 1)\n",
    "            hits_3 = c3 / (i + 1)\n",
    "            hits_10 = c10 / (i + 1)\n",
    "\n",
    "            # hits_1 = c1 / valid_cnt\n",
    "            # hits_3 = c3 / valid_cnt\n",
    "            # hits_10 = c10 / valid_cnt\n",
    "            \"\"\"\n",
    "            print(\"hit1=\", c1, \"/\", str(i+1), \"=\", hits_1)\n",
    "            print(\"hit3=\", c3, \"/\", str(i+1), \"=\", hits_3)\n",
    "            print(\"hit10=\", c10, \"/\", str(i+1), \"=\", hits_10)\"\"\"\n",
    "\n",
    "            # Create parent directories if they don't exist\n",
    "            eval_txt_dir = os.path.dirname(self.eval_txt_path)\n",
    "            if eval_txt_dir and not os.path.exists(eval_txt_dir):\n",
    "                os.makedirs(eval_txt_dir, exist_ok=True)\n",
    "\n",
    "            # Create the txt file if it doesn't exist\n",
    "            if not os.path.exists(self.eval_txt_path):\n",
    "                with open(self.eval_txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(\"\")  # Create empty file\n",
    "\n",
    "            with open(self.eval_txt_path, \"a\", encoding=\"utf-8\") as fout:\n",
    "                if self.args.tf == 1:\n",
    "                    fout.write(\"current model: \" + self.args.LORA_CHECKPOINT_DIR + \", \\n\")\n",
    "                else:\n",
    "                    fout.write(\"current model: \" + self.args.MODEL_NAME + \", \\n\")\n",
    "                fout.write(self.args.output_file + \" currently finished: \" + str(i + 1) + \"; results: \\n\")\n",
    "                fout.write(\"Hits@1: \" + str(round(hits_1, 3)) + \"\\n\")\n",
    "                fout.write(\"Hits@3: \" + str(round(hits_3, 3)) + \"\\n\")\n",
    "                fout.write(\"Hits@10: \" + str(round(hits_10, 3)) + \"\\n\")\n",
    "                fout.write(str(c1) + \"\\n\")\n",
    "                fout.write(str(c3) + \"\\n\")\n",
    "                fout.write(str(c10) + \"\\n\\n\")\n",
    "\n",
    "            with open(self.args.output_file, \"a\", encoding=\"utf-8\") as f:\n",
    "                f.write('{\"Test' + str(i) + '\": [\"' + \", \".join(content_to_write) + '\"]}, \\n\\n')\n",
    "            with open(self.args.output_file.replace(\".txt\", \"_raw.txt\"), \"a\", encoding=\"utf-8\") as f:\n",
    "                f.write('{\"Test' + str(i) + '\": [\"' + \", \".join(content_to_write2) + '\"]}, \\n\\n')\n",
    "\n",
    "            print(\"processing: \" + self.args.output_file, i + 1)\n",
    "            time.sleep(0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from transformers import LlamaTokenizer\n",
    "import torch\n",
    "import re\n",
    "\n",
    "import os\n",
    "\n",
    "# uv run inference.py --LORA_CHECKPOINT_DIR \"./model/icews14\" --output_file \"./results/llmda_new_rule/prediction_icews14.txt\" \n",
    "#  --input_file \"./data/processed_rule2/icews14/test/history_facts/history_facts_icews14.txt\" \n",
    "#  --test_ans_file \"./data/processed_rule2/icews14/test/test_answers/test_answers_icews14.txt\" >> logs/test-llmda-rule3.log 2>&1\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description=\"Config of Llama2-lora\")\n",
    "\n",
    "    # inference : \n",
    "    parser.add_argument('--MODEL_NAME', type=str, default=\"TheBloke/Llama-2-7B-fp16\", help='Model name')\n",
    "    parser.add_argument('--LORA_CHECKPOINT_DIR', type=str, default=\"./model/icews14\", help='Your Lora checkpoint')\n",
    "    parser.add_argument('--CONTEXT_LEN', type=int, default=4096, help='Truncation length of context (in json)')\n",
    "    parser.add_argument('--BIT_8', default=True, action=\"store_true\", help='Use 8-bit')\n",
    "    parser.add_argument('--BIT_4', default=False, action=\"store_true\", help='Use 4-bit')\n",
    "    parser.add_argument('--TEMPERATURE', type=int, default=0, help='Temperature when inference')\n",
    "    parser.add_argument('--PROMPT', type=str, default=\"Input your prompt\", help='Your prompt when inference')\n",
    "    parser.add_argument('--input_file', type=str, default=\"./retrieved_facts/icews14/test/history_facts/history_facts_icews14.txt\", help='Your history_facts file')\n",
    "    parser.add_argument('--output_file', type=str, default=\"prediction_result/prediction_icews14.txt\", help='Output text prediction')\n",
    "    parser.add_argument('--test_ans_file', type=str, default=\"./retrieved_facts/icews14/test/test_answers/test_answers_icews14.txt\", help='Your ground truth file')\n",
    "    parser.add_argument('--testfull', type=str, default=\"\", help='fulltest with dense quadruples. For whole set filtering')\n",
    "    parser.add_argument('--time2id', type=str, default=\"\", help='time2id json file. For whole set filtering')\n",
    "    parser.add_argument('--begin', type=int, default=0, help='Where to continue. default to -1')\n",
    "    parser.add_argument('--max_gen_len', type=int, default=27, help='18 for Gdelt 27 for Yago; 27 as default')\n",
    "    parser.add_argument('--max_seq_len', type=int, default=30, help='4096 for llama2 icl; 30 as default')\n",
    "    parser.add_argument('--last_metric', type=str, default=\"\", help='Last metric result *file* when interrupted. ')\n",
    "    parser.add_argument('--FILTER', type=int, default=1, help='Set 1 to filter multiple objects. ')\n",
    "    parser.add_argument('--path_results', type=str, default=\"\", help='Path of the result file to be filtered. ')\n",
    "    parser.add_argument('--tf', type=int, default=1, help='Set 0: no finetuned model. ')\n",
    "    parser.add_argument('--local-rank', type=int, default=0, help='for torch.distributed.launch. ')\n",
    "    parser.add_argument('--instruct_yes', type=int, default=0, help='Set 0 to give no instruction in the pompts. ')\n",
    "    \n",
    "    return parser.parse_known_args()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    args, _ = parse_args()\n",
    "\n",
    "    eval_txt_path = args.output_file[:-4] + \"_metric_results.txt\"\n",
    "\n",
    "    test_ans = read_test_an(args.test_ans_file)\n",
    "    if args.tf == 1:\n",
    "        model = decide_model(args)\n",
    "        tokenizer = LlamaTokenizer.from_pretrained(args.MODEL_NAME, trust_remote_code=True)\n",
    "    else:\n",
    "        args.max_batch_size = 9\n",
    "        model = None\n",
    "        tokenizer = None\n",
    "        print(\"Using unfinetuned model \")\n",
    "    tests = read_test_and_divide(args.input_file)\n",
    "\n",
    "    c = read_last_metric(args.last_metric)  # dict with keys: c1 c3 c10\n",
    "\n",
    "    pattern1 = re.compile(r\".*?[\\d:@][._](.*?)[\\]\\[]?([< ].*?)?$\")\n",
    "    pattern2 = re.compile(r\"<s> .*?[\\n]?([A-Z\\u00C0-\\u00DD\\u0388-\\u03AB\\u0410-\\u042F\\u0600-\\u06FF\\u4e00-\\u9fa5].*)\\]\")\n",
    "    pattern3 = re.compile(r\"<s> *(.*)\\]\")\n",
    "    is_with_id = False  # TODO change\n",
    "    if is_with_id:\n",
    "        patterns = [pattern1]\n",
    "    else:\n",
    "        patterns = [pattern1, pattern2, pattern3]\n",
    "    topk = 10\n",
    "    cnt = args.begin\n",
    "    early_stop_chars = [\n",
    "        torch.tensor([29962], device=\"cuda:0\"),  # ]\n",
    "        torch.tensor([29961], device=\"cuda:0\"),  # [\n",
    "        torch.tensor([4638], device=\"cuda:0\"),  # )]\n",
    "        torch.tensor([29871], device=\"cuda:0\"),\n",
    "    ]  #\n",
    "    obligations = []\n",
    "    evaler = Evaler(\n",
    "        topk, tests, test_ans, eval_txt_path, args, model, tokenizer, patterns, early_stop_chars, obligations\n",
    "    )\n",
    "\n",
    "    path_results = args.path_results\n",
    "    path_results = os.path.normpath(path_results)\n",
    "\n",
    "    if path_results != \".\":\n",
    "        evaler.eval(c, cnt, path_results)\n",
    "    else:\n",
    "        evaler.eval(c, cnt, filter_yes=args.FILTER)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
